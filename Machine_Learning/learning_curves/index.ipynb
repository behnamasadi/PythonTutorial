{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves\n",
    "The mathematical representation of the learning process is given by Learning Curves.\n",
    "\n",
    "\n",
    "\n",
    "- **Train Learning Curve:** The Learning curve determined from the dataset of training provides an indication of how well the model is learning.\n",
    "\n",
    "- **Validation Learning Curve:** A learning curve calculated from a dataset of hold-out validation that gives an indication of how well the model generalizes\n",
    "\n",
    "- **Optimization Learning Curves:** Learning curves, based on the metric by which the model parameters are optimized, i.e.loss.\n",
    "\n",
    "- **Performance Learning Curves:** Learning curves based onthe metric by which the model is tested and chosen, i.e. accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfit Learning/ High Bias Curves\n",
    "\n",
    "An Underfit/ High Bias model has a flat line (after feeding it with enugh taining sample) with  relatively high loss, indicating that the model was unable to learn the training dataset at all.\n",
    "\n",
    "<img src='../bias_variance/images/high_bias.svg' /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit Learning Curves/ High Variance\n",
    "\n",
    "Overfitting refers to a model that has learned too much in the training dataset, even the statistical noise or the training dataset's random fluctuations.\n",
    "\n",
    "- The plot of training loss continues to decrease with experience.\n",
    "- The plot of validation loss decreases to a point and begins increasing again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Fit Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unrepresentative Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs: [1](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Curves\n",
    "Refs: [1](https://scikit-learn.org/stable/modules/learning_curve.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation/ Training Error\n",
    "\n",
    "bias_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision/ Recall Error\n",
    "Precision-Recall curves summarize the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds.\n",
    "\n",
    "\n",
    "**Precision, Predictive Value (PPV)**: Precision is related to the performance of the classifier on <b>one the classes</b> in our data.\n",
    "\n",
    "${\\displaystyle \\mathrm {PPV} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP} }}}$\n",
    "\n",
    "\n",
    "**Recall, Sensitivity, Hit Rate, or True Positive Rate (TPR)**: \n",
    "\n",
    "${\\displaystyle \\mathrm {TPR} ={\\frac {\\mathrm {TP} }{\\mathrm {P} }}={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN} }}=1-\\mathrm {FNR} }$\n",
    "\n",
    "\n",
    "<strong>High recall, low precision:Â  </strong>This means that our classifier finds almost all positive examples in our test set but also recognizes a lot of negative examples as positive examples.\n",
    "\n",
    "<strong>Low recall, high precision: </strong>This means our classifier is very certain about positive examples (if it has labeled as positive, with high confident it is positive) meanwhile our classifier has missed a lot of positive example, kind of conservative classifier.\n",
    "\n",
    "\n",
    "\n",
    "Where there is a moderate to significant class imbalance, Precision-Recall curves can be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC, AUC\n",
    "ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds.\n",
    "\n",
    "\n",
    "ROC curves should be used when there are roughly equal numbers of observations for each class.\n",
    "\n",
    "\n",
    "<img src='../metrics_and_scoring_for_model_evaluation/images/ROC.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs: [1](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../metrics_and_scoring_for_model_evaluation/images/How-to-Choose-a-Metric-for-Imbalanced-Classification-latest.png'>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
