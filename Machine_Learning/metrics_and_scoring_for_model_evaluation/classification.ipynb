{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Validation Set and Test Set</h1>\n",
    "use the validation data to evaluate different trial choices of hyper-parameters such as the number of epochs to train for, the learning rate, the best network architecture, and so on.\n",
    "\n",
    "once we’ve got the hyper-parameters\n",
    "we want, we do a final evaluation of accuracy using the test_data. That gives us confidence\n",
    "that our results on the test_data are a true measure of how well our neural network generalizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have a binary classifier cats and non- cats, we have 1100 test images, 1000 non cats, 100 cats. The output of the classifier is either <strong>Positive</strong>  which means “<strong>cat</strong>” or <strong>Negative</strong> which  means <strong>non-cat. </strong>\n",
    "\n",
    "In the case of $TP,FP,TN,FN$:\n",
    "- **Positive** or **Negative** lable that classifier assignto the data.\n",
    "- **True** or **False** comes from correspondence of prediction with actual data, if both are same it is True otherweise False.\n",
    "\n",
    "The following is called confusion matrix:\n",
    "\n",
    "<h3>True Positive, Hit (TP)</h3>\n",
    "Data is positive, and is predicted to be positive.90 cats correctly labeled.\n",
    "<h3>True Negative,Correct Rejection (TN)</h3>\n",
    "Data is negative, and is predicted to be negative. 940 images labeled as non-cats, and they are non-cats.\n",
    "<h3>False Positive False Alarm, Type I error (FP) </h3>\n",
    "Data is negative, but is predicted positive. 60 non-cat images labeled cats, but they are cats.\n",
    "<h3>False Negative Miss, Type II error (FN)</h3>\n",
    "Data is positive, but is predicted negative.10 images labels as cat, but they are truly non-cats.\n",
    "\n",
    "<h3>Positive</h3>\n",
    "The number of real positive cases in the data.\n",
    "\n",
    "$P=TP+FN$\n",
    "\n",
    "<h3>Negative</h3>\n",
    "The number of real negative cases in the data.\n",
    "\n",
    "$N=TN+FP$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Confusion Matrix</h2>\n",
    "\n",
    "<img src=\"images/confusion_matrix.jpg\" />\n",
    "\n",
    "<tbody><tr>\n",
    "<td>\n",
    "<table class=\"wikitable\" style=\"border:none; float:left; margin-top:0; text-align:center\">\n",
    "<tbody><tr>\n",
    "<th style=\"background:white; border:none;\" colspan=\"2\" rowspan=\"2\">\n",
    "</th>\n",
    "<th colspan=\"2\" style=\"background:none;\">Actual class\n",
    "</th></tr>\n",
    "<tr>\n",
    "<th>P\n",
    "</th>\n",
    "<th>N\n",
    "</th></tr>\n",
    "<tr>\n",
    "<th rowspan=\"2\" style=\"height:6em;background:none;\"><div>Predicted<br />class</div>\n",
    "</th>\n",
    "<th>P\n",
    "</th>\n",
    "<td><b>TP</b>\n",
    "</td>\n",
    "<td>FP\n",
    "</td></tr>\n",
    "<tr>\n",
    "<th>N\n",
    "</th>\n",
    "<td>FN\n",
    "</td>\n",
    "<td><b>TN</b>\n",
    "</td></tr>\n",
    "</tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Accuracy (ACC)</h2>\n",
    "Accuracy belongs to the <b>eniter classifier</b> and not to the specific single class. Accuracy is defined as the number of correct predictions divided by the total number of predictions. \n",
    "\n",
    "${\\displaystyle \\mathrm {ACC} ={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {P} +\\mathrm {N} }}={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}}$\n",
    "\n",
    "Classification accuracy= (90+940)/(1000+100)= 1030/1100= 93.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Balanced Accuracy </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Top k Accuracy </h2>\n",
    "Let's assume you're using a neural network to implement a machine learning algorithm for object recognition. A image of a cat is shown, and these are the neural network outputs:\n",
    "\n",
    "- Tiger: 0.4\n",
    "- Dog: 0.3\n",
    "- Cat: 0.1\n",
    "- Lynx: 0.09\n",
    "- Lion: 0.08\n",
    "- Bird: 0.02\n",
    "- Bear: 0.01\n",
    "\n",
    "You will count this performance as incorrect, using **top-1 accuracy**, since it predicted a tiger.\n",
    "\n",
    "You count this production as accurate using **top-5 accuracy**, since the cat is among the top-5 guesses.\n",
    "\n",
    "Refs: [1](https://intellipaat.com/community/6715/evaluation-calculate-top-n-accuracy-top-1-and-top-5#:~:text=Top%2D1%20accuracy%20is%20the,must%20match%20the%20expected%20answer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Error Rate</h2>\n",
    "${\\displaystyle 1-\\mathrm {ACC} =1-{\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {P} +\\mathrm {N} }}=1-{\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}={\\frac {\\mathrm {FP} +\\mathrm {FN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}}$\n",
    "\n",
    "\n",
    "\n",
    "Why does one prefer accuracy vs. error? \n",
    "\n",
    "Human perception: \"99 percent accurate\" sends a different psychological message than \"has an error rate of 1 percent \" In comparison, a rise in accuracy from 99 percent to 99.9 percent increases accuracy by 1 percent, but a decrease in error from 1 percent to 0.1 percent is a 90 percent decrease in error, even though the two represent the same improvement in the real world.\n",
    "\n",
    "Refs: [1](https://stackoverflow.com/questions/52865390/definition-of-error-rate-in-classification-and-why-some-researchers-use-error-ra)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Top K Error Rate</h2>\n",
    "\n",
    "top-1 and top-5 error rate? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Accuracy vs Precision</h1>\n",
    "Statistician prefers to use the terms **bias** and **variability** instead of **accuracy** and **precision**.\n",
    "\n",
    "- *bias* is the amount of *inaccuracy*  \n",
    "- *variability* is the amount of *imprecision*.\n",
    "\n",
    "<img src='images/accuracy_and_precision.svg'/>\n",
    "\n",
    "<h2>High Accuracy Low Precision </h2>\n",
    "<img src='images/high_accuracy_low_precision.svg'/>\n",
    "\n",
    "\n",
    "<h2>High Precision High Accuracy </h2>\n",
    "<img src='images/high_precision_high_accuracy.svg'/>\n",
    "\n",
    "\n",
    "<h2>High Precision Low Accuracy </h2>\n",
    "<img src='images/high_precision_low_accuracy.svg'/>\n",
    "\n",
    "<h2>Low Precision Low Accuracy </h2>\n",
    "<img src='images/low_precision_low_accuracy.svg'/>\n",
    "\n",
    "Refs: [1](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/06.00-Figure-Code.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Precision, Predictive Value (PPV) </h2>\n",
    "Precision is related to the performance of the classifier on <b>one the classes</b> in our data. Precision is not always a good indicator for the performance of our classifier. If one class has more frequency in our set, and we predict it correctly while the classifier wrongly label the  smaller class, accuracy could be very high but the performance of the classifier is bad so:\n",
    "\n",
    "${\\displaystyle \\mathrm {PPV} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP} }}}$\n",
    "\n",
    "Precision cat=  90/(90+60) = 60%\n",
    "\n",
    "Precision non-cat= 940/950= 98.9%\n",
    "\n",
    "Refs: [1](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sensitivity, Recall, Hit Rate, or True Positive Rate (TPR)</h1>\n",
    "Recall is the ratio of the total number of correctly classified positive examples divide to the total number of positive examples, kind of optimistic classifier.\n",
    "\n",
    "${\\displaystyle \\mathrm {TPR} ={\\frac {\\mathrm {TP} }{\\mathrm {P} }}={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN} }}=1-\\mathrm {FNR} }$\n",
    "\n",
    "\n",
    "Recall cat= 90/100= 90%\n",
    "Recall non-cat= 940/1000= 94%\n",
    "\n",
    "<strong>High recall, low precision:  </strong>This means that our classifier finds almost all positive examples in our test set but also recognizes a lot of negative examples as positive examples.\n",
    "\n",
    "<strong>Low recall, high precision: </strong>This means our classifier is very certain about positive examples (if it has labeled as positive, with high confident it is positive) meanwhile our classifier has missed a lot of positive example, kind of conservative classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Specificity, Selectivity or True Negative Rate (TNR) </h1>\n",
    "Sensitivity and specificity are two other popular metrics mostly used in medical and biology. Specificity is basically computing recall for negative classes.\n",
    "\n",
    "\n",
    "${\\displaystyle \\mathrm {TNR} ={\\frac {\\mathrm {TN} }{\\mathrm {N} }}={\\frac {\\mathrm {TN} }{\\mathrm {TN} +\\mathrm {FP} }}=1-\\mathrm {FPR} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Fall Out, False Positive Rate (FPR) </h1>\n",
    "\n",
    "\n",
    "${\\displaystyle \\mathrm {FPR} ={\\frac {\\mathrm {FP} }{\\mathrm {N} }}={\\frac {\\mathrm {FP} }{\\mathrm {FP} +\\mathrm {TN} }}=1-\\mathrm {TNR} }$\n",
    "\n",
    "\n",
    "High values means: <strong>False Positive</strong> <strong>&gt;</strong> <strong>True Negative</strong> which means  our classifier labels many examples as Positive while they are Negative and this ratio is bigger than the examples that are actually Negative and correctly labeled as Negative.\n",
    "\n",
    "Small value means <strong>True Negative &gt; </strong><strong>False Positive</strong> which means our classifier truly labels examples that are negative and the ratio is bigger than the examples that are Negative and classifier labels them as Positive.\n",
    "<strong>False Positive Rate=1-Specificity=False Positive/ (False Positive + True Negative)</strong>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Miss Rate, False Negative Rate (FNR)</h1>\n",
    "${\\displaystyle \\mathrm {FNR} ={\\frac {\\mathrm {FN} }{\\mathrm {P} }}={\\frac {\\mathrm {FN} }{\\mathrm {FN} +\\mathrm {TP} }}=1-\\mathrm {TPR} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>F1-Score (Sørensen-Dice Coefficient)</h1>\n",
    "Is the harmonic mean of precision and sensitivity. Depending on application, you might be interested in a conservative or optimistic classifier. But sometimes you are not biased toward any of the classes in your set, so you need to combine precision and recall together.\n",
    "\n",
    "\n",
    "${\\displaystyle \\mathrm {F} _{1}=2\\cdot {\\frac {\\mathrm {PPV} \\cdot \\mathrm {TPR} }{\\mathrm {PPV} +\\mathrm {TPR} }}={\\frac {2\\mathrm {TP} }{2\\mathrm {TP} +\\mathrm {FP} +\\mathrm {FN} }}}$\n",
    "\n",
    "\n",
    "F1-score cat= 2*0.6*0.9/(0.6+0.9)= 72%\n",
    "\n",
    "<h3>F1 Micro </h3>\n",
    "<h3>F1 Macro </h3>\n",
    "<h3>F1 Weighted </h3>\n",
    "<h3>F1 Samples </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Receiver Operating Characteristic (ROC)</h1>\n",
    "\n",
    "Here we have a classifier which classifies obese (positive) and not obese (negative) base on the weight\n",
    "\n",
    "<img src='images/obese_classifier.svg'>\n",
    "\n",
    "\n",
    "We have to set a cut off value which below that the output of the classifier will be considerd **not obese** and above that obese. We can set different values and based on that we can calculate $TP, TN, FP, FN$.\n",
    "\n",
    "For instance if we set very low we would have:\n",
    "\n",
    "Classifier Cut Off Value is :0.1\n",
    "\n",
    "$TP=4,FP=2,TN=2,FN=0, TPR=1.0, FPR=0.5$\n",
    "\n",
    "<img src='images/classifier_cut_off_value_0.1.svg'/>\n",
    "\n",
    "\n",
    "Classifier Cut Off Value is :0.39\n",
    "\n",
    "$TP=3,FP=2,TN=2,FN=1, TPR=0.75, FPR=0.5$\n",
    "\n",
    "<img src='images/0.3913123398875481.svg'/>\n",
    "\n",
    "\n",
    "\n",
    "Classifier Cut Off Value is :0.97\n",
    "\n",
    "$TP=3,FP=0,TN=4,FN=1,TPR=0.75,FPR=0.0$\n",
    "<img src='images/0.9681187290638695.svg'/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC** curve is plotting <strong>TPR</strong> against <strong>FPR</strong> for various cut-off threshold values. \n",
    "\n",
    "ROC curve is a popular curve to look at overall model performance and pick a good cut-off threshold for the model.\n",
    "\n",
    "\n",
    "<img src='images/ROC.svg' />\n",
    "\n",
    "We can see that the cut off threshold at 0.97 is better than 0.39 since they have the same **true positive rate** but the one at  0.97 has less **false positive rate**.\n",
    "\n",
    "Depending on how many false positive we would like to accept, 0.97 or 0.1  would be the ideal cut off threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Area Under the Curve (AUC)</h1>\n",
    "Here we have used another classifier and we can see the area under curve in our first classifier is bigger, suggesting our first classifier is better.\n",
    "<img src='images/second_ROC.svg' />\n",
    "\n",
    "Refs: [1](https://www.youtube.com/watch?v=4jRBRDbJemM&t=2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> ROC AUC One-vs-rest </h2>\n",
    "<h2> ROC AUC One-vs-one </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Confident Interval</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neg Brier Score </h1>\n",
    "\n",
    "\n",
    "<h1>Neg Log Loss </h1>\n",
    "\n",
    "\n",
    "<h1>Jaccard </h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs:<a href=\"https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce\"> [1]</a>, <a href=\"https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\">[2]</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
