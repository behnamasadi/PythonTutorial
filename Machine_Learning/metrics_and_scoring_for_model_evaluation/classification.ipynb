{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Validation Set and Test Set</h1>\n",
    "use the validation data to evaluate different trial choices of hyper-parameters such as the number of epochs to train for, the learning rate, the best network architecture, and so on.\n",
    "\n",
    "once we’ve got the hyper-parameters\n",
    "we want, we do a final evaluation of accuracy using the test_data. That gives us confidence\n",
    "that our results on the test_data are a true measure of how well our neural network generalizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have a binary classifier cats and non- cats, we have 1100 test images, 1000 non cats, 100 cats. The output of the classifier is either <strong>Positive</strong>  which means “<strong>cat</strong>” or <strong>Negative</strong> which  means <strong>non-cat. </strong>The following is called confusion matrix:\n",
    "\n",
    "<h3>True Positive, Hit (TP)</h3>\n",
    "Data is positive, and is predicted to be positive.90 cats correctly labeled.\n",
    "<h3>True Negative,Correct Rejection (TN)</h3>\n",
    "Data is negative, and is predicted to be negative. 940 images labeled as non-cats, and they are non-cats.\n",
    "<h3>False Positive False Alarm, Type I error (FP) </h3>\n",
    "Data is negative, but is predicted positive. 60 non-cat images labeled cats, but they are cats.\n",
    "<h3>False Negative Miss, Type II error (FN)</h3>\n",
    "Data is positive, but is predicted negative.10 images labels as cat, but they are truly non-cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Confusion Matrix</h2>\n",
    "\n",
    "<img src=\"images/confusion_matrix.jpg\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Accuracy (ACC)</h2>\n",
    "Accuracy is defined as the number of correct predictions divided by the total number of predictions. \n",
    "\n",
    "${\\displaystyle \\mathrm {ACC} ={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {P} +\\mathrm {N} }}={\\frac {\\mathrm {TP} +\\mathrm {TN} }{\\mathrm {TP} +\\mathrm {TN} +\\mathrm {FP} +\\mathrm {FN} }}}$\n",
    "\n",
    "Classification accuracy= (90+940)/(1000+100)= 1030/1100= 93.6%\n",
    "\n",
    "<h2> Balanced Accuracy </h2>\n",
    "<h2> Top k Accuracy </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Accuracy vs Precision (Bias Variance tradeoff) </h1>\n",
    "Statistician prefers to use the terms **bias** and **variability** instead of **accuracy** and **precision**.\n",
    "\n",
    "- *bias* is the amount of *inaccuracy*  \n",
    "- *variability* is the amount of *imprecision*.\n",
    "\n",
    "<img src='images/accuracy_and_precision.svg'/>\n",
    "\n",
    "<h2>High Accuracy Low Precision </h2>\n",
    "<img src='images/high_accuracy_low_precision.svg'/>\n",
    "\n",
    "\n",
    "<h2>High Precision High Accuracy </h2>\n",
    "<img src='images/high_precision_high_accuracy.svg'/>\n",
    "\n",
    "\n",
    "<h2>High Precision Low Accuracy </h2>\n",
    "<img src='images/high_precision_low_accuracy.svg'/>\n",
    "\n",
    "<h2>Low Precision Low Accuracy </h2>\n",
    "<img src='images/low_precision_low_accuracy.svg'/>\n",
    "\n",
    "Refs: [1](https://www.youtube.com/watch?v=iuJgyiS7BKM), [2](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), [3](https://www.youtube.com/watch?v=lpkSGTT8uMg), [4](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/06.00-Figure-Code.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Precision, Predictive Value (PPV) </h2>\n",
    "Precision is not always a good indicator for the performance of our classifier. If one class has more frequency in our set, and we predict it correctly while the classifier wrongly label the  smaller class, accuracy could be very high but the performance of the classifier is bad so:\n",
    "\n",
    "${\\displaystyle \\mathrm {PPV} ={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FP} }}}$\n",
    "\n",
    "Precision cat=  90/(90+60) = 60%\n",
    "\n",
    "Precision non-cat= 940/950= 98.9%\n",
    "\n",
    "Refs: [1](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sensitivity, Recall, Hit Rate, or True Positive Rate (TPR)</h1>\n",
    "Recall is the ratio of the total number of correctly classified positive examples divide to the total number of positive examples, kind of optimistic classifier.\n",
    "\n",
    "${\\displaystyle \\mathrm {TPR} ={\\frac {\\mathrm {TP} }{\\mathrm {P} }}={\\frac {\\mathrm {TP} }{\\mathrm {TP} +\\mathrm {FN} }}=1-\\mathrm {FNR} }$\n",
    "\n",
    "\n",
    "Recall cat= 90/100= 90%\n",
    "Recall non-cat= 940/1000= 94%\n",
    "\n",
    "<strong>High recall, low precision:  </strong>This means that our classifier finds almost all positive examples in our test set but also recognizes a lot of negative examples as positive examples.\n",
    "\n",
    "<strong>Low recall, high precision: </strong>This means our classifier is very certain about positive examples (if it has labeled as positive, with high confident it is positive) meanwhile our classifier has missed a lot of positive example, kind of conservative classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Specificity, Selectivity or True Negative Rate (TNR) </h1>\n",
    "Sensitivity and specificity are two other popular metrics mostly used in medical and biology. Specificity is basically computing recall for negative classes.\n",
    "\n",
    "\n",
    "${\\displaystyle \\mathrm {TNR} ={\\frac {\\mathrm {TN} }{\\mathrm {N} }}={\\frac {\\mathrm {TN} }{\\mathrm {TN} +\\mathrm {FP} }}=1-\\mathrm {FPR} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Fall Out, False Positive Rate (FPR) </h1>\n",
    "\n",
    "\n",
    "${\\displaystyle \\mathrm {FPR} ={\\frac {\\mathrm {FP} }{\\mathrm {N} }}={\\frac {\\mathrm {FP} }{\\mathrm {FP} +\\mathrm {TN} }}=1-\\mathrm {TNR} }$\n",
    "\n",
    "\n",
    "High values means: <strong>False Positive</strong> <strong>&gt;</strong> <strong>True Negative</strong> which means  our classifier labels many examples as Positive while they are Negative and this ratio is bigger than the examples that are actually Negative and correctly labeled as Negative.\n",
    "\n",
    "Small value means <strong>True Negative &gt; </strong><strong>False Positive</strong> which means our classifier truly labels examples that are negative and the ratio is bigger than the examples that are Negative and classifier labels them as Positive.\n",
    "<strong>False Positive Rate=1-Specificity=False Positive/ (False Positive + True Negative)</strong>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Miss Rate, False Negative Rate (FNR)</h1>\n",
    "${\\displaystyle \\mathrm {FNR} ={\\frac {\\mathrm {FN} }{\\mathrm {P} }}={\\frac {\\mathrm {FN} }{\\mathrm {FN} +\\mathrm {TP} }}=1-\\mathrm {TPR} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>F1-Score (Sørensen-Dice Coefficient)</h1>\n",
    "Is the harmonic mean of precision and sensitivity. Depending on application, you might be interested in a conservative or optimistic classifier. But sometimes you are not biased toward any of the classes in your set, so you need to combine precision and recall together.\n",
    "\n",
    "\n",
    "${\\displaystyle \\mathrm {F} _{1}=2\\cdot {\\frac {\\mathrm {PPV} \\cdot \\mathrm {TPR} }{\\mathrm {PPV} +\\mathrm {TPR} }}={\\frac {2\\mathrm {TP} }{2\\mathrm {TP} +\\mathrm {FP} +\\mathrm {FN} }}}$\n",
    "\n",
    "\n",
    "F1-score cat= 2*0.6*0.9/(0.6+0.9)= 72%\n",
    "\n",
    "<h3>F1 Micro </h3>\n",
    "<h3>F1 Macro </h3>\n",
    "<h3>F1 Weighted </h3>\n",
    "<h3>F1 Samples </h3>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<h1>Receiver Operating Characteristic (ROC)</h1>\n",
    "ROC illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n",
    "The output of a classifier is usually a probabilistic number, and we based on a cut off value decided to accept or reject the value. ROC curve is plotting <strong>TPR</strong> against <strong>FPR</strong> for various threshold values. ROC curve is a popular curve to look at overall model performance and pick a good cut-off threshold for the model.\n",
    "\n",
    "\n",
    "\n",
    "<h1><strong class=\"hp ib\">Area Under the Curve (AUC)</strong></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> ROC AUC One-vs-rest </h2>\n",
    "<h2> ROC AUC One-vs-one </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Confident Interval</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neg Brier Score </h1>\n",
    "\n",
    "\n",
    "<h1>Neg Log Loss </h1>\n",
    "\n",
    "\n",
    "<h1>Jaccard </h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs:<a href=\"https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce\"> [1]</a>, <a href=\"https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\">[2]</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
