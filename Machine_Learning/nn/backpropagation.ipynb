{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "from sympy import *\n",
    "#init_printing(use_latex=True)\n",
    "init_printing(use_latex='svg')\n",
    "from IPython.core.display import SVG\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/nn_a(1)_0.svg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last layer $L$ has $n$ neuron.\n",
    "\n",
    "Layer $L-1$ has $m$ neuron.\n",
    "\n",
    "Layer $L-2$ has $p$ neuron.\n",
    "\n",
    "$\\textbf{W}^{(L)}_{n \\times m}$\n",
    "\n",
    "$\\textbf{W}^{(L-1)}_{m \\times p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "w_{0,0}^{(1)} &w_{0,1}^{(1)}  & \\cdots   &w_{0,5}^{(1)} \\\\ \n",
    "w_{1,0}^{(1)} &w_{1,1}^{(1)}  & \\cdots   &w_{1,5}^{(1)} \\\\ \n",
    " \\vdots &  \\ddots &  & \\\\ \n",
    "w_{3,0}^{(1)} &w_{3,1}^{(1)}  & \\cdots   &w_{3,5}^{(1)} \n",
    "\\end{bmatrix}\n",
    "_{4\\times6}\n",
    "\\begin{bmatrix}\n",
    "a^{(0)}_{0}\n",
    "\\\\ a^{(0)}_{1}\n",
    "\\\\ \\vdots\n",
    "\\\\ a^{(0)}_{5}\n",
    "\\end{bmatrix}\n",
    "_{6\\times1}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b^{(1)}_{0}\n",
    "\\\\ b^{(1)}_{1}\n",
    "\\\\ \\vdots\n",
    "\\\\ b^{(1)}_{5}\n",
    "\\end{bmatrix}\n",
    "_{6\\times1}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "z^{(1)}_{0}\n",
    "\\\\ z^{(1)}_{1}\n",
    "\\\\ \\vdots\n",
    "\\\\ z^{(1)}_{3}\n",
    "\\end{bmatrix}\n",
    "_{4\\times1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{(1)}_{0}=\\sigma( \n",
    "w_{0,0}\\times a^{(0)}_{0} +\n",
    "w_{0,1}\\times a^{(0)}_{1}+\n",
    "w_{0,2}\\times a^{(0)}_{2}+\n",
    "w_{0,3}\\times a^{(0)}_{3}+\n",
    "w_{0,4}\\times a^{(0)}_{4}+\n",
    "w_{0,5}\\times a^{(0)}_{5}+b_{0}^{(1)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_{j,i}^{(L)}$ indicates the weight that connect $i_{th}$ node in $L-1$ layer to $j_{th}$ node in the layer $L$ (We have $k$ neuron in the layer $L-1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_{j}^{(L)}=w_{j,0}^{(L)}\\times a^{(L-1)}_{0} +\n",
    "w_{j,1}^{(L)}\\times a^{(L-1)}_{1}+\n",
    "\\cdots+\n",
    "w_{j,i}^{(L)}\\times a^{(L-1)}_{i}+\n",
    "\\cdots+\n",
    "w_{j,k}^{(L)}\\times a^{(L-1)}_{k}+b_{j}$\n",
    "\n",
    "$a^{(L)}_{j}=\\sigma(z_{j}^{(L)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last layer we have $n$ output, the cost for our $\\textbf{first}$ example in the training set, $c_{0}$ is:\n",
    "\n",
    "$c_{0}=\\sum_{j=0}^{n-1} (a_{j}^{(L)}-y_{j})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing error relative to the changes of weights in the last layer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/nn_last_layer.svg' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\frac{\\partial c_{0}}{\\partial w_{jk}^{(L)}} =\n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial w_{jk}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing error relative to the changes of biases in the last layer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\frac{\\partial c_{0}}{\\partial b_{j}^{(L)}} =\n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial b_{j}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial C_{0}}{\\partial a_{j}^{(L)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the derivatives:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}}=2(a_{j}^{(L)}-y_{j})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) $\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}=\\sigma^{\\prime}(z_{j}^{(L)})$\n",
    "\n",
    "Assuming activation function is:\n",
    "$\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$\n",
    "\n",
    "\n",
    "$\\frac{\\sigma(x)}{\\partial x} = \\sigma(x)(1 - \\sigma(x))$\n",
    "Because:\n",
    "\n",
    "$\\begin{align}\n",
    "\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\\n",
    "&= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\\n",
    "&= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\\n",
    "&= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}}  \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}}  \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
    "&= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) $\\frac{\\partial z_{j}^{(L)}}{\\partial w_{jk}^{(L)}}=a^{(L-1)}_{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) $\\frac{\\partial z_{j}^{(L)}}{\\partial b_{j}^{(L)}}=1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}}$ is common in $\\frac{\\partial c_{0}}{\\partial b_{j}^{(L)}}$ and $\\frac{\\partial c_{0}}{\\partial w_{jk}^{(L)}}$ we call it $delta$ $\\delta^{(L)}_{j}$ which is the error of neuron $j$ in layer $l$:\n",
    "\n",
    "$\\delta^{(L)}_{j} =\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}}=\\frac{\\partial c}{\\partial z_{j}^{(L)}}=2(a_{j}^{(L)}-y_{j})\\sigma^{\\prime}(z_{j}^{(L)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give us:\n",
    "\n",
    "- $\\frac{\\partial c_{0}}{\\partial w_{jk}^{(L)}} =\n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial w_{jk}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial C_{0}}{\\partial a_{j}^{(L)}}=2(a_{j}^{(L)}-y_{j})\\sigma^{\\prime}(z_{j}^{(L)})a^{(L-1)}_{k}=\\delta^{(L)}_{j}a^{(L-1)}_{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\frac{\\partial c_{0}}{\\partial b_{j}^{(L)}} =\n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial b_{j}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial C_{0}}{\\partial a_{j}^{(L)}}=2(a_{j}^{(L)}-y_{j})\\sigma^{\\prime}(z_{j}^{(L)})=\\delta^{(L)}_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write these equations in vector form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{\\delta}^{(L)}\n",
    "=(\\delta^{(L)}_{0},\\delta^{(L)}_{1},...,\\delta^{(L)}_{n-1} ) \n",
    "=\\frac{\\partial \\textbf{c}}{\\partial \\textbf{z}^{(L)}}\n",
    "=(\\frac{\\partial c}{\\partial z_{0}^{(L)}} ,\\frac{\\partial c}{\\partial z_{1}^{(L)}},... ,\\frac{\\partial c}{\\partial z_{n-1}^{(L)}} )\n",
    "=(\\frac{\\partial a_{0}^{(L)}}{\\partial z_{0}^{(L)}}\\frac{\\partial c}{\\partial a_{0}^{(L)}},\\frac{\\partial a_{1}^{(L)}}{\\partial z_{1}^{(L)}}\\frac{\\partial c}{\\partial a_{1}^{(L)}},...\\frac{\\partial a_{n-1}^{(L)}}{\\partial z_{n-1}^{(L)}}\\frac{\\partial c}{\\partial a_{n-1}^{(L)}})\n",
    "=(2(a_{0}^{(L)}-y_{0})\\sigma^{\\prime}(z_{0}^{(L)}),2(a_{1}^{(L)}-y_{1})\\sigma^{\\prime}(z_{1}^{(L)}),...2(a_{n-1}^{(L)}-y_{n-1})\\sigma^{\\prime}(z_{n-1}^{(L)})) \n",
    "=2(\\textbf{a}^{(L)}-\\textbf{y}) \\odot   \\sigma^{\\prime}(\\textbf{z}^{(L)})\\tag{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial \\textbf{c}}{\\partial \\textbf{b}^{(L)}}=\\boldsymbol{\\delta}^{(L)} \\tag{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $m-1$ neuron in the layer $L-1$ \n",
    "- $\\frac{\\partial \\textbf{c}}{\\partial \\textbf{w}_{j}^{(L)}}\n",
    "=(\\frac{\\partial z_{j}^{(L)}}{\\partial w_{j0}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}},\n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial w_{j1}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}},...,\n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial w_{jm-1}^{(L)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial c_{0}}{\\partial a_{j}^{(L)}})\n",
    "=(\\delta^{(L)}_{j}a^{(L-1)}_{0},\\delta^{(L)}_{j}a^{(L-1)}_{1},...,\\delta^{(L)}_{j}a^{(L-1)}_{m-1})\n",
    "=\\delta_{j}^{(L)} \\textbf{a}^{(L-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if we write it in the matrix form:\n",
    "$ \\frac{\\partial \\textbf{c}}{\\partial \\textbf{W}^{(L)}}=\n",
    "\\begin{bmatrix}\n",
    "\\delta^{(L)}_{0}a^{(L-1)}_{0} & \\delta^{(L)}_{0}a^{(L-1)}_{1}&...&\\delta^{(L)}_{0}a^{(L-1)}_{m-1}\\\\ \n",
    "\\delta^{(L)}_{1}a^{(L-1)}_{0} & \\delta^{(L)}_{1}a^{(L-1)}_{1}&...&\\delta^{(L)}_{1}a^{(L-1)}_{m-1}\\\\\n",
    "\\vdots & \\vdots & \\vdots &\\vdots \\\\ \n",
    "\\delta^{(L)}_{j}a^{(L-1)}_{0} & \\delta^{(L)}_{j}a^{(L-1)}_{1}&...&\\delta^{(L)}_{j}a^{(L-1)}_{m-1}\\\\\n",
    "\\vdots & \\vdots & \\vdots &\\vdots \\\\ \n",
    "\\delta^{(L)}_{n-1}a^{(L-1)}_{0} & \\delta^{(L)}_{n-1}a^{(L-1)}_{1}&...&\\delta^{(L)}_{n-1}a^{(L-1)}_{m-1}\\\\ \n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "\\delta^{(L)}_{0}\\\\ \n",
    "\\delta^{(L)}_{1}\\\\ \n",
    "\\vdots\n",
    " \\\\ \n",
    "\\delta^{(L)}_{j}\\\\ \n",
    "\\vdots \\\\\n",
    "\\delta^{(L)}_{n-1}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "a^{(L-1)}_{0} & a^{(L-1)}_{1}&...&a^{(L-1)}_{m-1}\\\\ \n",
    "\\end{bmatrix}=\n",
    "\\boldsymbol{\\delta}^{(L)} \\cdot \\textbf{a}^{(L-1)}\\top \\tag{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing error relative to the changes of weights in the previous layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/nn_l-1_layer_a.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we know that $a_{k}^{(L-1)}$ (layer $L-1$ has $m$ neurons) has effect on all neuron in the layer $L$ layer (which has $n$ neurons), so to compute the rate of changes of error with respect to $a_{k}^{(L-1)}$ :\n",
    "\n",
    "$\\frac{\\partial c_{0}}{\\partial a_{k}^{(L-1)}} =\n",
    "\\sum_{j=0}^{n-1}( \n",
    "\\frac{\\partial z_{j}^{(L)}}{\\partial a_{k}^{(L-1)}}\n",
    "\\frac{\\partial a_{j}^{(L)}}{\\partial z_{j}^{(L)}}\n",
    "\\frac{\\partial C_{0}}{\\partial a_{j}^{(L)}})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know:\n",
    "\n",
    "$z_{j}^{(L)}=w_{j,0}^{(L)}\\times a^{(L-1)}_{0} +\n",
    "w_{j,1}^{(L)}\\times a^{(L-1)}_{1}+\n",
    "\\cdots+\n",
    "w_{j,i}^{(L)}\\times a^{(L-1)}_{i}+\n",
    "\\cdots+\n",
    "w_{j,k}^{(L)}\\times a^{(L-1)}_{k}+b_{j}$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\frac{\\partial z_{j}^{(L)}}{\\partial a_{k}^{(L-1)}}=w_{jk}^{(L)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently we have:\n",
    "\n",
    "\n",
    "$\\frac{\\partial c_{0}}{\\partial a_{k}^{(L-1)}} =\n",
    "\\sum_{i=0}^{n-1} 2(a_{i}^{(L)}-y_{i})\\sigma^{\\prime}(z_{i}^{(L)}) w_{ik}^{(L)}=\n",
    "\\sum_{i=0}^{n-1} \\delta^{(L)}_{i}w_{ik}^{(L)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Reminder:}$\n",
    "\n",
    "Last layer $L$ has $n$ neuron.\n",
    "\n",
    "Layer $L-1$ has $m$ neuron.\n",
    "\n",
    "Layer $L-2$ has $p$ neuron.\n",
    "\n",
    "$\\textbf{W}^{(L)}_{n \\times m}$\n",
    "\n",
    "$\\textbf{W}^{(L-1)}_{m \\times p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{W}^{(L)}_{n\\times m}=\\begin{bmatrix}\n",
    "w_{0,0}^{(L)} &w_{0,1}^{(L)}  & \\cdots   &w_{0,m-1}^{(L)} \\\\ \n",
    "w_{1,0}^{(L)} &w_{1,1}^{(L)}  & \\cdots   &w_{1,m-1}^{(L)} \\\\ \n",
    " \\vdots &  \\ddots &  & \\\\ \n",
    "w_{n-1,0}^{(L)} &w_{n-1,1}^{(L)}  & \\cdots   &w_{n-1,m-1}^{(L)} \n",
    "\\end{bmatrix}\n",
    "_{n\\times m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{W}^{(L)}_{m\\times n}\\top=\\begin{bmatrix}\n",
    "w_{0,0}^{(L)} &w_{1,0}^{(L)}  & \\cdots   & w_{n-1,0}^{(L)}\\\\ \n",
    "w_{0,1}^{(L)} &w_{1,1}^{(L)}  & \\cdots   &w_{1,n-1}^{(L)} \\\\ \n",
    " \\vdots &  \\ddots &  & \\\\ \n",
    "w_{0,m-1}^{(L)} &w_{m-1,1}^{(L)}  & \\cdots   &w_{m-1,n-1}^{(L)} \n",
    "\\end{bmatrix}\n",
    "_{m\\times n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol\\delta^{(L)}=\\begin{bmatrix}\n",
    "\\delta^{(L)}_{0}\\\\ \n",
    "\\delta^{(L)}_{1}\\\\ \n",
    "\\vdots\n",
    " \\\\ \n",
    "\\delta^{(L)}_{j}\\\\ \n",
    "\\vdots \\\\\n",
    "\\delta^{(L)}_{n-1}\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol \\sigma'(z^{(L-1)})=\n",
    "\\begin{bmatrix}\n",
    "\\sigma'(z^{(L-1)}_{0})\\\\ \n",
    "\\sigma'(z^{(L-1)}_{1})\\\\ \n",
    "\\vdots\\\\\n",
    "\\sigma'(z^{(L-1)}_{m-1})\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{a}^{(L-2)}\\top=\n",
    "\\begin{bmatrix}\n",
    "a^{(L-2)}_{0} & a^{(L-2)}_{1}&...&a^{(L-2)}_{p-1}\\\\ \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to compute the error relative to weight in layer $L-1$:\n",
    "\n",
    "$\\frac{\\partial c_{0}}{\\partial w_{jk}^{(L-1)}} =\n",
    "\\frac{\\partial z_{j}^{(L-1)}}{\\partial w_{jk}^{(L-1)}}\n",
    "\\frac{\\partial a_{j}^{(L-1)}}{\\partial z_{j}^{(L-1)}}\\frac{\\partial c_{0}}{\\partial a_{j}^{(L-1)}}=\n",
    "\\frac{\\partial z_{j}^{(L-1)}}{\\partial w_{jk}^{(L-1)}}\n",
    "\\frac{\\partial a_{j}^{(L-1)}}{\\partial z_{j}^{(L-1)}}\n",
    "\\sum_{i=0}^{n-1} \\delta^{(L)}_{i}w_{ij}^{(L)}=\n",
    "a_{k}^{(L-2)}\n",
    "\\sigma' (z_{j}^{(L-1)})\n",
    "\\sum_{i=0}^{n-1} \\delta^{(L)}_{i}w_{ij}^{(L)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial c_{0}}{\\partial w_{jk}^{(L-1)}} =\n",
    "a_{k}^{(L-2)}\n",
    "\\sum_{i=0}^{n-1} \\delta^{(L)}_{i}w_{ij}^{(L)}=\n",
    " \\left (   (\\textbf{W}^{(L)}_{(0:n-1,j)} \\top)\\cdot  \\boldsymbol\\delta^{(L)}\\right ) \\sigma' (z_{j}^{(L-1)})\n",
    " a_{k}^{(L-2)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was only one element in the $\\textbf{W}^{(L-1)}_{m \\times p}$, If we write it for entire matrix:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial\\textbf{c}}{\\partial\\textbf{W}^{(L-1)}}_{m\\times p}=\n",
    "%W\n",
    "\\begin{eqnarray} \n",
    "   (\\begin{bmatrix}\n",
    "w_{0,0}^{(L)} &w_{1,0}^{(L)}  & \\cdots   & w_{n-1,0}^{(L)}\\\\ \n",
    "w_{0,1}^{(L)} &w_{1,1}^{(L)}  & \\cdots   &w_{1,n-1}^{(L)} \\\\ \n",
    " \\vdots &  \\ddots &  & \\\\ \n",
    "w_{0,m-1}^{(L)} &w_{m-1,1}^{(L)}  & \\cdots   &w_{m-1,n-1}^{(L)} \n",
    "\\end{bmatrix}\n",
    "_{m\\times n}\n",
    "%delta \n",
    "\\begin{bmatrix}\n",
    "\\delta^{(L)}_{0}\\\\ \n",
    "\\delta^{(L)}_{1}\\\\ \n",
    "\\vdots\n",
    " \\\\ \n",
    "\\delta^{(L)}_{j}\\\\ \n",
    "\\vdots \\\\\n",
    "\\delta^{(L)}_{n-1}\n",
    "\\end{bmatrix}_{n \\times 1})\n",
    "\\odot \n",
    "%sigma prime\n",
    "\\begin{bmatrix}\n",
    "\\sigma'(z^{(L-1)}_{0})\\\\ \n",
    "\\sigma'(z^{(L-1)}_{1})\\\\ \n",
    "\\vdots\\\\\n",
    "\\sigma'(z^{(L-1)}_{m-1})\n",
    "\\end{bmatrix}_{m\\times 1}\n",
    "\\end{eqnarray}\n",
    "\\cdot \n",
    "%a L-2 vector\n",
    "\\begin{bmatrix}\n",
    "a^{(L-2)}_{0} & a^{(L-2)}_{1}&...&a^{(L-2)}_{p-1}\\\\ \n",
    "\\end{bmatrix}_{p \\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\frac{\\partial\\textbf{c}}{\\partial\\textbf{W}^{(L-1)}}=\n",
    "\\begin{eqnarray} \n",
    "   ((\\textbf{W}^{(L)})^T \\boldsymbol\\delta^{(L)}) \\odot \\boldsymbol\\sigma'(z^{(L-1)})\n",
    "\\end{eqnarray}\n",
    "\\cdot \\textbf{a}^{(L-2)}\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute $\\delta_k^{(L-1)}$:\n",
    "\n",
    "\n",
    "$\\delta_k^{(L-1)} = \\frac{\\partial C}{\\partial z_k^{(L-1)}} = \\sum_j ^{n-1} \\frac{\\partial C}{\\partial z_j^{(L)}} \\frac{\\partial z_j^{(L)}}{\\partial z_k^{(L-1)}} = \\sum_j  \\delta_j^{(L)}\\frac{\\partial z_j^{(L)}}{\\partial z_k^{(L-1)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since:\n",
    "$z^{(L)}_j = \\sum_{k} w_{jk}^{(L)} \\sigma(z_k^{(L) - 1}) + b_j^{(L)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivative is:\n",
    "\n",
    "$\\frac{\\partial z_j^{(L)}}{\\partial z_K^{(L-1)}} = w_{jK}^{(L)}\\sigma'(z_{K}^{(L-1)})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting these pieces together we have:\n",
    "\n",
    "$\\delta_k^{(L-1)} =\\sum_j^{n-1} \\delta_j^{(L)}\\frac{\\partial z_j^{(L)}}{\\partial z_K^{(L-1)}}= \\sum_j^{n-1}  (\\delta_j^{(L)}  w_{jK}^{(L)})\\sigma'(z_{K}^{(L-1)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  \\boldsymbol\\delta^{(L-1)}=\\frac{\\partial c}{\\partial z^{(L-1)}}  = ((\\textbf{W}^{(L)})^T \\boldsymbol\\delta^{(L)}) \\odot \\sigma'(z^{(L-1)})\n",
    "\\end{eqnarray}\\tag{4}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial\\textbf{c}}{\\partial \\textbf{b}^{(L-1)}}=\\boldsymbol\\delta^{(L-1)}\\tag{5}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\frac{\\partial\\textbf{c}}{\\partial\\textbf{W}^{(L-1)}}=\\boldsymbol\\delta^{(L-1)}\\cdot \\textbf{a}^{(L-2)}\\top\\tag{6}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that was only for the $\\textbf{first}$ item in the training set, you feed the rest of the training set into the network and for each $w_{jk}^{(L)}$ you make an average over all of them. Then that's the vector that you should use in your gradient descent function. In the output layer use equations (1), (2), (3) and in the hidden layers use equation (4), (5) and 6:\n",
    "\n",
    "$\\textbf{W}_{new}=\\textbf{W}_{initial}-\\eta \\nabla (\\textbf{W}_{initial})$\n",
    "\n",
    "$\\textbf{b}_{new}=\\textbf{b}_{initial}-\\eta \\nabla (\\textbf{b}_{initial})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you usually randomly shuffle your training set into batches called mini batches and compute the gradient for every batch.\n",
    "Ref: [1](https://stats.stackexchange.com/questions/414825/how-exactly-is-the-error-backpropagated-in-backpropagation), [2](https://www.youtube.com/watch?v=xClK__CqZnQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstable Gradient: Vanishing and Explosion Gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs [1](https://www.youtube.com/watch?v=qO_NLVjD6zE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
