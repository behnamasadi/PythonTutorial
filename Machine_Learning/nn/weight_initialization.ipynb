{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initialization\n",
    "First Lets have look at some properties of variance:\n",
    "## Variance Properties\n",
    "The variance can also be thought of as the covariance of a random variable with itself:\n",
    "\n",
    "1) $\\operatorname {Var} (X)=\\operatorname {Cov} (X,X)$.\n",
    " \n",
    "\n",
    "2) ${\\displaystyle {\\operatorname {Var} (X)=\\operatorname {E} \\left[(X-\\operatorname {E} [X])^{2}\\right]\\\\[4pt]=\\operatorname {E} \\left[X^{2}-2X\\operatorname {E} [X]+\\operatorname {E} [X]^{2}\\right]\\\\[4pt]=\\operatorname {E} \\left[X^{2}\\right]-2\\operatorname {E} [X]\\operatorname {E} [X]+\\operatorname {E} [X]^{2}\\\\[4pt]=\\operatorname {E} \\left[X^{2}\\right]-\\operatorname {E} [X]^{2}}}$\n",
    "\n",
    "Variance is invariant with respect to changes in a location parameter:\n",
    "\n",
    "3) $\\operatorname {Var} (X+a)=\\operatorname {Var} (X).$\n",
    "\n",
    "If all values are scaled by a constant, the variance is scaled by the square of that constant:\n",
    "\n",
    "4) $\\operatorname {Var} (aX)=a^{2}\\operatorname {Var} (X).$\n",
    "\n",
    "\n",
    "The variance of a sum of two random variables is given by:\n",
    "\n",
    "5) $\\operatorname {Var} (aX+bY)=a^{2}\\operatorname {Var} (X)+b^{2}\\operatorname {Var} (Y)+2ab\\,\\operatorname {Cov} (X,Y)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6) In general, for the sum of ${\\displaystyle N}$ random variables $\\{X_{1},\\dots ,X_{N}\\}$, the variance becomes:\n",
    "$\\operatorname {Var} \\left(\\sum _{i=1}^{N}X_{i}\\right)=\\sum _{i,j=1}^{N}\\operatorname {Cov} (X_{i},X_{j})=\\sum _{i=1}^{N}\\operatorname {Var} (X_{i})+\\sum _{i\\neq j}\\operatorname {Cov} (X_{i},X_{j})$\n",
    "\n",
    "\n",
    "7) These results lead to the variance of a linear combination as:\n",
    "\n",
    "\n",
    " ${\\begin{aligned}\\operatorname {Var} \\left(\\sum _{i=1}^{N}a_{i}X_{i}\\right)&=\\sum _{i,j=1}^{N}a_{i}a_{j}\\operatorname {Cov} (X_{i},X_{j})\\\\&=\\sum _{i=1}^{N}a_{i}^{2}\\operatorname {Var} (X_{i})+\\sum _{i\\not =j}a_{i}a_{j}\\operatorname {Cov} (X_{i},X_{j})\\\\&=\\sum _{i=1}^{N}a_{i}^{2}\\operatorname {Var} (X_{i})+2\\sum _{1\\leq i<j\\leq N}a_{i}a_{j}\\operatorname {Cov} (X_{i},X_{j}).\\end{aligned}}$\n",
    " \n",
    " \n",
    "8) Sum of uncorrelated variables (BienaymÃ© formula)\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}\\operatorname {Var} (X+Y)&=\\operatorname {E} \\left[X^{2}\\right]+2\\operatorname {E} [XY]+\\operatorname {E} \\left[Y^{2}\\right]-\\left(\\operatorname {E} [X]^{2}+2\\operatorname {E} [X]\\operatorname {E} [Y]+\\operatorname {E} [Y]^{2}\\right)\\\\[5pt]&=\\operatorname {E} \\left[X^{2}\\right]+\\operatorname {E} \\left[Y^{2}\\right]-\\operatorname {E} [X]^{2}-\\operatorname {E} [Y]^{2}\\\\[5pt]&=\\operatorname {Var} (X)+\\operatorname {Var} (Y).\\end{aligned}}}$\n",
    "\n",
    "\n",
    "${\\displaystyle \\operatorname {Var} \\left(\\sum _{i=1}^{n}X_{i}\\right)=\\sum _{i=1}^{n}\\operatorname {Var} (X_{i}).}$\n",
    "\n",
    "Refs [1](https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accroding to property number 6, we cas see:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/variance_property_sum_of_uncorrelated_variables.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set values $w_j$ from a normal distribution ($\\mu=0$ ,$\\sigma^2=1$) That means, in $z = \\sum_j w_j x_j+b$ which is sum of \n",
    "\n",
    "we can see from this graph that it's quite likely that |z| will be pretty large, then the output $\\sigma(z)$ from the hidden neuron will be very close to either 1 or 0.  That means our hidden neuron will have saturated (the dreivative for sigmoid function is almost 0).\n",
    "\n",
    "Suppose we have a neuron with $n_in$ input weights. Then we shall initialize those weights as Gaussian random variables with mean 0 and standard deviation $1/\\sqrt{n_{\\rm in}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function: $tanh$\n",
    "\n",
    "coefficient=0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/tanh_coefficient_0.01.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function: $tanh$\n",
    "\n",
    "coefficient=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<img src='images/tanh_coefficient_1.0.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function: $tanh$\n",
    "\n",
    "coefficient=each layer devided $input^{0.5} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<img src='images/tanh_sqrt(input_size).svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He et al., 2015\n",
    "\n",
    "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification Refs: [1](https://arxiv.org/abs/1502.01852)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function: $relu$\n",
    "\n",
    "coefficient=each layer devided $input^{0.5}/2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/relu_sqrt(input_size_divide_by2).svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs [1](http://neuralnetworksanddeeplearning.com/chap3.html#weight_initialization)\n",
    "\n",
    "Read more: [1](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi]), [2](https://arxiv.org/abs/1312.6120) [3](https://arxiv.org/abs/1412.6558), [4](https://arxiv.org/abs/1502.01852), [5](https://arxiv.org/abs/1511.06856), [6](https://arxiv.org/abs/1511.06422)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Weight Init\n",
    "\n",
    "[1](https://pytorch.org/docs/stable/nn.init.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
