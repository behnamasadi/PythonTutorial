{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "models with a large number of free parameters can describe\n",
    "an amazingly wide range of phenomena. Even if such a model agrees well with the available\n",
    "data, that doesn’t make it a good model. It may just mean there’s enough freedom in the\n",
    "model that it can describe almost any data set of the given size, without capturing any\n",
    "genuine insights into the underlying phenomenon. When that happens the model will work\n",
    "well for the existing data, but will fail to generalize to new situations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear least squares\n",
    "$X_{m\\times n}{\\vec {\\beta_{n\\times 1} }}=Y_{m\\times 1}$\n",
    "\n",
    "${\\displaystyle L(D,{\\vec {\\beta }})=||X{\\vec {\\beta }}-Y||^{2}=(X{\\vec {\\beta }}-Y)^{T}(X{\\vec {\\beta }}-Y)=Y^{T}Y-Y^{T}X{\\vec {\\beta }}-{\\vec {\\beta }}^{T}X^{T}Y+{\\vec {\\beta }}^{T}X^{T}X{\\vec {\\beta }}}$\n",
    "\n",
    "\n",
    "${\\displaystyle {\\frac {\\partial L(D,{\\vec {\\beta }})}{\\partial {\\vec {\\beta }}}}={\\frac {\\partial \\left(Y^{T}Y-Y^{T}X{\\vec {\\beta }}-{\\vec {\\beta }}^{T}X^{T}Y+{\\vec {\\beta }}^{T}X^{T}X{\\vec {\\beta }}\\right)}{\\partial {\\vec {\\beta }}}}=-2X^{T}Y+2X^{T}X{\\vec {\\beta }}}$\n",
    "\n",
    "\n",
    "setting the gradient of the loss to zero and solving for ${\\displaystyle {\\vec {\\beta }}}$ we get: \n",
    "\n",
    "${\\displaystyle -2X^{T}Y+2X^{T}X{\\vec {\\beta }}=0\\Rightarrow X^{T}Y=X^{T}X{\\vec {\\beta }}\\Rightarrow {\\vec {\\hat {\\beta }}}=(X^{T}X)^{-1}X^{T}Y}{\\displaystyle -2X^{T}Y+2X^{T}X{\\vec {\\beta }}=0}$\n",
    "\n",
    "\n",
    "$\\Rightarrow X^{T}Y=X^{T}X{\\vec {\\beta }}\\Rightarrow {\\vec {\\hat {\\beta }}}=(X^{T}X)^{-1}X^{T}Y$\n",
    "\n",
    "## Tikhonov regularization (ridge regression) with L2 norm\n",
    "We add the magnitude of $\\beta$ to our cost to plenalize huge weights and keep the weights small (close to zero)  and all other things being equal. \n",
    "\n",
    "${\\displaystyle {\\hat {\\beta }}_{R}=(\\mathbf {X} ^{\\mathsf {T}}\\mathbf {X} +\\lambda \\mathbf {I} )^{-1}\\mathbf {X} ^{\\mathsf {T}}\\mathbf {y} }$\n",
    "\n",
    "$\\begin{eqnarray} C = -\\frac{1}{n} \\sum_{xj} \\left[ y_j \\ln a^L_j+(1-y_j) \\ln\n",
    "(1-a^L_j)\\right] + \\frac{\\lambda}{2n} \\sum_w w^2.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "The first term is the cross-entropy and the second term, is the squares of all the weights in the network. \n",
    "\n",
    "\n",
    "$\\begin{eqnarray} C = \\frac{1}{2n} \\sum_x \\|y-a^L\\|^2 +\n",
    "  \\frac{\\lambda}{2n} \\sum_w w^2.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "n both cases we can write the regularized cost function as:\n",
    "\n",
    "$\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{2n}\n",
    "\\sum_w w^2,\n",
    "\\end{eqnarray}$\n",
    "\n",
    "$C_0$ is the original, unregularized cost function.\n",
    "\n",
    "$\\lambda$: when $\\lambda$ is small we prefer to minimize the original cost function, but when $\\lambda$ is\n",
    "large we prefer small weights.\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "b_{new} = b -\\eta \\frac{\\partial C_0}{\\partial b}.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  w_{new}= & & w-\\eta \\frac{\\partial C_0}{\\partial\n",
    "    w}-\\frac{\\eta \\lambda}{n} w   & = & \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\eta \\frac{\\partial\n",
    "    C_0}{\\partial w}. \n",
    "\\end{eqnarray}$\n",
    "\n",
    "For stochastic gradient descent we can estimate $\\partial C_0 / \\partial w$ by averaging over a mini-batch of m training examples. Thus the regularized learning rule for stochastic gradient descent becomes:\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  w_{new}= \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\frac{\\eta}{m}\n",
    "  \\sum_x \\frac{\\partial C_x}{\\partial w}, \n",
    "\\end{eqnarray}$\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  b_{new} = b - \\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial b},\n",
    "\\end{eqnarray}$\n",
    "\n",
    "\n",
    "$n$ is, as usual, the size of our training set\n",
    "\n",
    "$m$ is size of the mini-batch training examples\n",
    "\n",
    "\n",
    "Heuristically, if the cost function is unregularized, then the length of the weight vector is likely to grow, all other things being equal. Over time this can lead to the weight vector being very large indeed. This can cause the weight vector to get stuck pointing in more or less the same direction, since changes due to gradient descent only make tiny changes to the direction, when the length is long, which is making it hard for our learning algorithm to properly explore the weight space, and consequently harder to find good minima of the cost function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/regularization.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso with L1 norm\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator)\n",
    "\n",
    "Refs: [1](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Regularization\n",
    "[1](https://www.youtube.com/watch?v=ATo7vnzy5sY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
