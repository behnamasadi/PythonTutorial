{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "models with a large number of free parameters can describe\n",
    "an amazingly wide range of phenomena. Even if such a model agrees well with the available\n",
    "data, that doesn’t make it a good model. It may just mean there’s enough freedom in the\n",
    "model that it can describe almost any data set of the given size, without capturing any\n",
    "genuine insights into the underlying phenomenon. When that happens the model will work\n",
    "well for the existing data, but will fail to generalize to new situations.\n",
    "\n",
    "\n",
    "Regularization is a technique used in machine learning to prevent overfitting. Overfitting occurs when a model performs very well on training data but poorly on new, unseen data. Regularization introduces a penalty on the complexity of the model, effectively reducing the risk of overfitting.\n",
    "\n",
    "Here are some of the common regularization methods and a brief example of how they are applied in the context of linear regression:\n",
    "\n",
    "1. **L1 Regularization (Lasso Regression)**:\n",
    "    - It adds a penalty equivalent to the absolute value of the magnitude of the coefficients.\n",
    "    - Objective = Loss (e.g., Mean Squared Error) + λ * |w|\n",
    "\n",
    "2. **L2 Regularization (Ridge Regression)**:\n",
    "    - It adds a penalty equivalent to the square of the magnitude of the coefficients.\n",
    "    - Objective = Loss (e.g., Mean Squared Error) + λ * w^2\n",
    "\n",
    "3. **Elastic Net**:\n",
    "    - A combination of L1 and L2 regularization.\n",
    "    - Objective = Loss (e.g., Mean Squared Error) + λ1 * |w| + λ2 * w^2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear least squares\n",
    "$X_{m\\times n}{\\vec {\\beta_{n\\times 1} }}=Y_{m\\times 1}$\n",
    "\n",
    "${\\displaystyle L(D,{\\vec {\\beta }})=||X{\\vec {\\beta }}-Y||^{2}=(X{\\vec {\\beta }}-Y)^{T}(X{\\vec {\\beta }}-Y)=Y^{T}Y-Y^{T}X{\\vec {\\beta }}-{\\vec {\\beta }}^{T}X^{T}Y+{\\vec {\\beta }}^{T}X^{T}X{\\vec {\\beta }}}$\n",
    "\n",
    "\n",
    "${\\displaystyle {\\frac {\\partial L(D,{\\vec {\\beta }})}{\\partial {\\vec {\\beta }}}}={\\frac {\\partial \\left(Y^{T}Y-Y^{T}X{\\vec {\\beta }}-{\\vec {\\beta }}^{T}X^{T}Y+{\\vec {\\beta }}^{T}X^{T}X{\\vec {\\beta }}\\right)}{\\partial {\\vec {\\beta }}}}=-2X^{T}Y+2X^{T}X{\\vec {\\beta }}}$\n",
    "\n",
    "\n",
    "setting the gradient of the loss to zero and solving for ${\\displaystyle {\\vec {\\beta }}}$ we get: \n",
    "\n",
    "${\\displaystyle -2X^{T}Y+2X^{T}X{\\vec {\\beta }}=0\\Rightarrow X^{T}Y=X^{T}X{\\vec {\\beta }}\\Rightarrow {\\vec {\\hat {\\beta }}}=(X^{T}X)^{-1}X^{T}Y}{\\displaystyle -2X^{T}Y+2X^{T}X{\\vec {\\beta }}=0}$\n",
    "\n",
    "\n",
    "$\\Rightarrow X^{T}Y=X^{T}X{\\vec {\\beta }}\\Rightarrow {\\vec {\\hat {\\beta }}}=(X^{T}X)^{-1}X^{T}Y$\n",
    "\n",
    "## Tikhonov regularization (ridge regression) with L2 norm\n",
    "We add the magnitude of $\\beta$ to our cost to plenalize huge weights and keep the weights small (close to zero)  and all other things being equal. \n",
    "\n",
    "${\\displaystyle {\\hat {\\beta }}_{R}=(\\mathbf {X} ^{\\mathsf {T}}\\mathbf {X} +\\lambda \\mathbf {I} )^{-1}\\mathbf {X} ^{\\mathsf {T}}\\mathbf {y} }$\n",
    "\n",
    "$\\begin{eqnarray} C = -\\frac{1}{n} \\sum_{xj} \\left[ y_j \\ln a^L_j+(1-y_j) \\ln\n",
    "(1-a^L_j)\\right] + \\frac{\\lambda}{2n} \\sum_w w^2.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "The first term is the cross-entropy and the second term, is the squares of all the weights in the network. \n",
    "\n",
    "\n",
    "$\\begin{eqnarray} C = \\frac{1}{2n} \\sum_x \\|y-a^L\\|^2 +\n",
    "  \\frac{\\lambda}{2n} \\sum_w w^2.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "n both cases we can write the regularized cost function as:\n",
    "\n",
    "$\\begin{eqnarray}  C = C_0 + \\frac{\\lambda}{2n}\n",
    "\\sum_w w^2,\n",
    "\\end{eqnarray}$\n",
    "\n",
    "$C_0$ is the original, unregularized cost function.\n",
    "\n",
    "$\\lambda$: when $\\lambda$ is small we prefer to minimize the original cost function, but when $\\lambda$ is\n",
    "large we prefer small weights.\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "b_{new} = b -\\eta \\frac{\\partial C_0}{\\partial b}.\n",
    "\\end{eqnarray}$\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  w_{new}= & & w-\\eta \\frac{\\partial C_0}{\\partial\n",
    "    w}-\\frac{\\eta \\lambda}{n} w   & = & \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\eta \\frac{\\partial\n",
    "    C_0}{\\partial w}. \n",
    "\\end{eqnarray}$\n",
    "\n",
    "For stochastic gradient descent we can estimate $\\partial C_0 / \\partial w$ by averaging over a mini-batch of m training examples. Thus the regularized learning rule for stochastic gradient descent becomes:\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  w_{new}= \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\frac{\\eta}{m}\n",
    "  \\sum_x \\frac{\\partial C_x}{\\partial w}, \n",
    "\\end{eqnarray}$\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  b_{new} = b - \\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial b},\n",
    "\\end{eqnarray}$\n",
    "\n",
    "\n",
    "$n$ is, as usual, the size of our training set\n",
    "\n",
    "$m$ is size of the mini-batch training examples\n",
    "\n",
    "\n",
    "Heuristically, if the cost function is unregularized, then the length of the weight vector is likely to grow, all other things being equal. Over time this can lead to the weight vector being very large indeed. This can cause the weight vector to get stuck pointing in more or less the same direction, since changes due to gradient descent only make tiny changes to the direction, when the length is long, which is making it hard for our learning algorithm to properly explore the weight space, and consequently harder to find good minima of the cost function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/regularization.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso with L1 norm\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator)\n",
    "\n",
    "Refs: [1](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Regularization\n",
    "[1](https://www.youtube.com/watch?v=ATo7vnzy5sY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ElasticNet(alpha=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ElasticNet</label><div class=\"sk-toggleable__content\"><pre>ElasticNet(alpha=0.1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ElasticNet(alpha=0.1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate some sample data\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=0.1)\n",
    "\n",
    "# Lasso Regression (L1 regularization)\n",
    "lasso = Lasso(alpha=0.1)  # alpha here is equivalent to the lambda in the formula\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Ridge Regression (L2 regularization)\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X, y)\n",
    "\n",
    "# Elastic Net (Combination of L1 and L2)\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio controls the mixture of L1 vs. L2\n",
    "elastic_net.fit(X, y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the `alpha` parameter controls the amount of regularization: a larger value means more regularization. The `l1_ratio` parameter in `ElasticNet` determines the balance between L1 and L2 regularization.\n",
    "\n",
    "In practice, choosing the best regularization type and parameter often requires experimentation and cross-validation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
