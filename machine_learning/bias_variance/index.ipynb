{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let say your estimator function is: $h_{\\theta}(x)=\\theta_0+\\theta_1x+ \\theta_2x^2+...$\n",
    "\n",
    "You have to different types of error to plot during training:\n",
    "- Training error $J_{train}(\\theta)=\\frac{1}{2m}\\sum_{i}^{m} ( h_{\\theta}(x^{(i)}) -y^{(i)} )^2 $\n",
    "- Cross validation error $J_{cv}(\\theta)=\\frac{1}{2m}\\sum_{i}^{m}( h_{\\theta}(x_{cv}^{(i)}) -y_{cv}^{(i)} )^2 $\n",
    "\n",
    "## High Bias (Underfitting)\n",
    "If you have a naive estimator (using low order of polynomial i.e. a line or quadratic function) which tries to estimate underlying unknown function you will have high error on both training set and validation set. By adding more and more training set your error will increase. Also your cross validation will be large but it will be close to your training error. Adding more items to your trainingset will not solve your problem and because you have **high bias (underfitting)** error problem.\n",
    "\n",
    "High bias can cause an algorithm to miss the relevant relations between features and target outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/high_bias.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Variance (Overfitting)\n",
    "If your estimator use a complex model that doesn't properly generalize the structure of the unknown underlying function which it tries to estimate, your training error will be very small but your cross validation error will be big and there will be a large gap between your cross validation error and training (Overfitting problem).\n",
    "\n",
    "<img src='images/high_variance.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias of an estimator is its average error for different training sets. The variance of an estimator indicates how sensitive it is to varying training sets.\n",
    "\n",
    "So you have to find a suit spot between simple model and coplex model, such that you have a model with low variance and low bias. Main available approchaedare:\n",
    "- Regularization\n",
    "- Bagging\n",
    "- Booting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "combines \"strong\" learners in a way that reduces their variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Booting\n",
    "combines many \"weak\" (high bias) models in an ensemble that has lower bias than the individual models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs: [1](https://www.youtube.com/watch?v=ISBGFY-gBug), [2](https://www.youtube.com/watch?v=EuBBz3bI-aA,), [3](https://www.youtube.com/watch?v=iuJgyiS7BKM), [4](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), [5](https://www.youtube.com/watch?v=lpkSGTT8uMg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
