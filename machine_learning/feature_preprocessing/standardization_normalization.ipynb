{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization Vs Normalization\n",
    "\n",
    "## Normalization (min-max Normalization or Feature Scaling)\n",
    "Normalization rescales the values into a range of [0,1]. This might be useful in some cases where all parameters need to have the same positive scale.\n",
    "\n",
    "$X_{norm}=\\frac{X-X_{min}}{X_{max}-X_{min}}$\n",
    "\n",
    "\n",
    "Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization (Z-Score Normalization)\n",
    "Scaling to normal distribution $\\mu=0$ and $\\sigma^2=1$\n",
    "\n",
    "$X_{standard}=\\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does **not** have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects\n",
    "\n",
    "In theory, regression is insensitive to standardization since any linear transformation of input data can be counteracted by adjusting model parameters.\n",
    "\n",
    "Despite the fact that in theroy standardization plays little role in regression, it is used in regression because of the followings:\n",
    "\n",
    "1) Standardization improves the numerical stability of your model\n",
    "\n",
    "2) Standardization may speed up the training process\n",
    "if different features have drastically different ranges, the learning rate is determined by the feature with the largest range. This leads to another advantage of standardization: speeds up the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch allows us to normalize our dataset using the standardization process we've just seen by passing in the mean and standard deviation values for each color channel to the Normalize() transform.\n",
    "\n",
    "torchvision.transforms.Normalize(\n",
    "      [meanOfChannel1, meanOfChannel2, meanOfChannel3] \n",
    "    , [stdOfChannel1, stdOfChannel2, stdOfChannel3] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs [1](https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0), [2](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/), [3](https://en.wikipedia.org/wiki/Correlation_and_dependence), [4](https://deeplizard.com/learn/video/lu7TCu7HeYc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
