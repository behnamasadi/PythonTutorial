{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83663cf9-a770-4e1f-bcd2-21e93e7a6543",
   "metadata": {},
   "source": [
    "\n",
    "### **Hidden Markov Model (HMM) Overview**\n",
    "An HMM consists of:\n",
    "1. **States**: $ S = \\{S_1, S_2\\} $ (e.g., \"Rainy\" and \"Sunny\").\n",
    "2. **Observations**: $ O = \\{o_1, o_2, o_3\\} $ (e.g., \"Walk\", \"Shop\", \"Clean\").\n",
    "3. **Transition Probabilities**: $ A = \\{a_{ij}\\} $, the probability of transitioning from state $ S_i $ to state $ S_j $.\n",
    "4. **Emission Probabilities**: $ B = \\{b_{i}(o_k)\\} $, the probability of observation $ o_k $ given state $ S_i $.\n",
    "5. **Initial Probabilities**: $ \\pi = \\{\\pi_i\\} $, the probability of starting in state $ S_i $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "\n",
    "#### States\n",
    "- $ S = \\{\\text{Rainy, Sunny}\\} $\n",
    "\n",
    "#### Observations\n",
    "- $ O = \\{\\text{Walk, Shop, Clean}\\} $\n",
    "\n",
    "#### Parameters\n",
    "- Initial probabilities:\n",
    "  $\n",
    "  \\pi = \\{P(\\text{Rainy}) = 0.6, P(\\text{Sunny}) = 0.4\\}\n",
    "  $\n",
    "- Transition probabilities:\n",
    "  $\n",
    "  A = \n",
    "  \\begin{bmatrix}\n",
    "  P(\\text{Rainy} \\to \\text{Rainy}) = 0.7 & P(\\text{Rainy} \\to \\text{Sunny}) = 0.3 \\\\\n",
    "  P(\\text{Sunny} \\to \\text{Rainy}) = 0.4 & P(\\text{Sunny} \\to \\text{Sunny}) = 0.6\n",
    "  \\end{bmatrix}\n",
    "  $\n",
    "- Emission probabilities:\n",
    "  $\n",
    "  B = \n",
    "  \\begin{bmatrix}\n",
    "  P(\\text{Walk}|\\text{Rainy}) = 0.1 & P(\\text{Shop}|\\text{Rainy}) = 0.4 & P(\\text{Clean}|\\text{Rainy}) = 0.5 \\\\\n",
    "  P(\\text{Walk}|\\text{Sunny}) = 0.6 & P(\\text{Shop}|\\text{Sunny}) = 0.3 & P(\\text{Clean}|\\text{Sunny}) = 0.1\n",
    "  \\end{bmatrix}\n",
    "  $\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Equation**\n",
    "\n",
    "The probability of a sequence of observations $ O = \\{o_1, o_2, \\ldots, o_T\\} $ given the model $ \\lambda = (\\pi, A, B) $ is computed using the **forward algorithm**:\n",
    "\n",
    "$\n",
    "P(O|\\lambda) = \\sum_{S} \\pi_{S_1} b_{S_1}(o_1) \\prod_{t=2}^T a_{S_{t-1}, S_t} b_{S_t}(o_t)\n",
    "$\n",
    "\n",
    "For example, if $ O = \\{\\text{Walk, Shop}\\} $:\n",
    "- Compute all possible state sequences, $ \\{\\text{Rainy} \\to \\text{Rainy}\\}, \\{\\text{Rainy} \\to \\text{Sunny}\\}, $ etc.\n",
    "- Multiply the initial, transition, and emission probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### **Graph Representation**\n",
    "- **Nodes**: Represent states $ S $.\n",
    "- **Edges**: Represent transitions $ A $.\n",
    "- **Observation Emissions**: Emissions $ B $ are attached to the nodes.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Here is the graphical representation of the Hidden Markov Model (HMM). The graph shows:\n",
    "\n",
    "1. **States** as nodes: \"Rainy\" and \"Sunny\".\n",
    "2. **Transitions** as directed edges with associated probabilities (e.g., $ P(\\text{Rainy} \\to \\text{Sunny}) = 0.3 $).\n",
    "3. **Emission probabilities** as text annotations near each state.\n",
    "\n",
    "<img src=\"images/hmm.png\" height=\"50%\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ff7e6-6364-4b4f-972f-89c15f8ec1a7",
   "metadata": {},
   "source": [
    "## Bayesian Network for HMM\n",
    "\n",
    "\n",
    "\n",
    "A Bayesian Network representation of a Hidden Markov Model (HMM) illustrates how the hidden states and observations are related over time. \n",
    "\n",
    "---\n",
    "\n",
    "### **Structure of a Bayesian Network for HMM**\n",
    "1. **Hidden States ($S_t$)**: Represent the latent variables.\n",
    "2. **Observations ($O_t$)**: Depend on the corresponding hidden state.\n",
    "3. **Transition Dependencies**:\n",
    "   - $S_t$ depends on $S_{t-1}$ (transition probability).\n",
    "4. **Emission Dependencies**:\n",
    "   - $O_t$ depends only on $S_t$ (emission probability).\n",
    "\n",
    "The Bayesian Network for an HMM can be represented with directed edges:\n",
    "- $S_{t-1} \\to S_t$: Transition relationship.\n",
    "- $S_t \\to O_t$: Emission relationship.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "For $T=3$ time steps:\n",
    "1. States: $ S_1, S_2, S_3 $.\n",
    "2. Observations: $ O_1, O_2, O_3 $.\n",
    "3. Directed edges:\n",
    "   - $ S_1 \\to S_2 \\to S_3 $ (state transitions).\n",
    "   - $ S_1 \\to O_1, S_2 \\to O_2, S_3 \\to O_3 $ (state-to-observation dependencies).\n",
    "\n",
    "---\n",
    "\n",
    "I'll create a graphical representation of this Bayesian Network.\n",
    "\n",
    "Here is the Bayesian Network representation of a Hidden Markov Model (HMM). \n",
    "\n",
    "### Key Features:\n",
    "1. **Hidden States ($S_1, S_2, S_3$)**: Shown as nodes in the top layer.\n",
    "2. **Observations ($O_1, O_2, O_3$)**: Shown as nodes in the bottom layer.\n",
    "3. **Directed Edges**:\n",
    "   - Between states ($S_t \\to S_{t+1}$) represent transition probabilities.\n",
    "   - Between states and observations ($S_t \\to O_t$) represent emission probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f6a58c-a13b-46e6-91ea-23fe10a2f1d7",
   "metadata": {},
   "source": [
    " <img src=\"images/bayesian_network_representation_hmm.png\" height=\"50%\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca64c54-95c9-44a0-b782-5172cb7a18de",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Algorithms for HMMs**\n",
    "1. **Forward Algorithm**\n",
    "   - Calculates the probability of an observed sequence given the model.\n",
    "   - Used for likelihood computation.\n",
    "\n",
    "2. **Backward Algorithm**\n",
    "   - Computes probabilities of future observations given the current state.\n",
    "   - Complements the forward algorithm for efficient calculations.\n",
    "\n",
    "3. **Forward-Backward Algorithm**\n",
    "   - A combination of forward and backward algorithms.\n",
    "   - Used in Expectation-Maximization (EM) for parameter re-estimation.\n",
    "\n",
    "4. **Viterbi Algorithm**\n",
    "   - Finds the most likely sequence of hidden states (decoding).\n",
    "   - Dynamic programming approach.\n",
    "\n",
    "5. **Baum-Welch Algorithm**\n",
    "   - A special case of Expectation-Maximization (EM) for HMMs.\n",
    "   - Used for training and parameter estimation from observed data.\n",
    "\n",
    "6. **Sampling Techniques**\n",
    "   - Gibbs Sampling\n",
    "   - Particle Filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d9626-7458-4302-9a5d-1f786a2f68be",
   "metadata": {},
   "source": [
    "### **Maximum A-Posteriori (MAP) Inference**\n",
    "\n",
    "MAP inference aims to find the most probable sequence of hidden states $S = \\{S_1, S_2, \\ldots, S_T\\}$ given a sequence of observations $O = \\{O_1, O_2, \\ldots, O_T\\}$ in a Hidden Markov Model (HMM). \n",
    "\n",
    "---\n",
    "\n",
    "### **MAP Formula**\n",
    "\n",
    "MAP is based on Bayes' theorem:\n",
    "\n",
    "$\n",
    "S^* = \\arg\\max_{S} P(S | O)\n",
    "$\n",
    "\n",
    "Using Bayes' rule:\n",
    "\n",
    "$\n",
    "P(S | O) = \\frac{P(S)P(O | S)}{P(O)}\n",
    "$\n",
    "\n",
    "Since $P(O)$ is constant for all $S$, the objective becomes:\n",
    "\n",
    "$\n",
    "S^* = \\arg\\max_{S} P(S)P(O | S)\n",
    "$\n",
    "\n",
    "Breaking this down using the HMM structure:\n",
    "- $P(S)$: Depends on the transition probabilities ($P(S_t | S_{t-1})$).\n",
    "- $P(O | S)$: Depends on the emission probabilities ($P(O_t | S_t)$).\n",
    "\n",
    "Thus:\n",
    "\n",
    "$\n",
    "S^* = \\arg\\max_{S} \\prod_{t=1}^{T} P(S_t | S_{t-1}) \\cdot P(O_t | S_t)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **MAP Using the Viterbi Algorithm**\n",
    "\n",
    "To efficiently compute $S^*$, the **Viterbi algorithm** is used:\n",
    "1. **Initialization**:\n",
    "   $\n",
    "   V_1(s) = P(s_1 = s) \\cdot P(O_1 | s)\n",
    "   $\n",
    "   For each state $s$.\n",
    "\n",
    "2. **Recursion**:\n",
    "   For $t = 2, \\ldots, T$, compute:\n",
    "   $\n",
    "   V_t(s) = \\max_{s'} \\left[ V_{t-1}(s') \\cdot P(s | s') \\cdot P(O_t | s) \\right]\n",
    "   $\n",
    "   Store the maximizing state $s'$ in a backpointer table.\n",
    "\n",
    "3. **Termination**:\n",
    "   $\n",
    "   S_T^* = \\arg\\max_s V_T(s)\n",
    "   $\n",
    "\n",
    "4. **Backtracking**:\n",
    "   Use the backpointer table to reconstruct the most likely sequence $S^*$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "#### Given:\n",
    "- States: $S = \\{\\text{Rainy}, \\text{Sunny}\\}$\n",
    "- Observations: $O = \\{\\text{Walk, Shop, Clean}\\}$\n",
    "- Initial probabilities, transition probabilities, and emission probabilities as described earlier.\n",
    "\n",
    "#### Task:\n",
    "Find the most probable state sequence $S^*$ for $O = \\{\\text{Walk, Shop, Clean}\\}$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b459a6-4a0a-4166-a72b-5dfc3317927b",
   "metadata": {},
   "source": [
    "### Robotic Example for Maximum A-Posteriori (MAP) Inference\n",
    "\n",
    "Consider a robot navigating through three discrete states ($X_1, X_2, X_3$) with measurements ($Z_1, Z_2, Z_3$) observed at each state. The goal is to determine the most probable sequence of states given the measurements using MAP inference.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Description**\n",
    "\n",
    "1. **States**: $X = \\{X_1, X_2, X_3\\}$\n",
    "   - Represents the discrete positions of the robot.\n",
    "2. **Measurements**: $Z = \\{Z_1, Z_2, Z_3\\}$\n",
    "   - Observations (e.g., sensor readings) recorded at each state.\n",
    "3. **Transition Probabilities** ($P(X_t | X_{t-1})$):\n",
    "   - Probability of transitioning from one state to another.\n",
    "4. **Emission Probabilities** ($P(Z_t | X_t)$):\n",
    "   - Probability of observing a measurement $Z_t$ given the robot is in state $X_t$.\n",
    "5. **Initial Probabilities** ($P(X_1)$):\n",
    "   - Probability of starting in each state.\n",
    "\n",
    "---\n",
    "\n",
    "### **Parameters**\n",
    "\n",
    "#### Transition Probabilities:\n",
    "$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "P(X_1 \\to X_1) = 0.6 & P(X_1 \\to X_2) = 0.3 & P(X_1 \\to X_3) = 0.1 \\\\\n",
    "P(X_2 \\to X_1) = 0.2 & P(X_2 \\to X_2) = 0.5 & P(X_2 \\to X_3) = 0.3 \\\\\n",
    "P(X_3 \\to X_1) = 0.1 & P(X_3 \\to X_2) = 0.3 & P(X_3 \\to X_3) = 0.6\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "#### Emission Probabilities:\n",
    "$\n",
    "B = \n",
    "\\begin{bmatrix}\n",
    "P(Z_1 | X_1) = 0.7 & P(Z_2 | X_1) = 0.2 & P(Z_3 | X_1) = 0.1 \\\\\n",
    "P(Z_1 | X_2) = 0.3 & P(Z_2 | X_2) = 0.5 & P(Z_3 | X_2) = 0.2 \\\\\n",
    "P(Z_1 | X_3) = 0.2 & P(Z_2 | X_3) = 0.4 & P(Z_3 | X_3) = 0.4\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "#### Initial Probabilities:\n",
    "$\n",
    "\\pi = \\{P(X_1) = 0.5, P(X_2) = 0.3, P(X_3) = 0.2\\}\n",
    "$\n",
    "\n",
    "#### Observations:\n",
    "$\n",
    "Z = \\{Z_1, Z_2, Z_3\\}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **MAP Inference Using Viterbi Algorithm**\n",
    "\n",
    "#### Step 1: **Initialization**\n",
    "For each state:\n",
    "$\n",
    "V_1(X_i) = P(X_i) \\cdot P(Z_1 | X_i)\n",
    "$\n",
    "\n",
    "#### Step 2: **Recursion**\n",
    "For $t = 2, 3, \\ldots, T$:\n",
    "$\n",
    "V_t(X_i) = \\max_{X_{t-1}} \\left[ V_{t-1}(X_{t-1}) \\cdot P(X_i | X_{t-1}) \\cdot P(Z_t | X_i) \\right]\n",
    "$\n",
    "Store the backpointer for each state.\n",
    "\n",
    "#### Step 3: **Termination**\n",
    "$\n",
    "S_T^* = \\arg\\max_{X_i} V_T(X_i)\n",
    "$\n",
    "\n",
    "#### Step 4: **Backtracking**\n",
    "Use the backpointer table to reconstruct the most probable sequence of states.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de66ea-fc16-49bb-9cc8-7f1ec78d167b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
