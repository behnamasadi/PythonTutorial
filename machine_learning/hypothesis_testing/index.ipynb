{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550c4629",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "\n",
    "When do you need a hypothesis? Whenever you want to prove or draw a conclusion about the population with a sample of that population.\n",
    "\n",
    "\n",
    "So we make a sample from a population, if you only want to describe that sample you use **descriptive statistic**, and you calculate for instance mean or standard deviation in order to describe the sample, but if we want to make a statement about the whole population, so we use our sample and with Hypothesis Testing we can infer the population, so the goal of Hypothesis Testing is use sample from a population to test a hypothesis about the population.\n",
    "\n",
    "When you want to formulate hypothesis there are always two hypothesis that claim the opposite\n",
    "\n",
    "## Null Hypothesis\n",
    "\n",
    "**Null Hypothesis ($ H_0 $)**: This is a statement that there is no effect or no difference. It's what you're trying to test against. For instance there is no difference between **Drug A** and **Drug B**.\n",
    "\n",
    "## Alternative Hypothesis \n",
    "**Alternative Hypothesis ($ H_a $ or $ H_1 $)**: This is what you want to prove, e.g., that there is a difference or an effect.\n",
    "For instance, **Drug A** is superior to Drug B.\n",
    "\n",
    "\n",
    "Hypothesis testing can only determine with a probability of error whether a Hypothesis is accepted or rejected.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018b92a",
   "metadata": {},
   "source": [
    "## P values\n",
    "\n",
    "The **$ p $-value** used in statistics to measure how surprising or unlikely your data is, under  null hypothesis.\n",
    "\n",
    "### The intuition behind P values\n",
    "Imagine you have a coin and you suspect it might be biased towards heads. To test this, you decide to flip the coin 100 times. Your null hypothesis (the assumption you're testing against) is that the coin is fair, meaning it has an equal chance of landing heads or tails.\n",
    "\n",
    "After flipping the coin 100 times, suppose you get an unusually high number of heads, say 70 heads and 30 tails. You might start to think this result is pretty strange if the coin were truly fair.\n",
    "\n",
    "Here's where the **$ p $-value** comes in. The **$ p $-value** is a number between 0 and 1 that tells you how likely it is to see a result as extreme as yours (or more extreme) if the null hypothesis were true. In our example, if the **$ p $-value** is very low (let's say 0.01), it means that getting 70 heads out of 100 flips would be very unlikely if the coin were fair. A low **$ p $-value** suggests that maybe your assumption (the null hypothesis) that the coin is fair might not be right.\n",
    "\n",
    "\n",
    "- A low **$ p $-value** (typically, a threshold like 0.05 or 5% is used), it suggests that the observed data is inconsistent with the null hypothesis, so you might reject the null hypothesis in favor of the alternative hypothesis (which is the hypothesis that there is an effect or a difference).\n",
    "- A high **$ p $-value** means you don't have enough statistical evidence to reject the null hypothesis.\n",
    "\n",
    "However, a low p-value doesn't prove the alternative hypothesis is true. It only suggests that the data you observed are unlikely under the assumption that the null hypothesis is true. Other factors, like the design of the experiment and assumptions of the statistical test, also play critical roles in the interpretation of **$ p $-value**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c9556f",
   "metadata": {},
   "source": [
    "### Technical way of describing the p-value\n",
    "\n",
    "The **$ p $-value** is the probability of observing a test statistic as extreme as, or more extreme than, the statistic computed from the data, assuming that the null hypothesis is true. \n",
    "\n",
    "\n",
    "**Test Statistic**: \n",
    "Test statistics are numerical values used in statistical testing to decide whether to reject the null hypothesis. The choice of test statistic depends on the type of data and the hypothesis being tested. Here's a list of common test statistics used in various statistical tests:\n",
    "\n",
    "1. **Z-statistic**: Used in Z-tests when testing hypotheses concerning population proportions or means, particularly when the sample size is large and the population variance is known.\n",
    "\n",
    "2. **T-statistic**: Used in t-tests when testing hypotheses about means, especially when the population variance is unknown and the sample size is small. There are different forms of t-tests, including one-sample, two-sample, and paired t-tests, each with its own t-statistic formula.\n",
    "\n",
    "3. **Chi-square statistic $\\chi^2$**: Employed in chi-square tests for independence, goodness of fit, or homogeneity. It tests hypotheses about frequency counts of categorical data to see if observed frequencies differ from expected frequencies.\n",
    "\n",
    "4. **F-statistic**: Used in ANOVA (Analysis of Variance) tests to compare the variances across multiple groups to see if at least one sample mean differs significantly from others.\n",
    "\n",
    "5. **U-statistic**: Utilized in Mann-Whitney U tests (a non-parametric test) to compare differences between two independent groups when the assumption of normality is not met.\n",
    "\n",
    "For instance, in our coin flip example, the test statistic could be the number of heads observed.\n",
    "\n",
    "\n",
    "\n",
    "**Observing a Test Statistic as Extreme as, or More Extreme Than, the Statistic Computed from the Data**: comparing what you actually observed in your experiment or study to what you would expect under the null hypothesis. \"As extreme as, or more extreme than\" refers to outcomes that are at least as unlikely as the actual outcome you got, given the null hypothesis is true.\n",
    "\n",
    "**Assuming That the Null Hypothesis is True**: The calculation of the p-value is done under the assumption that the null hypothesis is correct. This is crucial because the p-value is meant to test the strength of evidence against the null hypothesis.\n",
    "\n",
    "**Probability**: The **$ p $-value** itself is a probability. It measures the likelihood of observing your actual test statistic (or one more extreme) purely by chance if the null hypothesis were true. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3124df24",
   "metadata": {},
   "source": [
    "### t-test\n",
    "The t-test is a statistical test used to determine if there is a significant difference between the means of two groups, which may be related in certain features. It's widely used in hypothesis testing to assess the significance of the differences between two sample means. There are several types of t-tests, including:\n",
    "\n",
    "1. **One-sample t-test** compares the mean of a single group against a known mean. For instance a chocklate factory claim that its chockale br is 50 grams, We gather 30 bars and calculated the mean value which is 48 grams and we can compare it to the 50 grams.\n",
    "\n",
    "2. **Two-sample (independent) t-test** compares the means of two independent or unrelated groups to determine if there is a significant difference between them. Example: effectiveness of two painkillers on two groups of patient, compring the mean value of the time that painkiller took to effect.\n",
    "\n",
    "3. **Paired t-test** compares the means from the same group at different times (say, one year apart), or the means from two groups that are somehow related or paired. For example effectiveness of a diet, we calculated the mean weight of participants before a diet and after the diet. \n",
    "\n",
    "### Formula for Two-sample (independent) t-test\n",
    "\n",
    "The formula to calculate the t-value in an independent t-test is:\n",
    "\n",
    "$ t = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{ \\frac{s_1^2}{N_1} + \\frac{s_2^2}{N_2} }} $\n",
    "\n",
    "where:\n",
    "- $ \\bar{X}_1 $ and $ \\bar{X}_2 $ are the sample means,\n",
    "- $ s_1^2 $ and $ s_2^2 $ are the sample variances,\n",
    "- $ N_1 $ and $ N_2 $ are the sample sizes.\n",
    "\n",
    "The degrees of freedom (df) for this test is $ df = N_1 + N_2 - 2 $.\n",
    "\n",
    "After calculating the t-value, you compare it against a critical value from the t-distribution table based on your chosen significance level (alpha, typically 0.05) and degrees of freedom to determine if the difference is statistically significant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c989d6",
   "metadata": {},
   "source": [
    "## Example Drug A and Drug B\n",
    "\n",
    "Imagine you have **Drug A** and **Drug B** and you test them on two patients, can we say because it worked on **patient1** and didn't work on **patient2** it is working? There might be several factors that contributed to that result. So now let's try it on $2000$ patients and **Drug A** cured $97\\%$ of people while **Drug B** cured only $3\\%$, so now the chance the result was random and there is no difference between them is unrealistic. Now imagine the success of **Drug A** is $37%$ and **Drug B** is  $31%$ on $50$ patients.\n",
    "\n",
    "So given that no study is perfect and there are always a few random things that change the result, how can we become confident that Drug A is superior?\n",
    "\n",
    "That's where the **$ p $-value** comes in. **$ p $-value** are numbers between  0 and 1 and quantify how confident we should be **Drug A** is different from **Drug B**.\n",
    "The closer a **$ p $-value** is to 0 the more confident we are that **Drug A** and **Drug B** are different. \n",
    "\n",
    "\n",
    "In practice, the commonly used threshold is $0.05$, meaning if there is no difference between **Drug A** and **Drug B**, and we did the exact same experiment then only $5\\%$ of those experiments would result is the wrong decision. Now let's repeat the experiment repeatedly, and we get the following (**$ p $-value** calculated using the Fisher test):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d133df9",
   "metadata": {},
   "source": [
    "| Drug A  |           | Drug B |           | p-value |\n",
    "|---------|-----------|--------|-----------|---------|\n",
    "| Cured   | Not Cured | Cured  | Not Cured |         |\n",
    "| 73      | 125       | 71     | 127       | 0.9     |\n",
    "| 71      | 127       | 72     | 126       | 1.0     |\n",
    "| 75      | 123       | 70     | 128       | 0.7     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a882a915",
   "metadata": {},
   "source": [
    "## Example Effects of a new Fertilizer on Plant Growth\n",
    "\n",
    "Imagine you're a botanist studying the effects of a new fertilizer on plant growth. You have two groups of plants:\n",
    "\n",
    "1. **Control Group**: Plants not given the fertilizer.\n",
    "2. **Treatment Group**: Plants given the fertilizer.\n",
    "\n",
    "You want to know if the fertilizer has a significant effect on plant growth. To do this, you measure the height of the plants after a fixed period.\n",
    "\n",
    "**Hypotheses**:\n",
    "- $ H_0 $: The fertilizer has no effect on plant growth. (Mean height of Control Group = Mean height of Treatment Group)\n",
    "- $ H_a $: The fertilizer has an effect on plant growth. (Mean height of Control Group ≠ Mean height of Treatment Group)\n",
    "\n",
    "**Data**:\n",
    "Let's assume you measured the height (in cm) of 10 plants from each group:\n",
    "\n",
    "- Control Group: [15, 17, 16, 14, 15, 16, 17, 15, 16, 17]\n",
    "- Treatment Group: [18, 19, 20, 19, 18, 21, 19, 20, 19, 18]\n",
    "\n",
    "We'll use a two-sample t-test to determine if there's a significant difference in the means of the two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1fe46f",
   "metadata": {},
   "source": [
    "To calculate the p-value using a t-test for comparing the means of the control group and the treatment group, we'll go through the following steps:\n",
    "\n",
    "1. **Calculate the mean** of each group.\n",
    "2. **Calculate the standard deviation** of each group.\n",
    "3. **Calculate the standard error of the mean (SEM)** for each group.\n",
    "4. **Calculate the t-statistic** using the means, SEMs, and sample sizes of both groups.\n",
    "5. **Calculate the degrees of freedom** needed to look up the p-value.\n",
    "6. **Calculate the p-value** based on the t-statistic and degrees of freedom.\n",
    "\n",
    "\n",
    "\n",
    "For a two-tailed test (which checks for any difference between the means, not specifying direction), the p-value can be conceptualized as:\n",
    "\n",
    "$ \\text{p-value} = 2 \\times (1 - \\text{CDF}(t, df)) $\n",
    "\n",
    "Where:\n",
    "- $ \\text{CDF} $ refers to the cumulative distribution function for the t-distribution.\n",
    "- $ t $ is the observed t-statistic calculated from your data.\n",
    "- $ df $ are the degrees of freedom, which, for an independent samples t-test, are usually calculated as $ n_1 + n_2 - 2 $ for equal variances, or using a more complex formula for unequal variances and sample sizes.\n",
    "\n",
    "This formula is calculating the probability of observing a t-statistic as extreme as, or more extreme than, the observed t-statistic under the null hypothesis. The \"2 ×\" part accounts for both tails of the distribution since we're interested in differences in either direction (higher or lower).\n",
    "\n",
    "In practice, this calculation is not done manually but through  software. These functions internally use the properties of the t-distribution to find the p-value corresponding to the calculated t-statistic and the degrees of freedom for your specific test scenario.\n",
    "\n",
    "\n",
    "\n",
    "### Control Group\n",
    "- **Mean**: 15.8 cm\n",
    "- **Standard Deviation**: 1.033 cm\n",
    "- **Standard Error of the Mean (SEM)**: 0.327 cm\n",
    "\n",
    "### Treatment Group\n",
    "- **Mean**: 19.1 cm\n",
    "- **Standard Deviation**: 0.994 cm\n",
    "- **Standard Error of the Mean (SEM)**: 0.314 cm\n",
    "\n",
    "### T-test Results\n",
    "- **T-statistic**: -7.279\n",
    "- **P-value**: approximately 0.0000009162\n",
    "\n",
    "The T-statistic is -7.279, which indicates a significant difference between the control and treatment groups, given the very low P-value (less than 0.001). This means there is a statistically significant difference in plant height between the control group and the treatment group, favoring the hypothesis that the new fertilizer has a positive effect on plant growth.\n",
    "\n",
    "\n",
    "\n",
    "Refs [1](https://www.youtube.com/watch?v=vemZtEM63GY), [2](https://www.youtube.com/watch?v=udyAvvaMjfM), [3](https://www.youtube.com/watch?v=p0W1oKPP6eQ), [4](https://www.youtube.com/watch?v=0oc49DyA3hU), [5](https://www.youtube.com/watch?v=JQc3yx0-Q9E), [6](https://www.youtube.com/watch?v=5koKb5B_YWo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11e88a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.8,\n",
       " 1.0327955589886444,\n",
       " 19.1,\n",
       " 0.9944289260117533,\n",
       " 0.3265986323710904,\n",
       " 0.31446603773522014,\n",
       " -7.278624758728698,\n",
       " 9.162003368633656e-07)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "control_group = np.array([15, 17, 16, 14, 15, 16, 17, 15, 16, 17])\n",
    "treatment_group = np.array([18, 19, 20, 19, 18, 21, 19, 20, 19, 18])\n",
    "\n",
    "# Step 1 & 2: Calculate mean and standard deviation\n",
    "mean_control = np.mean(control_group)\n",
    "std_dev_control = np.std(control_group, ddof=1)  # Sample standard deviation\n",
    "mean_treatment = np.mean(treatment_group)\n",
    "std_dev_treatment = np.std(treatment_group, ddof=1)  # Sample standard deviation\n",
    "\n",
    "# Step 3: Calculate the Standard Error of the Mean (SEM) for each group\n",
    "n_control = len(control_group)\n",
    "n_treatment = len(treatment_group)\n",
    "sem_control = std_dev_control / np.sqrt(n_control)\n",
    "sem_treatment = std_dev_treatment / np.sqrt(n_treatment)\n",
    "\n",
    "# Step 4 & 5: Calculate t-statistic and degrees of freedom\n",
    "# Using scipy to calculate t-statistic and p-value directly\n",
    "t_stat, p_value = ttest_ind(control_group, treatment_group)\n",
    "\n",
    "mean_control, std_dev_control, mean_treatment, std_dev_treatment, sem_control, sem_treatment, t_stat, p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd992cf-05a6-4904-810e-7266d410f5d5",
   "metadata": {},
   "source": [
    "## Hypothesis testing in deep learning\n",
    "Hypothesis testing in deep learning can be applied in several scenarios, especially when you're interested in comparing models, understanding the significance of model improvements, or analyzing the behavior of models under specific conditions. Here are some common use cases:\n",
    "\n",
    "1. **Model Comparison**: When you have two or more different models or approaches and want to determine if one model is significantly better than the others. Hypothesis testing can help you assess if the differences in performance metrics (like accuracy, precision, recall) are statistically significant or just due to random chance.\n",
    "\n",
    "2. **Feature Importance**: To evaluate the impact of certain features on the model's predictions. Hypothesis testing can be used to determine if removing or adding a specific feature significantly affects the model's performance, helping in feature selection and model simplification.\n",
    "\n",
    "3. **Regularization and Hyperparameter Tuning**: When adjusting model hyperparameters (like learning rate, dropout rate, or regularization strength), hypothesis testing can help in determining if the changes in the hyperparameters lead to a statistically significant improvement or degradation in model performance.\n",
    "\n",
    "4. **Transfer Learning and Domain Adaptation**: In scenarios involving transfer learning or domain adaptation, where a model trained on one domain is adapted for use in another, hypothesis testing can be used to assess if the adaptation leads to significant improvements in performance on the new domain.\n",
    "\n",
    "5. **Fairness and Bias Assessment**: Hypothesis testing can be instrumental in identifying biases in model predictions across different groups or demographics. It can help in determining if disparities in model outcomes are statistically significant, which is crucial for developing fair and unbiased models.\n",
    "\n",
    "6. **A/B Testing**: In the deployment phase, especially for models integrated into products or services, A/B testing with hypothesis testing can evaluate the real-world impact of using one model version over another on user behavior or other key performance indicators (KPIs).\n",
    "\n",
    "7. **Robustness and Generalization**: To test the robustness of models against variations in input data, including adversarial examples, noise, or data from different distributions. Hypothesis testing can help determine if a model is significantly more robust or generalizes better to unseen data compared to others.\n",
    "\n",
    "8. **Time Series and Sequence Models**: For models dealing with time series or sequential data, hypothesis testing can be used to assess the significance of temporal features or the impact of different sequence modeling techniques (like RNNs, GRUs, or LSTMs) on prediction accuracy.\n",
    "\n",
    "In practice, implementing hypothesis testing in deep learning involves choosing the right statistical test based on the data distribution and experiment design, defining null and alternative hypotheses, calculating the test statistic, and interpreting the p-value to make decisions. It's a powerful tool to add rigor and confidence to the conclusions drawn from deep learning experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
