{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf70462-b9a5-4174-b6be-aff425c29f1f",
   "metadata": {},
   "source": [
    "###  **What is a p-value?**\n",
    "\n",
    "A **p-value** is a number between **0 and 1** that helps us decide **how confident we can be that a result is not due to random chance**.\n",
    "\n",
    "It comes into play when comparing two things—like **drug A vs. drug B**—and asking:\n",
    "\n",
    "> “Is the difference in outcomes real, or could it just be due to random variation?”\n",
    "\n",
    "---\n",
    "\n",
    "### **Example from the video**\n",
    "\n",
    "1. Initially, Drug A cures 1 person, and Drug B doesn’t cure another.\n",
    "\n",
    "   * Can we say Drug A is better? **No.** Too small a sample.\n",
    "2. Eventually, we try both drugs on **a lot of people**:\n",
    "\n",
    "   * Drug A cures **1043/1046** (≈ 99.7%)\n",
    "   * Drug B cures **2/1434** (≈ 0.1%)\n",
    "\n",
    "Now, the difference is **too large** to be due to chance. We say Drug A is better.\n",
    "\n",
    "But what if the numbers were:\n",
    "\n",
    "* Drug A: **37% cured**\n",
    "* Drug B: **31% cured**\n",
    "\n",
    "Now it’s **less clear**. There is a difference, but **is it statistically significant**? That’s where the **p-value** helps.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Interpreting the p-value**\n",
    "\n",
    "* A **small p-value** (close to 0) means:\n",
    "\n",
    "  * It's **unlikely** the observed difference is due to chance.\n",
    "  * We can **reject the null hypothesis** (which says \"no difference\").\n",
    "\n",
    "* A **large p-value** (close to 1) means:\n",
    "\n",
    "  * The results are **likely due to chance**.\n",
    "  * We **fail to reject the null hypothesis**.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Common Threshold (α level)**\n",
    "\n",
    "The most common threshold is **0.05**:\n",
    "\n",
    "* If **p < 0.05**, we say the difference is **statistically significant**.\n",
    "* This means there's less than a **5% chance** that the observed difference is due to randomness.\n",
    "\n",
    " **But!**\n",
    "\n",
    "* This does **not** mean there is a 95% chance the result is true.\n",
    "* It means: \"If there were no real difference, there's a 5% chance you'd still get these results by random chance.\"\n",
    "\n",
    "---\n",
    "\n",
    "###  **False Positives**\n",
    "\n",
    "* A p-value **below 0.05** when there's **actually no difference** is called a **false positive**.\n",
    "* Using p=0.05 means you'd expect **5 false positives in every 100 experiments** where there's no actual difference.\n",
    "\n",
    "You can lower this risk:\n",
    "\n",
    "* Use **p = 0.01** or **p = 0.00001** for stricter confidence (e.g., in medicine).\n",
    "* Use **p = 0.2** for more tolerance (e.g., guessing when an ice cream truck arrives).\n",
    "\n",
    "---\n",
    "\n",
    "###  **Two final key points:**\n",
    "\n",
    "1. **P-value ≠ Effect Size**\n",
    "\n",
    "   * A small p-value doesn’t mean the difference is big.\n",
    "   * With **large samples**, even tiny differences can be statistically significant.\n",
    "\n",
    "2. **Hypothesis Testing**\n",
    "\n",
    "   * The p-value is used to **test a hypothesis**.\n",
    "   * The **null hypothesis** assumes \"no difference.\"\n",
    "   * A small p-value suggests we **reject the null hypothesis**.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "| Concept                    | Meaning                                                              |\n",
    "| -------------------------- | -------------------------------------------------------------------- |\n",
    "| **P-value**                | Probability of observing your data (or more extreme) if null is true |\n",
    "| **Small p-value (< 0.05)** | Evidence **against** null → suggests a **real difference**           |\n",
    "| **Large p-value**          | Not enough evidence to say there's a difference                      |\n",
    "| **Doesn’t measure**        | Size of the effect or how important it is                            |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39751630-c6da-41e4-b167-8eaeb4a8a188",
   "metadata": {},
   "source": [
    "## **How to Calculate p-value**\n",
    "\n",
    "\n",
    "The way you calculate the p-value depends on:\n",
    "\n",
    "1. The **test** you're using (e.g., t-test, z-test, chi-square test, etc.)\n",
    "2. Your **null hypothesis** (H₀)\n",
    "3. The **distribution** of the test statistic under the null\n",
    "\n",
    "---\n",
    "\n",
    "##  **Example: Two-sample t-test** (comparing means)\n",
    "\n",
    "Let’s say:\n",
    "\n",
    "* Group A (Drug A) has: $n_1 = 30$, mean $\\bar{x}_1 = 70$, std dev $s_1 = 10$\n",
    "* Group B (Drug B) has: $n_2 = 30$, mean $\\bar{x}_2 = 65$, std dev $s_2 = 12$\n",
    "\n",
    "You want to test if **Drug A is significantly better than Drug B** (i.e., if means differ).\n",
    "\n",
    "###  Step-by-step:\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Define hypotheses**\n",
    "\n",
    "* H₀: $\\mu_1 = \\mu_2$ (no difference)\n",
    "* H₁: $\\mu_1 \\neq \\mu_2$ (two-sided)\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Compute the t-statistic**\n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n",
    "= \\frac{70 - 65}{\\sqrt{\\frac{10^2}{30} + \\frac{12^2}{30}}}\n",
    "= \\frac{5}{\\sqrt{3.33 + 4.8}} = \\frac{5}{\\sqrt{8.13}} \\approx \\frac{5}{2.85} \\approx 1.75\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Degrees of freedom** (Welch’s approximation)\n",
    "\n",
    "$$\n",
    "df \\approx \\frac{\n",
    "\\left( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2\n",
    "}{\n",
    "\\frac{\\left( \\frac{s_1^2}{n_1} \\right)^2}{n_1 - 1}\n",
    "+ \\frac{\\left( \\frac{s_2^2}{n_2} \\right)^2}{n_2 - 1}\n",
    "}\n",
    "$$\n",
    "\n",
    "Let’s skip manual computation and say: **df ≈ 57**\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Calculate the p-value**\n",
    "\n",
    "Now use the **t-distribution** with 57 degrees of freedom:\n",
    "\n",
    "* Two-tailed p-value:\n",
    "\n",
    "$$\n",
    "p = 2 \\cdot P(T > |1.75|) \\approx 2 \\cdot 0.043 = 0.086\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "###  **Interpretation**\n",
    "\n",
    "* Since **p = 0.086 > 0.05**, you **fail to reject** the null.\n",
    "* There's **not enough evidence** to say the drugs are significantly different.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3cacb5-bbb3-4182-b237-c3c0642c9136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 1.75\n",
      "P-value: 0.0850\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Sample data\n",
    "mean1, std1, n1 = 70, 10, 30\n",
    "mean2, std2, n2 = 65, 12, 30\n",
    "\n",
    "# Two-sample t-test (unequal variance)\n",
    "t_stat, p_value = stats.ttest_ind_from_stats(\n",
    "    mean1=mean1, std1=std1, nobs1=n1,\n",
    "    mean2=mean2, std2=std2, nobs2=n2,\n",
    "    equal_var=False  # Welch's t-test\n",
    ")\n",
    "\n",
    "print(f\"T-statistic: {t_stat:.2f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375a1ce-dd50-49b8-9277-3832dd406a95",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Other Common Tests\n",
    "\n",
    "| Scenario                            | Test Name             | How p-value is computed           |\n",
    "| ----------------------------------- | --------------------- | --------------------------------- |\n",
    "| Compare two means                   | t-test                | From t-distribution               |\n",
    "| Compare two proportions             | z-test                | From standard normal distribution |\n",
    "| Compare categorical distributions   | Chi-square test       | From chi-square distribution      |\n",
    "| Compare survival curves             | Log-rank test         | From chi-square distribution      |\n",
    "| Regression coefficient significance | t-test for regression | From t-distribution               |\n",
    "| Model fit or independence           | Likelihood ratio test | From chi-square or F-distribution |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018b92a",
   "metadata": {},
   "source": [
    "## P values\n",
    "\n",
    "The **$ p $-value** used in statistics to measure how surprising or unlikely your data is, under  null hypothesis.\n",
    "\n",
    "### The intuition behind P values\n",
    "Imagine you have a coin and you suspect it might be biased towards heads. To test this, you decide to flip the coin 100 times. Your null hypothesis (the assumption you're testing against) is that the coin is fair, meaning it has an equal chance of landing heads or tails.\n",
    "\n",
    "After flipping the coin 100 times, suppose you get an unusually high number of heads, say 70 heads and 30 tails. You might start to think this result is pretty strange if the coin were truly fair.\n",
    "\n",
    "Here's where the **$ p $-value** comes in. The **$ p $-value** is a number between 0 and 1 that tells you how likely it is to see a result as extreme as yours (or more extreme) if the null hypothesis were true. In our example, if the **$ p $-value** is very low (let's say 0.01), it means that getting 70 heads out of 100 flips would be very unlikely if the coin were fair. A low **$ p $-value** suggests that maybe your assumption (the null hypothesis) that the coin is fair might not be right.\n",
    "\n",
    "\n",
    "- A low **$ p $-value** (typically, a threshold like 0.05 or 5% is used), it suggests that the observed data is inconsistent with the null hypothesis, so you might reject the null hypothesis in favor of the alternative hypothesis (which is the hypothesis that there is an effect or a difference).\n",
    "- A high **$ p $-value** means you don't have enough statistical evidence to reject the null hypothesis.\n",
    "\n",
    "However, a low p-value doesn't prove the alternative hypothesis is true. It only suggests that the data you observed are unlikely under the assumption that the null hypothesis is true. Other factors, like the design of the experiment and assumptions of the statistical test, also play critical roles in the interpretation of **$ p $-value**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c9556f",
   "metadata": {},
   "source": [
    "### Technical way of describing the p-value\n",
    "\n",
    "The **$ p $-value** is the probability of observing a test statistic as extreme as, or more extreme than, the statistic computed from the data, assuming that the null hypothesis is true. \n",
    "\n",
    "\n",
    "**Test Statistic**: \n",
    "Test statistics are numerical values used in statistical testing to decide whether to reject the null hypothesis. The choice of test statistic depends on the type of data and the hypothesis being tested. Here's a list of common test statistics used in various statistical tests:\n",
    "\n",
    "1. **Z-statistic**: Used in Z-tests when testing hypotheses concerning population proportions or means, particularly when the sample size is large and the population variance is known.\n",
    "\n",
    "2. **T-statistic**: Used in t-tests when testing hypotheses about means, especially when the population variance is unknown and the sample size is small. There are different forms of t-tests, including one-sample, two-sample, and paired t-tests, each with its own t-statistic formula.\n",
    "\n",
    "3. **Chi-square statistic $\\chi^2$**: Employed in chi-square tests for independence, goodness of fit, or homogeneity. It tests hypotheses about frequency counts of categorical data to see if observed frequencies differ from expected frequencies.\n",
    "\n",
    "4. **F-statistic**: Used in ANOVA (Analysis of Variance) tests to compare the variances across multiple groups to see if at least one sample mean differs significantly from others.\n",
    "\n",
    "5. **U-statistic**: Utilized in Mann-Whitney U tests (a non-parametric test) to compare differences between two independent groups when the assumption of normality is not met.\n",
    "\n",
    "For instance, in our coin flip example, the test statistic could be the number of heads observed.\n",
    "\n",
    "\n",
    "\n",
    "**Observing a Test Statistic as Extreme as, or More Extreme Than, the Statistic Computed from the Data**: comparing what you actually observed in your experiment or study to what you would expect under the null hypothesis. \"As extreme as, or more extreme than\" refers to outcomes that are at least as unlikely as the actual outcome you got, given the null hypothesis is true.\n",
    "\n",
    "**Assuming That the Null Hypothesis is True**: The calculation of the p-value is done under the assumption that the null hypothesis is correct. This is crucial because the p-value is meant to test the strength of evidence against the null hypothesis.\n",
    "\n",
    "**Probability**: The **$ p $-value** itself is a probability. It measures the likelihood of observing your actual test statistic (or one more extreme) purely by chance if the null hypothesis were true. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3124df24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93c989d6",
   "metadata": {},
   "source": [
    "## Example Drug A and Drug B\n",
    "\n",
    "Imagine you have **Drug A** and **Drug B** and you test them on two patients, can we say because it worked on **patient1** and didn't work on **patient2** it is working? There might be several factors that contributed to that result. So now let's try it on $2000$ patients and **Drug A** cured $97\\%$ of people while **Drug B** cured only $3\\%$, so now the chance the result was random and there is no difference between them is unrealistic. Now imagine the success of **Drug A** is $37%$ and **Drug B** is  $31%$ on $50$ patients.\n",
    "\n",
    "So given that no study is perfect and there are always a few random things that change the result, how can we become confident that Drug A is superior?\n",
    "\n",
    "That's where the **$ p $-value** comes in. **$ p $-value** are numbers between  0 and 1 and quantify how confident we should be **Drug A** is different from **Drug B**.\n",
    "The closer a **$ p $-value** is to 0 the more confident we are that **Drug A** and **Drug B** are different. \n",
    "\n",
    "\n",
    "In practice, the commonly used threshold is $0.05$, meaning if there is no difference between **Drug A** and **Drug B**, and we did the exact same experiment then only $5\\%$ of those experiments would result is the wrong decision. Now let's repeat the experiment repeatedly, and we get the following (**$ p $-value** calculated using the Fisher test):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d133df9",
   "metadata": {},
   "source": [
    "| Drug A  |           | Drug B |           | p-value |\n",
    "|---------|-----------|--------|-----------|---------|\n",
    "| Cured   | Not Cured | Cured  | Not Cured |         |\n",
    "| 73      | 125       | 71     | 127       | 0.9     |\n",
    "| 71      | 127       | 72     | 126       | 1.0     |\n",
    "| 75      | 123       | 70     | 128       | 0.7     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a882a915",
   "metadata": {},
   "source": [
    "## Example Effects of a new Fertilizer on Plant Growth\n",
    "\n",
    "Imagine you're a botanist studying the effects of a new fertilizer on plant growth. You have two groups of plants:\n",
    "\n",
    "1. **Control Group**: Plants not given the fertilizer.\n",
    "2. **Treatment Group**: Plants given the fertilizer.\n",
    "\n",
    "You want to know if the fertilizer has a significant effect on plant growth. To do this, you measure the height of the plants after a fixed period.\n",
    "\n",
    "**Hypotheses**:\n",
    "- $ H_0 $: The fertilizer has no effect on plant growth. (Mean height of Control Group = Mean height of Treatment Group)\n",
    "- $ H_a $: The fertilizer has an effect on plant growth. (Mean height of Control Group ≠ Mean height of Treatment Group)\n",
    "\n",
    "**Data**:\n",
    "Let's assume you measured the height (in cm) of 10 plants from each group:\n",
    "\n",
    "- Control Group: [15, 17, 16, 14, 15, 16, 17, 15, 16, 17]\n",
    "- Treatment Group: [18, 19, 20, 19, 18, 21, 19, 20, 19, 18]\n",
    "\n",
    "We'll use a two-sample t-test to determine if there's a significant difference in the means of the two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1fe46f",
   "metadata": {},
   "source": [
    "To calculate the p-value using a t-test for comparing the means of the control group and the treatment group, we'll go through the following steps:\n",
    "\n",
    "1. **Calculate the mean** of each group.\n",
    "2. **Calculate the standard deviation** of each group.\n",
    "3. **Calculate the standard error of the mean (SEM)** for each group.\n",
    "4. **Calculate the t-statistic** using the means, SEMs, and sample sizes of both groups.\n",
    "5. **Calculate the degrees of freedom** needed to look up the p-value.\n",
    "6. **Calculate the p-value** based on the t-statistic and degrees of freedom.\n",
    "\n",
    "\n",
    "\n",
    "For a two-tailed test (which checks for any difference between the means, not specifying direction), the p-value can be conceptualized as:\n",
    "\n",
    "$ \\text{p-value} = 2 \\times (1 - \\text{CDF}(t, df)) $\n",
    "\n",
    "Where:\n",
    "- $ \\text{CDF} $ refers to the cumulative distribution function for the t-distribution.\n",
    "- $ t $ is the observed t-statistic calculated from your data.\n",
    "- $ df $ are the degrees of freedom, which, for an independent samples t-test, are usually calculated as $ n_1 + n_2 - 2 $ for equal variances, or using a more complex formula for unequal variances and sample sizes.\n",
    "\n",
    "This formula is calculating the probability of observing a t-statistic as extreme as, or more extreme than, the observed t-statistic under the null hypothesis. The \"2 ×\" part accounts for both tails of the distribution since we're interested in differences in either direction (higher or lower).\n",
    "\n",
    "In practice, this calculation is not done manually but through  software. These functions internally use the properties of the t-distribution to find the p-value corresponding to the calculated t-statistic and the degrees of freedom for your specific test scenario.\n",
    "\n",
    "\n",
    "\n",
    "### Control Group\n",
    "- **Mean**: 15.8 cm\n",
    "- **Standard Deviation**: 1.033 cm\n",
    "- **Standard Error of the Mean (SEM)**: 0.327 cm\n",
    "\n",
    "### Treatment Group\n",
    "- **Mean**: 19.1 cm\n",
    "- **Standard Deviation**: 0.994 cm\n",
    "- **Standard Error of the Mean (SEM)**: 0.314 cm\n",
    "\n",
    "### T-test Results\n",
    "- **T-statistic**: -7.279\n",
    "- **P-value**: approximately 0.0000009162\n",
    "\n",
    "The T-statistic is -7.279, which indicates a significant difference between the control and treatment groups, given the very low P-value (less than 0.001). This means there is a statistically significant difference in plant height between the control group and the treatment group, favoring the hypothesis that the new fertilizer has a positive effect on plant growth.\n",
    "\n",
    "\n",
    "\n",
    "Refs [1](https://www.youtube.com/watch?v=vemZtEM63GY), [2](https://www.youtube.com/watch?v=udyAvvaMjfM), [3](https://www.youtube.com/watch?v=p0W1oKPP6eQ), [4](https://www.youtube.com/watch?v=0oc49DyA3hU), [5](https://www.youtube.com/watch?v=JQc3yx0-Q9E), [6](https://www.youtube.com/watch?v=5koKb5B_YWo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11e88a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.8,\n",
       " 1.0327955589886444,\n",
       " 19.1,\n",
       " 0.9944289260117533,\n",
       " 0.3265986323710904,\n",
       " 0.31446603773522014,\n",
       " -7.278624758728698,\n",
       " 9.162003368633656e-07)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "control_group = np.array([15, 17, 16, 14, 15, 16, 17, 15, 16, 17])\n",
    "treatment_group = np.array([18, 19, 20, 19, 18, 21, 19, 20, 19, 18])\n",
    "\n",
    "# Step 1 & 2: Calculate mean and standard deviation\n",
    "mean_control = np.mean(control_group)\n",
    "std_dev_control = np.std(control_group, ddof=1)  # Sample standard deviation\n",
    "mean_treatment = np.mean(treatment_group)\n",
    "std_dev_treatment = np.std(treatment_group, ddof=1)  # Sample standard deviation\n",
    "\n",
    "# Step 3: Calculate the Standard Error of the Mean (SEM) for each group\n",
    "n_control = len(control_group)\n",
    "n_treatment = len(treatment_group)\n",
    "sem_control = std_dev_control / np.sqrt(n_control)\n",
    "sem_treatment = std_dev_treatment / np.sqrt(n_treatment)\n",
    "\n",
    "# Step 4 & 5: Calculate t-statistic and degrees of freedom\n",
    "# Using scipy to calculate t-statistic and p-value directly\n",
    "t_stat, p_value = ttest_ind(control_group, treatment_group)\n",
    "\n",
    "mean_control, std_dev_control, mean_treatment, std_dev_treatment, sem_control, sem_treatment, t_stat, p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd992cf-05a6-4904-810e-7266d410f5d5",
   "metadata": {},
   "source": [
    "## Hypothesis testing in deep learning\n",
    "Hypothesis testing in deep learning can be applied in several scenarios, especially when you're interested in comparing models, understanding the significance of model improvements, or analyzing the behavior of models under specific conditions. Here are some common use cases:\n",
    "\n",
    "1. **Model Comparison**: When you have two or more different models or approaches and want to determine if one model is significantly better than the others. Hypothesis testing can help you assess if the differences in performance metrics (like accuracy, precision, recall) are statistically significant or just due to random chance.\n",
    "\n",
    "2. **Feature Importance**: To evaluate the impact of certain features on the model's predictions. Hypothesis testing can be used to determine if removing or adding a specific feature significantly affects the model's performance, helping in feature selection and model simplification.\n",
    "\n",
    "3. **Regularization and Hyperparameter Tuning**: When adjusting model hyperparameters (like learning rate, dropout rate, or regularization strength), hypothesis testing can help in determining if the changes in the hyperparameters lead to a statistically significant improvement or degradation in model performance.\n",
    "\n",
    "4. **Transfer Learning and Domain Adaptation**: In scenarios involving transfer learning or domain adaptation, where a model trained on one domain is adapted for use in another, hypothesis testing can be used to assess if the adaptation leads to significant improvements in performance on the new domain.\n",
    "\n",
    "5. **Fairness and Bias Assessment**: Hypothesis testing can be instrumental in identifying biases in model predictions across different groups or demographics. It can help in determining if disparities in model outcomes are statistically significant, which is crucial for developing fair and unbiased models.\n",
    "\n",
    "6. **A/B Testing**: In the deployment phase, especially for models integrated into products or services, A/B testing with hypothesis testing can evaluate the real-world impact of using one model version over another on user behavior or other key performance indicators (KPIs).\n",
    "\n",
    "7. **Robustness and Generalization**: To test the robustness of models against variations in input data, including adversarial examples, noise, or data from different distributions. Hypothesis testing can help determine if a model is significantly more robust or generalizes better to unseen data compared to others.\n",
    "\n",
    "8. **Time Series and Sequence Models**: For models dealing with time series or sequential data, hypothesis testing can be used to assess the significance of temporal features or the impact of different sequence modeling techniques (like RNNs, GRUs, or LSTMs) on prediction accuracy.\n",
    "\n",
    "In practice, implementing hypothesis testing in deep learning involves choosing the right statistical test based on the data distribution and experiment design, defining null and alternative hypotheses, calculating the test statistic, and interpreting the p-value to make decisions. It's a powerful tool to add rigor and confidence to the conclusions drawn from deep learning experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
