{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "Model selection in **machine learning** is the process of choosing the best model (or model configuration) from a set of candidate models for a given task, based on performance criteria.\n",
    "\n",
    "It’s essentially answering:\n",
    "\n",
    "> *“Out of all the models I could use, which one should I trust to perform best on unseen data?”*\n",
    "\n",
    "---\n",
    "\n",
    "##  Why Model Selection Matters\n",
    "\n",
    "Even if you have the most advanced algorithm, if it’s not the right fit for your data, it can underperform. Model selection helps to:\n",
    "\n",
    "* **Avoid overfitting** (model fits training data too closely, failing on new data)\n",
    "* **Avoid underfitting** (model is too simple to capture the data patterns)\n",
    "* **Maximize generalization** (good performance on unseen data)\n",
    "\n",
    "---\n",
    "\n",
    "##  Common Scenarios for Model Selection\n",
    "\n",
    "* Choosing **between algorithms** (e.g., logistic regression vs. random forest vs. neural network)\n",
    "* Choosing **hyperparameters** (e.g., number of trees in a random forest, learning rate in a neural net)\n",
    "* Selecting **feature sets** or **data preprocessing methods** that yield the best performance\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "##  Example: Choosing a Classification Model\n",
    "\n",
    "Let’s say you have 10,000 labeled emails (spam / not spam).\n",
    "\n",
    "1. **Split data**: 70% train, 15% validation, 15% test.\n",
    "2. **Train models**: Logistic Regression, Random Forest, XGBoost.\n",
    "3. **Hyperparameter tuning**: Use grid search + 5-fold CV for each model.\n",
    "4. **Compare**:\n",
    "\n",
    "   | Model               | Validation F1 |\n",
    "   | ------------------- | ------------- |\n",
    "   | Logistic Regression | 0.89          |\n",
    "   | Random Forest       | 0.91          |\n",
    "   | XGBoost             | 0.93 ✅        |\n",
    "5. **Select XGBoost** and test → F1 = 0.92 (good generalization).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Choosing Right Model Estimator\n",
    "\n",
    "\n",
    "<img src='images/ml_map.png'>\n",
    "\n",
    "Refs: [1](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/overfitting_underfitting.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs: [1](https://towardsdatascience.com/a-short-introduction-to-model-selection-bb1bb9c73376)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
