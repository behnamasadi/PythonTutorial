{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7756f6c",
   "metadata": {},
   "source": [
    "Finding the basis of the null space of matrix $A$ using Singular Value Decomposition (SVD). \n",
    "\n",
    "1. **SVD Decomposition**: The SVD of a matrix $A$ is given by $A = U\\Sigma V^T$, where:\n",
    "   - $U$ and $V$ are orthogonal matrices.\n",
    "   - $\\Sigma$ is a diagonal matrix with singular values of $A$ on its diagonal.\n",
    "   - $V^T$ is the transpose of $V$.\n",
    "\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?%7B%5Cdisplaystyle%20%5C%20%5Cmathbf%20%7BA_%7Bm%5Ctimes%20n%7D%7D%20%3D%5Cmathbf%20%7BU_%7Bm%5Ctimes%20m%7D%20%5CSigma_%7Bm%5Ctimes%20n%7D%20V%5E%7B*%7D_%7Bn%5Ctimes%20n%7D%7D%20%5C%20%2C%7D\" alt=\"https://latex.codecogs.com/svg.latex?{\\displaystyle \\ \\mathbf {A_{m\\times n}} =\\mathbf {U_{m\\times m} \\Sigma_{m\\times n} V^{*}_{n\\times n}} \\ ,}\" />\n",
    "\n",
    "\n",
    "\n",
    "2. **`np.linalg.svd(A)`**: This function computes the SVD of matrix $A$. \n",
    "It returns three matrices: \n",
    "- $U$\n",
    "- $\\Sigma$ (in Numpy this is as a vector of singular values instead of a diagonal matrix), \n",
    "- $V^T$.\n",
    "\n",
    "3. **`Vt.T`**: `Vt` is the $V^T$ matrix from the SVD, and `Vt.T` is the transpose of $V^T$, which is $V$. Transposing $V^T$ gives us the matrix $V$ whose __**columns**__ are the eigenvectors of $A^TA$.\n",
    "\n",
    "4. **`np.linalg.matrix_rank(A)`**: This function computes the rank of matrix $A$, which is the number of non-zero singular values in $\\Sigma$. The rank of $A$ tells us the dimension of the column space (or range) of $A$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cccdd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "S:  [ True  True False]\n",
      "Null space of A:\n",
      " [[-0.40824829]\n",
      " [ 0.81649658]\n",
      " [-0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def null_space(A, tol=1e-15):\n",
    "    # Perform Singular Value Decomposition\n",
    "    U, S, Vh = np.linalg.svd(A)\n",
    "    \n",
    "    # Find the rank of the matrix A, which is number of non-zero on the diagonal, \n",
    "    print(\"S: \" ,S > tol)\n",
    "    rank = (S > tol).sum()\n",
    "    \n",
    "    # Extract the null space, which are the columns in V corresponding to non zero elemenst or rank of matrix\n",
    "    null_space = Vh[rank:].T\n",
    "    \n",
    "    return null_space\n",
    "\n",
    "# Example usage\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)\n",
    "\n",
    "\n",
    "null_space_A = null_space(A)\n",
    "print(\"Null space of A:\\n\", null_space_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e861cb7",
   "metadata": {},
   "source": [
    "Lets say we have the following matrix:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.image?A=\\begin{bmatrix}1%20&%201%20&%202%20&%201%20\\\\3%20&%201%20&%20%204&%20%204\\\\4%20&%20-4%20&%200%20&%20%208\\\\\\end{bmatrix}%20\\in%20\\mathbb{R}^4\" alt=\"https://latex.codecogs.com/svg.image?A=\\begin{bmatrix} 1 & 1 & 2 & 1 \\\\3 & 1 &  4&  4\\\\ 4 & -4 & 0 &  8\\\\ \\end{bmatrix} \\in \\mathbb{R}^4\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5595d",
   "metadata": {},
   "source": [
    "The null space of the matrix \\(A\\) is spanned by the following basis vectors:\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "v_1 & = \\begin{bmatrix} -0.58293078 \\\\ -0.57332806 \\\\ 0.57572874 \\\\ 0.00480136 \\end{bmatrix}, \\\\\n",
    "v_2 & = \\begin{bmatrix} -0.65065321 \\\\ 0.47320874 \\\\ -0.19224325 \\\\ 0.56193097 \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "Now, if we choose arbitrary coefficients for the linear combination and multiply it by \\(A\\) we should get a zero vector.\n",
    "\n",
    "**`Vt.T[:, np.linalg.matrix_rank(A):]`**: This slicing operation selects all columns from $V$ starting from the column index equal to the rank of $A$ to the last column. These columns correspond to the null space (or kernel) of $A$. The reason this works is that the columns of $V$ (after the rank index) correspond to the singular values that are zero (or near zero due to numerical precision), and these vectors span the null space of $A$. In other words, these vectors are orthogonal to all rows of $A$, meaning that when $A$ is multiplied by any of these vectors, the result is the zero vector.\n",
    "\n",
    "To sum up, `null_space_basis = Vt.T[:, np.linalg.matrix_rank(A):]` computes the basis vectors of the null space of $A$ by selecting the appropriate columns from the matrix $V$ (the right singular vectors) that correspond to the zero singular values, after performing an SVD on $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f78a36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank of matrix is:  2  so we want the first  2  columns of V\n",
      "V is: \n",
      " [[-0.46152645 -0.15438731 -0.58293078 -0.65065321]\n",
      " [ 0.26277123 -0.61507699 -0.57332806  0.47320874]\n",
      " [-0.19875522 -0.7694643   0.57572874 -0.19224325]\n",
      " [-0.82367529  0.07595753  0.00480136  0.56193097]]\n",
      "The null space basis: \n",
      " [[-0.58293078 -0.65065321]\n",
      " [-0.57332806  0.47320874]\n",
      " [ 0.57572874 -0.19224325]\n",
      " [ 0.00480136  0.56193097]]\n",
      "A @ linear_combination: [0.00000000e+00 2.66453526e-15 7.10542736e-15]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the matrix A\n",
    "A = np.array([[1, 1, 2, 1], [3, 1, 4, 4], [4, -4, 0, 8]])\n",
    "\n",
    "# Compute the null space of matrix A\n",
    "_, _, Vt = np.linalg.svd(A)\n",
    "\n",
    "\n",
    "rank=np.linalg.matrix_rank(A)\n",
    "\n",
    "print(\"rank of matrix is: \", rank, \" so we want the first \", rank, \" columns of V\")\n",
    "\n",
    "\n",
    "V=Vt.T\n",
    "\n",
    "print(\"V is: \\n\",V)\n",
    "\n",
    "null_space_basis = V[:, np.linalg.matrix_rank(A):]\n",
    "\n",
    "print(\"The null space basis: \\n\", V[:, np.linalg.matrix_rank(A):])\n",
    "\n",
    "# Define arbitrary coefficients for the linear combination\n",
    "coefficients = np.array([2, 3])\n",
    "\n",
    "# Calculate the linear combination of the basis vectors\n",
    "linear_combination = null_space_basis @ coefficients\n",
    "\n",
    "# Multiply the linear combination by A to verify the result\n",
    "verification_result = A @ linear_combination\n",
    "\n",
    "# Display the verification result\n",
    "print(\"A @ linear_combination:\", verification_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c17254a",
   "metadata": {},
   "source": [
    "After multiplying the linear combination of the null space basis by matrix \\(A\\), the result is approximately equal to the zero vector: \n",
    "\n",
    "$ \\begin{bmatrix} 0.00000000e+00 \\\\ 2.66453526e-15 \\\\ 7.10542736e-15 \\end{bmatrix} $\n",
    "\n",
    "This result, which is very close to $\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, verifies that the calculated basis indeed spans the null space of \\(A\\). The small non-zero values are due to numerical precision limits in computation and are effectively considered as zero in numerical linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5280ad7",
   "metadata": {},
   "source": [
    "# Use of the null space in optimization\n",
    "Let's consider a numerical example related to the least squares problem, which illustrates the use of the null space in optimization.\n",
    "\n",
    "### Problem Statement\n",
    "Suppose we have a system of linear equations represented by $Ax = b$, where\n",
    "\n",
    "$A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 2 \\\\ 3 \\\\ 4 \\end{pmatrix}$\n",
    "\n",
    "This system is overdetermined (more equations than unknowns), and it's unlikely to find $x$ that exactly solves $Ax = b$. Instead, we aim to find $x$ that minimizes the squared error $\\|Ax - b\\|^2$, which is the least squares problem.\n",
    "\n",
    "### Steps\n",
    "1. Compute the pseudo-inverse of $A$, denoted as $A^+$, to find the least squares solution $\\hat{x} = A^+b$.\n",
    "2. Determine the null space of $A$. The null space consists of all vectors $n$ for which $An = 0$.\n",
    "3. Verify that adding any vector from the null space to the solution $\\hat{x}$ does not change the error term, illustrating the principle that the least squares solution is unique only if the null space of $A$ contains only the zero vector.\n",
    "\n",
    "Let's perform these calculations.\n",
    "\n",
    "The least squares solution to the problem, $\\hat{x}$, is given by $[-1, 1.5]$. This means that for our system of equations $Ax = b$, the vector $\\hat{x} = [-1, 1.5]$ minimizes the squared error $\\|Ax - b\\|^2$.\n",
    "\n",
    "In this particular example, the null space of $A$ does not contain any non-zero vectors (as indicated by an empty array in the output), which means that every vector in the null space is essentially the zero vector. This outcome implies that our least squares solution $\\hat{x}$ is unique. There are no other vectors $n$ in the null space of $A$ (other than the zero vector) that we could add to $\\hat{x}$ to obtain another solution with the same error norm. Therefore, in this specific case, the least squares solution is uniquely determined by the pseudo-inverse of $A$.\n",
    "\n",
    "This example illustrates how the null space concept is applied in least squares optimization to understand the solution's uniqueness and structure. In scenarios where the null space of the matrix involved in the optimization problem contains non-trivial vectors (i.e., vectors other than the zero vector), those vectors define directions along which one can move without affecting the optimization criterion, leading to multiple solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59a662d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1. ,  1.5]), array([], shape=(2, 0), dtype=float64))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the matrix A and vector b\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "b = np.array([2, 3, 4])\n",
    "\n",
    "# Compute the pseudo-inverse of A\n",
    "A_pinv = np.linalg.pinv(A)\n",
    "\n",
    "# Compute the least squares solution\n",
    "x_hat = A_pinv @ b\n",
    "\n",
    "# Determine the null space of A\n",
    "# For the null space, we can use the SVD (Singular Value Decomposition) of A\n",
    "U, S, VT = np.linalg.svd(A)\n",
    "null_mask = S <= np.max(S) * np.finfo(S.dtype).eps\n",
    "null_space = VT[null_mask, :].conj().T\n",
    "\n",
    "x_hat, null_space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd9a6a3",
   "metadata": {},
   "source": [
    "Let's consider another numerical example involving a different matrix where the null space is non-trivial, implying it contains vectors other than the zero vector. This will help us see the impact of the null space on the solutions of a least squares problem.\n",
    "\n",
    "### Problem Statement\n",
    "Suppose we now have a system of linear equations represented by $Ax = b$, where\n",
    "\n",
    "$A = \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 2 \\\\ 2 & 3 & 4 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 6 \\\\ 5 \\\\ 11 \\end{pmatrix}$\n",
    "\n",
    "Again, we seek $x$ that minimizes the squared error $\\|Ax - b\\|^2$.\n",
    "\n",
    "### Steps\n",
    "1. Compute the pseudo-inverse of $A$ to find the least squares solution $\\hat{x} = A^+b$.\n",
    "2. Determine the null space of $A$, consisting of all vectors $n$ for which $An = 0$.\n",
    "3. Illustrate how vectors from the null space of $A$ relate to the least squares solution.\n",
    "\n",
    "Let's perform these calculations now.\n",
    "\n",
    "The least squares solution to the new problem, $\\hat{x}$, is approximately $[1.33, 1.33, 1.33]$. This means that for our system of equations $Ax = b$, the vector $\\hat{x} = [1.33, 1.33, 1.33]$ minimizes the squared error $\\|Ax - b\\|^2$.\n",
    "\n",
    "In this case, the null space of $A$ contains a non-trivial vector: approximately $[-0.41, 0.82, -0.41]$. This indicates that there are directions in which we can adjust the solution $\\hat{x}$ without affecting the squared error $\\|Ax - b\\|^2$. Specifically, any scalar multiple of this null space vector can be added to the least squares solution $\\hat{x}$ to obtain another solution to the least squares problem.\n",
    "\n",
    "This example illustrates how the presence of non-trivial vectors in the null space of $A$ indicates the existence of multiple solutions to the least squares problem. These solutions are all valid in terms of minimizing the squared error, but they differ by vectors in the null space of $A$. This is a crucial concept in understanding the solution space of optimization problems, particularly those involving underdetermined systems or systems with redundancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0855311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_hat: \n",
      " [[1.33333333]\n",
      " [1.33333333]\n",
      " [1.33333333]]\n",
      "b: \n",
      " [ 6  5 11]\n",
      "A@ x_hat: \n",
      " [[ 4.]\n",
      " [ 4.]\n",
      " [12.]]\n",
      "null space of A: \n",
      " [[-0.40824829]\n",
      " [ 0.81649658]\n",
      " [-0.40824829]]\n",
      "A@new_x: \n",
      " [[ 4.]\n",
      " [ 4.]\n",
      " [12.]]\n"
     ]
    }
   ],
   "source": [
    "# Define the matrix A and vector b\n",
    "A = np.array([[1, 1, 1], [0, 1, 2], [2, 3, 4]])\n",
    "b = np.array([6, 5, 11])\n",
    "\n",
    "# Compute the pseudo-inverse of the  A\n",
    "A_pinv = np.linalg.pinv(A)\n",
    "\n",
    "# Compute the least squares solution for the new A\n",
    "x_hat = A_pinv @ b\n",
    "x_hat = x_hat[:, np.newaxis]\n",
    "\n",
    "print(\"x_hat: \\n\", x_hat)\n",
    "print(\"b: \\n\", b)\n",
    "print(\"A@ x_hat: \\n\", A@ x_hat)\n",
    "\n",
    "\n",
    "\n",
    "# Determine the null space of the new A\n",
    "U, S, VT = np.linalg.svd(A)\n",
    "null_mask = S <= np.max(S) * np.finfo(S.dtype).eps\n",
    "null_space = VT[null_mask, :].conj().T\n",
    "\n",
    "print(\"null space of A: \\n\",null_space ) \n",
    "\n",
    "\n",
    "# any scalar multiple of this null space vector can be added to the least squares solution x_hat to obtain another solution to the least squares problem.\n",
    "\n",
    "coefficient=-10.45\n",
    "new_x=null_space*coefficient+x_hat\n",
    "\n",
    "print(\"A@new_x: \\n\",A@new_x ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e0bdb",
   "metadata": {},
   "source": [
    "The vector $ \\hat{x} $  minimizes the Euclidean norm of the residual, $ \\mathbf{b} - A\\hat{x} $,\n",
    "\n",
    "After computing $ \\hat{x} $ as $ A^{+}\\mathbf{b} $, where $ A^{+} $ is the pseudo-inverse of $ A $, we are checking the result by multiplying $ A $ with $ \\hat{x} $ to see if it equals $ \\mathbf{b} $.\n",
    "\n",
    "In an ideal scenario, with exact arithmetic and if $ A $ is square and invertible, or if the system is overdetermined but consistent (meaning all equations can be satisfied exactly), you would expect $ A\\hat{x} $ to closely approximate $ \\mathbf{b} $. However, in most practical scenarios, especially with overdetermined or underdetermined systems, $ A\\hat{x} $ represents the projection of $ \\mathbf{b} $ onto the column space of $ A $. This means $ A\\hat{x} $ will be as close as possible to $ \\mathbf{b} $ in the least squares sense but may not be exactly equal due to rounding errors, numerical precision limits, or because the system of equations does not have an exact solution within the constraints.\n",
    "\n",
    "\n",
    "After computing the least squares solution $ \\hat{x} $, we find $ \\hat{x} = [1.333, 1.333, 1.333] $. When we multiply $ A $ by this $ \\hat{x} $, we get $ A\\hat{x} = [4, 4, 12] $, which is different from the original $ \\mathbf{b} = [6, 5, 11] $.\n",
    "\n",
    "This result indicates that $ A\\hat{x} $ does not exactly equal $ \\mathbf{b} $, which is expected in a least squares solution when the system of equations does not have an exact solution that satisfies all equations perfectly. Instead, $ \\hat{x} $ minimizes the sum of the squares of the residuals, the differences between the entries of $ \\mathbf{b} $ and those of $ A\\hat{x} $. This outcome is typical for problems solved by the method of least squares, especially in over-determined systems where there are more equations than unknowns, or the matrix $ A $ does not have full rank, leading to an infinite number of solutions where one seeks the solution with the minimum norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141bded",
   "metadata": {},
   "source": [
    "# Constrained Optimization\n",
    "\n",
    "Constrained optimization problems often involve finding the minimum or maximum of a function subject to equality and/or inequality constraints. The null space plays a significant role, particularly in equality-constrained problems, where it helps identify directions that satisfy the constraints.\n",
    "\n",
    "### Example: Equality-Constrained Optimization\n",
    "\n",
    "#### Problem Statement\n",
    "Consider the problem of minimizing the function $f(x_1, x_2) = x_1^2 + x_2^2$ subject to the linear constraint $g(x_1, x_2) = x_1 + 2x_2 - 1 = 0$.\n",
    "\n",
    "#### Solution Approach\n",
    "1. **Formulate the Lagrangian**: The Lagrangian $L(x_1, x_2, \\lambda) = f(x_1, x_2) - \\lambda(g(x_1, x_2) - 1)$ combines the objective function and the constraint, where $\\lambda$ is the Lagrange multiplier.\n",
    "2. **Solve for the Extrema**: Find where the gradient of the Lagrangian is zero to identify candidate solutions.\n",
    "3. **Determine the Null Space of the Constraint**: The null space of the constraint matrix (derived from the gradient of $g(x_1, x_2)$) identifies directions along which the constraint $g(x_1, x_2) = 0$ remains satisfied.\n",
    "\n",
    "#### Calculations\n",
    "1. The Lagrangian is $L(x_1, x_2, \\lambda) = x_1^2 + x_2^2 - \\lambda(x_1 + 2x_2 - 1)$.\n",
    "2. We solve for $\\nabla L = 0$ to find $(x_1, x_2, \\lambda)$.\n",
    "3. The constraint can be represented as $A\\vec{x} = b$ with $A = [1, 2]$ and $b = 1$, so we find the null space of $A$.\n",
    "\n",
    "Let's perform these calculations to find the extremum and the null space of the constraint.\n",
    "\n",
    "The solution to the constrained optimization problem is approximately $x_1 = 0.2$ and $x_2 = 0.4$, minimizing the function $f(x_1, x_2) = x_1^2 + x_2^2$ subject to the constraint $x_1 + 2x_2 - 1 = 0$.\n",
    "\n",
    "The null space of the constraint matrix $A = [1, 2]$ is spanned by the vector $\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$. This vector represents the direction in the input space $(x_1, x_2)$ along which the constraint $x_1 + 2x_2 - 1 = 0$ remains satisfied. Moving along this direction from any point on the constraint line will keep you on the line, indicating how changes in the variables can occur without violating the constraint.\n",
    "\n",
    "This example demonstrates how the null space concept is applied in constrained optimization to understand the degrees of freedom available within the constraints. In this case, although we have a unique solution due to the specific form of the objective function and the constraint, the null space gives insight into how the constraint shapes the feasible region of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "099a931e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.2, 0.4]),\n",
       " [Matrix([\n",
       "  [-2],\n",
       "  [ 1]])])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from sympy import Matrix\n",
    "\n",
    "# Define the objective function\n",
    "def objective(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "# Define the constraint as a callable function\n",
    "def constraint(x):\n",
    "    return x[0] + 2*x[1] - 1\n",
    "\n",
    "# The constraint in a form suitable for 'minimize'\n",
    "cons = ({'type': 'eq', 'fun': constraint})\n",
    "\n",
    "# Initial guess\n",
    "x0 = [0.5, 0.5]\n",
    "\n",
    "# Perform the optimization\n",
    "sol = minimize(objective, x0, constraints=cons)\n",
    "\n",
    "# Determine the null space of the constraint matrix\n",
    "A = Matrix([[1, 2]])\n",
    "null_space = A.nullspace()\n",
    "\n",
    "sol.x, null_space\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
