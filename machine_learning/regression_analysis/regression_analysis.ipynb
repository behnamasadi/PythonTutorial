{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression analysis\n",
    "regression analysis is a set of statistical processes for estimating the relationships between a dependent variable, often called the **outcome variable** and one or more independent variables, often called **predictors**, **covariates**, or **features**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Linear Regression\n",
    "Although the terms **least squares** and **linear model** are closely linked, they are not synonymous. Linear regression models are often fitted using the least squares approach but other approaches could be used, i.e.  minimizing a penalized version of the least squares cost function as in ridge regression (Tikhonov regularization\n",
    " and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a data set ${\\displaystyle \\{y_{i},\\,x_{i1},\\ldots ,x_{in}\\}_{i=1}^{m}}$,  regression model assumes that the relationship between the dependent variable $y$ and the $\\textbf{x}$ is linear.\n",
    "\n",
    "${\\displaystyle y_{i}=\\beta _{0}+\\beta _{1}x_{i1}+\\cdots +\\beta _{p}x_{ip}+\\varepsilon _{i}=\\mathbf {x} _{i}^{\\mathsf {T}}{\\boldsymbol {\\beta }}+\\varepsilon _{i},\\qquad i=1,\\ldots ,m,}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing it in the matrix form:\n",
    "\n",
    "${\\displaystyle {\\begin{pmatrix}1&x_{11}&\\cdots &x_{1n}\\\\1&x_{21}&\\cdots &x_{2n}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\1&x_{m1}&\\cdots &x_{mn}\\end{pmatrix}}}\\mathbf {\\begin{pmatrix}\\beta_{1}\\\\ \\beta_{2}\\\\\\vdots \\\\ \\beta_{n}\\end{pmatrix}}=\\mathbf {\\begin{pmatrix}y_{1}\\\\y_{2}\\\\\\vdots \\\\ y_{m}\\end{pmatrix}} $\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X_{m\\times n}{\\vec {\\beta_{n\\times 1} }}=Y_{m\\times 1}$\n",
    "\n",
    "${\\displaystyle L(D,{\\vec {\\beta }})=||X{\\vec {\\beta }}-Y||^{2}=(X{\\vec {\\beta }}-Y)^{T}(X{\\vec {\\beta }}-Y)=Y^{T}Y-Y^{T}X{\\vec {\\beta }}-{\\vec {\\beta }}^{T}X^{T}Y+{\\vec {\\beta }}^{T}X^{T}X{\\vec {\\beta }}}$\n",
    "\n",
    "\n",
    "${\\displaystyle {\\frac {\\partial L(D,{\\vec {\\beta }})}{\\partial {\\vec {\\beta }}}}={\\frac {\\partial \\left(Y^{T}Y-Y^{T}X{\\vec {\\beta }}-{\\vec {\\beta }}^{T}X^{T}Y+{\\vec {\\beta }}^{T}X^{T}X{\\vec {\\beta }}\\right)}{\\partial {\\vec {\\beta }}}}=-2X^{T}Y+2X^{T}X{\\vec {\\beta }}}$\n",
    "\n",
    "\n",
    "setting the gradient of the loss to zero and solving for ${\\displaystyle {\\vec {\\beta }}}$ we get: \n",
    "\n",
    "${\\displaystyle -2X^{T}Y+2X^{T}X{\\vec {\\beta }}=0\\Rightarrow X^{T}Y=X^{T}X{\\vec {\\beta }}\\Rightarrow {\\vec {\\hat {\\beta }}}=(X^{T}X)^{-1}X^{T}Y}{\\displaystyle -2X^{T}Y+2X^{T}X{\\vec {\\beta }}=0}$\n",
    "\n",
    "\n",
    "$\\Rightarrow X^{T}Y=X^{T}X{\\vec {\\beta }}\\Rightarrow {\\vec {\\hat {\\beta }}}=(X^{T}X)^{-1}X^{T}Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Logistic regression\n",
    "logistic model is used to model the probability of a certain class or event existing such as pass/fail, win/lose. This can be extended to model several classes of events with **Multinomial logistic regression**.\n",
    "Using linear regression the outputs may not be between zero and one. Basically we have $\\textbf{X} \\in \\mathbb{R}$ and we are inerested to map it to a probability value  $p(\\textbf{X}) \\in [0,1]$, so we use sigmoid function to limit the outcome between 0 and 1:\n",
    "\n",
    "\n",
    "$p(x)=\\frac{1}{1+e^{-\\beta x}}$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1) Odds\n",
    "Odds provide a measure of the likelihood of a particular outcome. The odds of rolling a 6 is 1:5, The odds of rolling either a 5 or 6 is 2:4.\n",
    "\n",
    "\n",
    "$odds=\\frac{P(event)}{1-P(event)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2) Logit\n",
    "The logit function or the log-odds is the logarithm of the odds. If $p$ is a probability, \n",
    "\n",
    "${\\displaystyle \\operatorname {logit} (p)=\\log \\left({\\frac {p}{1-p}}\\right)=\\log(p)-\\log(1-p)=-\\log \\left({\\frac {1}{p}}-1\\right)\\,.}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3) Parameter Estimation\n",
    "In regression we used least square to estimate the parameter, here we use $MLE$ to estimate the parameter. For every sample in our training sete, since all samples are conditionally indipendent of each other we have:\n",
    "\n",
    "$p(x_i\\cap x_j)=p(x_i, x_j)=p(x_i)p(x_j)$\n",
    "\n",
    "\n",
    "We have to labels (classes): $0$ and $1$ $ {\\displaystyle Y\\in \\{0,1\\}}$\n",
    "\n",
    "We are looking for parameter $\\theta $ such that it maximized the likelihood:\n",
    "\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}L(\\theta \\mid y;x)&=\\Pr(Y\\mid X;\\theta )\\\\&=\\prod _{i}\\Pr(y_{i}\\mid x_{i};\\theta )\\\\&=\\prod _{i}h_{\\theta }(x_{i})^{y_{i}}(1-h_{\\theta }(x_{i}))^{(1-y_{i})}\\end{aligned}}}$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\Pr(Y=1\\mid X;\\theta )={\\displaystyle h_{\\theta }(X)={\\frac {1}{1+e^{-\\theta ^{T}X}}}}$\n",
    "\n",
    "\n",
    "$\\Pr(Y=0\\mid X;\\theta )=1-\\Pr(Y=1\\mid X;\\theta )={\\displaystyle 1-h_{\\theta }(X)=1-{\\frac {1}{1+e^{-\\theta ^{T}X}}}}$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking $log$ function from both side we can turn multipication into summation:\n",
    "\n",
    "$l(\\theta)=\\sum_{i} y_{i}\\theta x_{i} - log(1+e^{\\theta x_{i}})$\n",
    "\n",
    "which is a A **transcendental equation**. A **transcendental function** is an analytic function that does not satisfy a polynomial equation, in contrast to an algebraic function, i.e: $x=e^{-x}, x=\\cos x, 2^{x}=x^{2}$\n",
    "This equation canbe solved numerically by algorithm such as \n",
    "- Gradient descent\n",
    "- Newton–Raphson method\n",
    "- Quasi-Newton methods\n",
    "- Davidon–Fletcher–Powell formula\n",
    "- Broyden–Fletcher–Goldfarb–Shanno algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refs: [1](https://www.youtube.com/watch?v=YMJtsYIp4kg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4) Example:\n",
    "Refs [1](http://faculty.cas.usf.edu/mbrannick/regression/Logistic.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5) Multinomial logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Nonlinear regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
