{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6242308f",
   "metadata": {},
   "source": [
    "# Vector quantization\n",
    "Vector quantization (VQ) is a fundamental technique in the field of digital signal processing and pattern recognition. It involves the process of mapping vectors from a high-dimensional space into a finite set of vectors in the same space, known as code vectors or centroids. These centroids are chosen to represent the distribution of the input vectors efficiently. The set of centroids forms a codebook.\n",
    "\n",
    "The main steps in vector quantization include:\n",
    "\n",
    "1. **Codebook Generation**: This step involves creating the codebook by identifying the centroids. A common approach to generate the codebook is through clustering algorithms like k-means, where the dataset is partitioned into clusters, and the centroid of each cluster becomes a code vector.\n",
    "\n",
    "2. **Encoding**: During encoding, each input vector (which could represent different types of data, such as an image patch, audio segment, or any other kind of signal) is compared against the centroids in the codebook, and it is mapped to the nearest centroid. The index of this centroid in the codebook is stored or transmitted instead of the original vector. This process effectively reduces the size of the data, as the index requires significantly less space than the original vector.\n",
    "\n",
    "3. **Decoding**: In decoding, the received or stored indices are used to look up the corresponding centroids in the codebook, and these centroids are used to reconstruct the original signal. The reconstructed signal is an approximation of the original, with the level of fidelity depending on the size and quality of the codebook.\n",
    "\n",
    "\n",
    "\n",
    "The primary advantages of vector quantization include data compression and the reduction of computational complexity for subsequent processing tasks. However, the accuracy of the reconstruction or the effectiveness in pattern recognition tasks depends on the quality of the codebook and the resolution (size) of the codebook, with larger codebooks generally providing higher fidelity at the cost of increased computational and storage requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117d450",
   "metadata": {},
   "source": [
    "Vector Quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It's widely used in data compression and pattern recognition. To provide a numerical example, we'll go through a simple case of vector quantization using a set of 2D vectors (points) and cluster them into groups (quantize) using a simple algorithm like K-means, which is a common approach in VQ.\n",
    "\n",
    "### Problem Statement:\n",
    "Suppose we have the following set of 2D points (vectors):\n",
    "$ V = \\{(2, 3), (3, 4), (5, 6), (7, 8), (9, 1), (2, 2), (3, 3)\\} $\n",
    "\n",
    "We want to quantize these vectors into $k = 2$ clusters (or codebooks) using a simple version of the K-means algorithm.\n",
    "\n",
    "### Steps:\n",
    "1. **Initialization**: Randomly select $k$ centroids. For simplicity, let's choose $C_1 = (2, 3)$ and $C_2 = (7, 8)$ as the initial centroids.\n",
    "2. **Assignment step**: Assign each vector to the nearest centroid.\n",
    "3. **Update step**: Update the centroids based on the mean of the vectors assigned to them.\n",
    "4. **Iteration**: Repeat the assignment and update steps until convergence (i.e., the assignments no longer change).\n",
    "\n",
    "Let's execute this process.\n",
    "\n",
    "After running the simplified K-means algorithm, we've quantized the vectors into 2 clusters over 2 iterations, with the final centroids and cluster assignments as follows:\n",
    "\n",
    "- **Cluster 1** (with centroid at approximately $[3.8, 2.6]$):\n",
    "    - Vectors: $(2, 3)$, $(3, 4)$, $(9, 1)$, $(2, 2)$, $(3, 3)$\n",
    "\n",
    "- **Cluster 2** (with centroid at approximately $[6.0, 7.0]$):\n",
    "    - Vectors: $(5, 6)$, $(7, 8)$\n",
    "\n",
    "### Explanation:\n",
    "1. **Initialization**: We started with initial centroids at $(2, 3)$ and $(7, 8)$.\n",
    "2. **Assignment**: Each vector was assigned to the nearest centroid based on Euclidean distance.\n",
    "3. **Update**: The centroids were updated based on the mean of the vectors assigned to them.\n",
    "4. **Iteration**: The process repeated until the assignments no longer changed, which took 2 iterations in this case.\n",
    "\n",
    "This example illustrates the basic concept of vector quantization, where vectors are quantized into clusters represented by centroids. This method is widely applied in various fields, including image compression, where it helps in reducing the amount of data needed to represent images by quantizing pixel values into a smaller set of representative colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc78888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: [array([2, 3]),\n",
       "   array([3, 4]),\n",
       "   array([9, 1]),\n",
       "   array([2, 2]),\n",
       "   array([3, 3])],\n",
       "  1: [array([5, 6]), array([7, 8])]},\n",
       " array([[3.8, 2.6],\n",
       "        [6. , 7. ]]),\n",
       " 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given set of 2D points\n",
    "V = np.array([[2, 3], [3, 4], [5, 6], [7, 8], [9, 1], [2, 2], [3, 3]])\n",
    "\n",
    "# Initial centroids\n",
    "centroids = np.array([[2, 3], [7, 8]])\n",
    "\n",
    "def k_means(V, centroids):\n",
    "    has_converged = False\n",
    "    iterations = 0\n",
    "    while not has_converged:\n",
    "        iterations += 1\n",
    "        # Assignment step\n",
    "        clusters = {}\n",
    "        for x in V:\n",
    "            distances = np.linalg.norm(x - centroids, axis=1)\n",
    "            closest_centroid = np.argmin(distances)\n",
    "            if closest_centroid in clusters:\n",
    "                clusters[closest_centroid].append(x)\n",
    "            else:\n",
    "                clusters[closest_centroid] = [x]\n",
    "        \n",
    "        # Update step\n",
    "        new_centroids = []\n",
    "        for i in range(len(centroids)):\n",
    "            if i in clusters:\n",
    "                new_centroids.append(np.mean(clusters[i], axis=0))\n",
    "            else:\n",
    "                new_centroids.append(centroids[i])\n",
    "        \n",
    "        new_centroids = np.array(new_centroids)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.array_equal(new_centroids, centroids):\n",
    "            has_converged = True\n",
    "        else:\n",
    "            centroids = new_centroids\n",
    "    \n",
    "    return clusters, centroids, iterations\n",
    "\n",
    "clusters, final_centroids, iterations = k_means(V, centroids)\n",
    "\n",
    "clusters, final_centroids, iterations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
