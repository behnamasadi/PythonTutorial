{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4f4b0a",
   "metadata": {},
   "source": [
    "Belief Propagation (BP) is an algorithm used for inference in graphical models, such as Bayesian networks and factor graphs. It operates by passing \"messages\" between nodes (variables and factors) in the graph. BP can be used for both exact inference in tree-structured graphs and approximate inference in loopy graphs.\n",
    "\n",
    "Let's break down Belief Propagation using a simple example:\n",
    "\n",
    "### Example: Disease Diagnosis\n",
    "\n",
    "Imagine a simple medical scenario where a patient might have a disease (D) based on two symptoms: fever (F) and cough (C). We want to determine the probability of the disease given the observed symptoms.\n",
    "\n",
    "The Bayesian network might look like this:\n",
    "\n",
    "```\n",
    "  D\n",
    " / \\\n",
    "F   C\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $ P(D) $ is the prior probability of the disease.\n",
    "- $ P(F|D) $ is the probability of fever given the disease.\n",
    "- $ P(C|D) $ is the probability of cough given the disease.\n",
    "\n",
    "Given some observations, say $ F = \\text{true} $ and $ C = \\text{true} $, we want to compute $ P(D|F, C) $.\n",
    "\n",
    "### Belief Propagation Steps:\n",
    "\n",
    "1. **Initialization**: Each node initializes its beliefs based on its local information. For instance, the symptom nodes (F and C) will initialize their beliefs based on the observed values.\n",
    "\n",
    "2. **Message Passing**:\n",
    "   - The symptom nodes (F and C) send messages to the disease node (D). These messages represent the evidence from each symptom about the disease.\n",
    "   - For our example, the message from F to D might be computed as:\n",
    "     $ m_{F \\to D}(D) = \\sum_{F} P(F|D) \\times \\text{belief}(F) $\n",
    "   - Similarly, a message is computed from C to D.\n",
    "\n",
    "3. **Update Beliefs**: Each node updates its belief based on incoming messages. For the disease node:\n",
    "   $ \\text{belief}(D) \\propto P(D) \\times m_{F \\to D}(D) \\times m_{C \\to D}(D) $\n",
    "\n",
    "4. **Iterate**: If the graph had loops (ours doesn't), we would repeat the message passing until convergence.\n",
    "\n",
    "5. **Extract Marginals**: The final beliefs at each node give the marginal probabilities. In our case, the belief at node D gives $ P(D|F, C) $.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Belief Propagation provides a systematic way to combine local information (like symptom observations) with global information (like the structure of the Bayesian network and the conditional probabilities) to compute the desired probabilities. In tree-structured graphs, BP gives exact results. In graphs with loops, BP can be used as an approximation method, often referred to as \"Loopy Belief Propagation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9736e195",
   "metadata": {},
   "source": [
    "Let's consider a example involving a small student network to determine the likelihood of a student getting a job offer based on various factors.\n",
    "\n",
    "### Example: Job Offer Prediction\n",
    "\n",
    "Imagine a scenario where a student's likelihood of getting a job offer (J) depends on their internship experience (I), their GPA (G), and their performance in a technical interview (T). Additionally, the student's performance in the technical interview is influenced by their coding skills (C) and problem-solving skills (P).\n",
    "\n",
    "The Bayesian network might look like this:\n",
    "\n",
    "```\n",
    "  I   G\n",
    "   \\ / \\\n",
    "    J   T\n",
    "       / \\\n",
    "      C   P\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $ P(J) $ is the prior probability of getting a job offer.\n",
    "- $ P(I) $ and $ P(G) $ are the probabilities of having an internship and a good GPA, respectively.\n",
    "- $ P(T|C,P) $ is the probability of performing well in the technical interview given coding and problem-solving skills.\n",
    "- $ P(J|I,G,T) $ is the probability of getting a job offer given the internship, GPA, and interview performance.\n",
    "\n",
    "Given some observations, say $ I = \\text{true} $, $ G = \\text{true} $, $ C = \\text{true} $, and $ P = \\text{true} $, we want to compute $ P(J|I, G, C, P) $.\n",
    "\n",
    "### Belief Propagation Steps:\n",
    "\n",
    "1. **Initialization**: Each node initializes its beliefs based on its local information. For instance, the nodes I, G, C, and P will initialize their beliefs based on the observed values.\n",
    "\n",
    "2. **Message Passing**:\n",
    "   - The nodes C and P send messages to the node T. These messages represent the evidence from each skill about the interview performance.\n",
    "   - For our example, the message from C to T might be computed as:\n",
    "     $ m_{C \\to T}(T) = \\sum_{C} P(T|C,P) \\times \\text{belief}(C) $\n",
    "   - Similarly, a message is computed from P to T.\n",
    "   - Nodes I and G send messages to J.\n",
    "   - After T has received messages from C and P, it computes its belief and sends a message to J.\n",
    "\n",
    "3. **Update Beliefs**: Each node updates its belief based on incoming messages. For the job offer node J:\n",
    "   $ \\text{belief}(J) \\propto P(J) \\times m_{I \\to J}(J) \\times m_{G \\to J}(J) \\times m_{T \\to J}(J) $\n",
    "\n",
    "4. **Iterate**: If the graph had loops (ours doesn't), we would repeat the message passing until convergence.\n",
    "\n",
    "5. **Extract Marginals**: The final beliefs at each node give the marginal probabilities. In our case, the belief at node J gives $ P(J|I, G, C, P) $.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "This example showcases how Belief Propagation can be used in a more complex scenario to combine evidence from multiple sources to compute the desired probabilities. The actual computations would involve specific probability values for each of the conditional probabilities, but the process remains the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e3d5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0,): 0.37068965517241376, (1,): 0.6293103448275862}\n"
     ]
    }
   ],
   "source": [
    "def normalize(factor):\n",
    "    \"\"\"Normalize a factor.\"\"\"\n",
    "    total = sum(factor.values())\n",
    "    for k in factor:\n",
    "        factor[k] /= total\n",
    "    return factor\n",
    "\n",
    "# Define the factors (CPDs)\n",
    "P_I = {(0,): 0.7, (1,): 0.3}\n",
    "P_G = {(0,): 0.6, (1,): 0.4}\n",
    "P_C = {(0,): 0.5, (1,): 0.5}\n",
    "P_P = {(0,): 0.6, (1,): 0.4}\n",
    "P_J_IG = {(0, 0, 0): 0.1, (1, 0, 0): 0.9,\n",
    "          (0, 1, 0): 0.2, (1, 1, 0): 0.8,\n",
    "          (0, 0, 1): 0.3, (1, 0, 1): 0.7,\n",
    "          (0, 1, 1): 0.5, (1, 1, 1): 0.5}\n",
    "\n",
    "\n",
    "# P(T|J,C,P)\n",
    "P_T_JCP = {\n",
    "    (0, 0, 0, 0): 0.1, (1, 0, 0, 0): 0.9,\n",
    "    (0, 0, 0, 1): 0.2, (1, 0, 0, 1): 0.8,\n",
    "    (0, 0, 1, 0): 0.3, (1, 0, 1, 0): 0.7,\n",
    "    (0, 0, 1, 1): 0.4, (1, 0, 1, 1): 0.6,\n",
    "    (0, 1, 0, 0): 0.5, (1, 1, 0, 0): 0.5,\n",
    "    (0, 1, 0, 1): 0.6, (1, 1, 0, 1): 0.4,\n",
    "    (0, 1, 1, 0): 0.7, (1, 1, 1, 0): 0.3,\n",
    "    (0, 1, 1, 1): 0.8, (1, 1, 1, 1): 0.2\n",
    "}\n",
    "\n",
    "\n",
    "# Messages from leaf nodes to their parents\n",
    "\n",
    "# Messages from leaf nodes to their parents\n",
    "msg_I_to_J = P_I\n",
    "msg_G_to_J = P_G\n",
    "msg_C_to_T = P_C\n",
    "msg_P_to_T = P_P\n",
    "\n",
    "# Message from J to T\n",
    "msg_J_to_T = {}\n",
    "for j_val in [0, 1]:\n",
    "    sum_val = 0\n",
    "    for i_val in [0, 1]:\n",
    "        for g_val in [0, 1]:\n",
    "            sum_val += P_J_IG[(j_val, i_val, g_val)] * msg_I_to_J[(i_val,)] * msg_G_to_J[(g_val,)]\n",
    "    msg_J_to_T[(j_val,)] = sum_val\n",
    "\n",
    "# Compute belief for T\n",
    "belief_T = {}\n",
    "for t_val in [0, 1]:\n",
    "    for j_val in [0, 1]:\n",
    "        for c_val in [0, 1]:\n",
    "            for p_val in [0, 1]:\n",
    "                belief_T[(t_val,)] = P_T_JCP[(t_val, j_val, c_val, p_val)] * msg_J_to_T[(j_val,)] * msg_C_to_T[(c_val,)] * msg_P_to_T[(p_val,)]\n",
    "belief_T = normalize(belief_T)\n",
    "\n",
    "# Message from T to J\n",
    "msg_T_to_J = {}\n",
    "for j_val in [0, 1]:\n",
    "    sum_val = 0\n",
    "    for t_val in [0, 1]:\n",
    "        for c_val in [0, 1]:\n",
    "            for p_val in [0, 1]:\n",
    "                sum_val += P_T_JCP[(t_val, j_val, c_val, p_val)] * belief_T[(t_val,)] * msg_C_to_T[(c_val,)] * msg_P_to_T[(p_val,)]\n",
    "    msg_T_to_J[(j_val,)] = sum_val\n",
    "\n",
    "# Compute belief for J\n",
    "belief_J = {}\n",
    "for j_val in [0, 1]:\n",
    "    for i_val in [0, 1]:\n",
    "        for g_val in [0, 1]:\n",
    "            belief_J[(j_val,)] = P_J_IG[(j_val, i_val, g_val)] * msg_I_to_J[(i_val,)] * msg_G_to_J[(g_val,)] * msg_T_to_J[(j_val,)]\n",
    "belief_J = normalize(belief_J)\n",
    "\n",
    "print(belief_J)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119395cd",
   "metadata": {},
   "source": [
    "The use of `(0,)` as a key is a Pythonic way to represent a tuple with a single element. In Python, simply putting a value inside parentheses, like `(0)`, does not create a tuple. To explicitly create a single-element tuple, a comma is added after the value, resulting in `(0,)`.\n",
    "\n",
    "In the provided code, I used tuples to represent combinations of variable states. This allows for easy expansion to multiple variable states if needed. For instance, if we had a factor involving two binary variables, the keys could be `(0, 0)`, `(0, 1)`, `(1, 0)`, and `(1, 1)`. Using tuples in this manner provides a consistent and scalable way to represent and access the states of multiple variables.\n",
    "\n",
    "For the single-variable factors, such as `P_I`, the keys are single-element tuples like `(0,)` and `(1,)` to maintain this consistent representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96faa873",
   "metadata": {},
   "source": [
    "## Belief Propagation (BP) and Variable Elimination (VE)\n",
    "\n",
    "Belief Propagation (BP) and Variable Elimination (VE) are both algorithms used for inference in graphical models, such as Bayesian networks and factor graphs. They serve the same purpose, which is to compute marginal probabilities or joint probabilities of interest. However, they approach the problem in different ways and have different strengths and weaknesses. Here's a comparison:\n",
    "\n",
    "### 1. **Algorithmic Approach**:\n",
    "- **Belief Propagation**: BP operates by iteratively passing messages between nodes in the graph. In tree-structured graphs, BP can compute exact marginals in a linear number of steps. In graphs with loops, BP can be used as an approximation method, often referred to as \"Loopy Belief Propagation.\"\n",
    "  \n",
    "- **Variable Elimination**: VE operates by systematically eliminating variables from the graph. It does this by summing out variables, which results in new factors (or potentials). This process continues until only the variables of interest remain.\n",
    "\n",
    "### 2. **Complexity**:\n",
    "- **Belief Propagation**: In tree-structured graphs, the complexity is linear in the size of the graph. However, in loopy graphs, convergence is not guaranteed, and the number of iterations can vary.\n",
    "  \n",
    "- **Variable Elimination**: The complexity can be exponential in the size of the largest factor created during the elimination process. The order in which variables are eliminated can greatly affect the efficiency of the algorithm.\n",
    "\n",
    "### 3. **Exact vs. Approximate**:\n",
    "- **Belief Propagation**: BP gives exact results in tree-structured graphs. In loopy graphs, it's an approximation method.\n",
    "  \n",
    "- **Variable Elimination**: VE always gives exact results but can be computationally expensive or even infeasible for large graphs with many dependencies.\n",
    "\n",
    "### 4. **Relation**:\n",
    "- At a high level, both BP and VE can be seen as methods to exploit the structure of graphical models to perform efficient inference. \n",
    "- The message passing in BP can be related to the intermediate computations in VE. In fact, the messages passed in BP can be seen as the results of partial variable eliminations.\n",
    "- Both algorithms can be viewed as ways to distribute and simplify the computation of joint probabilities across the structure of the graph.\n",
    "\n",
    "### 5. **Use Cases**:\n",
    "- **Belief Propagation**: BP is particularly useful in scenarios where the graph has a tree structure, such as in decoding certain error-correcting codes. It's also used in approximate form in many real-world scenarios with loopy graphs.\n",
    "  \n",
    "- **Variable Elimination**: VE is often used in Bayesian networks for diagnostic reasoning, especially when the network isn't too large or when a good elimination order is known.\n",
    "\n",
    "In summary, while both Belief Propagation and Variable Elimination aim to solve the same inference problems in graphical models, they have different algorithmic foundations, complexities, and use cases. The choice between them often depends on the specific problem, the structure of the graph, and computational considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886cc6c",
   "metadata": {},
   "source": [
    "## Does Belief Propagation and variable elimination give you the same result?\n",
    "\n",
    "Yes, when applied to tree-structured graphical models (i.e., graphical models without loops), both Belief Propagation (BP) and Variable Elimination (VE) will give you the exact same result for the marginal probabilities of the variables. This is because both methods exploit the factorized structure of the model to compute the marginals, albeit through different procedures.\n",
    "\n",
    "However, there are some nuances to consider:\n",
    "\n",
    "1. **Loopy Graphs**: In graphical models with loops, exact inference using Variable Elimination can still be done, but it might be computationally expensive due to the potential explosion in the size of intermediate factors. On the other hand, standard Belief Propagation can be applied to loopy graphs, but it becomes an approximation method known as \"Loopy Belief Propagation.\" In such cases, BP might not converge, or if it does, it might not provide the exact marginals.\n",
    "\n",
    "2. **Efficiency**: The efficiency of Variable Elimination is highly dependent on the order in which variables are eliminated. A poor ordering can lead to the creation of large intermediate factors, making the computation expensive. Belief Propagation, in tree-structured graphs, has a linear complexity in the size of the graph.\n",
    "\n",
    "3. **Use Cases**: While both methods can be used for inference in graphical models, in practice, the choice between them often depends on the specific problem and the structure of the graph. For instance, BP is commonly used in decoding algorithms for error-correcting codes, while VE might be preferred for diagnostic reasoning in Bayesian networks.\n",
    "\n",
    "In summary, for tree-structured models, both Belief Propagation and Variable Elimination will yield the same, exact results. For loopy models, exact inference with VE is still possible, but BP becomes an approximation method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
