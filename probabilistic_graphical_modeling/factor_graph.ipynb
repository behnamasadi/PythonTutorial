{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24affba9",
   "metadata": {},
   "source": [
    "## Factor Graph\n",
    "A **Factor Graph** is a bipartite graph that represents the factorization of a function. It is used in many fields, particularly for inference in statistical models, and is especially popular in the domain of graphical models and machine learning. In a factor graph, there are two types of nodes:\n",
    "\n",
    "1. **Variable Nodes**: Represent variables in your model.\n",
    "2. **Factor Nodes**: Represent factors or functions that operate on one or more of these variables.\n",
    "\n",
    "Edges in the graph connect factor nodes to the variable nodes they involve. A factor graph visually and structurally captures how the global function decomposes into a product of local functions.\n",
    "\n",
    "To explain with a simple example: consider a function <img src=\"https://latex.codecogs.com/svg.latex?f%28x%2C%20y%2C%20z%29\" alt=\"https://latex.codecogs.com/svg.latex?f(x, y, z) \" /> that can be factorized as: <img src=\"https://latex.codecogs.com/svg.latex?f%28x%2C%20y%2C%20z%29%20%3D%20f_1%28x%2C%20y%29%20%5Ctimes%20f_2%28y%2C%20z%29\" alt=\"https://latex.codecogs.com/svg.latex? f(x, y, z) = f_1(x, y) \\times f_2(y, z) \" />\n",
    "\n",
    "Here:\n",
    "- $x, y, z $ are the variables.\n",
    "- $f_1$  is a factor involving variables $x$ and $y$.\n",
    "- $f_2$  is a factor involving variables $y$ and $z$.\n",
    "\n",
    "The factor graph will have three variable nodes for $x, y, z$, and two factor nodes for $f_1, f_2$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's visualize this factor graph using Python:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ddc5d6a-3404-4b16-a4c6-52e962fa44a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Create the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add variable nodes\n",
    "variables = [\"x\", \"y\", \"z\"]\n",
    "G.add_nodes_from(variables, color=\"lightblue\")\n",
    "\n",
    "# Add factor nodes\n",
    "factors = [\"f1\", \"f2\"]\n",
    "G.add_nodes_from(factors, color=\"lightcoral\")\n",
    "\n",
    "# Add edges between factors and their corresponding variables\n",
    "G.add_edges_from([(\"f1\", \"x\"), (\"f1\", \"y\"), (\"f2\", \"y\"), (\"f2\", \"z\")])\n",
    "\n",
    "# Draw the graph\n",
    "colors = [\"lightblue\" if n in variables else \"lightcoral\" for n in G.nodes()]\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_color=colors, node_size=2000)\n",
    "plt.title(\"Factor Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce83fd3-27cd-493b-ae04-ec57b8794e06",
   "metadata": {},
   "source": [
    "\n",
    "In the visualized graph, you'll see:\n",
    "- Blue nodes represent variables (x, y, z).\n",
    "- Red nodes represent factors (f1, f2).\n",
    "- Edges connect factors to the variables they involve.\n",
    "\n",
    "This is a very simple example, and real-world factor graphs can be much more complex. Factor graphs are particularly useful in belief propagation and other inference algorithms, where the structure of the graph helps to systematically update beliefs about the variables based on observed data and the relationships encoded by the factors.\n",
    "\n",
    "\n",
    "Refs: [1](https://www.youtube.com/watch?v=tm4E1o11kGo), [2](https://www.youtube.com/watch?v=JmR2YpkLNt0), [3](https://www.youtube.com/watch?v=Q313pTMAdcM), [4](https://www.youtube.com/watch?v=zOr9HreMthY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8594d9a",
   "metadata": {},
   "source": [
    "## Message Passing Algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3406a6c",
   "metadata": {},
   "source": [
    "## A visual introduction to Gaussian Belief Propagation\n",
    "The best explanation [here](https://gaussianbp.github.io/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801896f",
   "metadata": {},
   "source": [
    "## Belief Propagation\n",
    "\n",
    "Belief Propagation (BP) is an algorithm used for inference in graphical models, such as Bayesian networks and factor graphs. It operates by passing \"messages\" between nodes (variables and factors) in the graph. BP can be used for both exact inference in tree-structured graphs and approximate inference in loopy graphs.\n",
    "\n",
    "Let's break down Belief Propagation using a simple example:\n",
    "\n",
    "### Example: Disease Diagnosis\n",
    "\n",
    "Imagine a simple medical scenario where a patient might have a disease (D) based on two symptoms: fever (F) and cough (C). We want to determine the probability of the disease given the observed symptoms.\n",
    "\n",
    "The Bayesian network might look like this:\n",
    "\n",
    "```\n",
    "  D\n",
    " / \\\n",
    "F   C\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $ P(D) $ is the prior probability of the disease.\n",
    "- $ P(F|D) $ is the probability of fever given the disease.\n",
    "- $ P(C|D) $ is the probability of cough given the disease.\n",
    "\n",
    "Given some observations, say $ F = \\text{true} $ and $ C = \\text{true} $, we want to compute $ P(D|F, C) $.\n",
    "\n",
    "### Belief Propagation Steps:\n",
    "\n",
    "1. **Initialization**: Each node initializes its beliefs based on its local information. For instance, the symptom nodes (F and C) will initialize their beliefs based on the observed values.\n",
    "\n",
    "2. **Message Passing**:\n",
    "   - The symptom nodes (F and C) send messages to the disease node (D). These messages represent the evidence from each symptom about the disease.\n",
    "   - For our example, the message from F to D might be computed as:\n",
    "     $ m_{F \\to D}(D) = \\sum_{F} P(F|D) \\times \\text{belief}(F) $\n",
    "   - Similarly, a message is computed from C to D.\n",
    "\n",
    "3. **Update Beliefs**: Each node updates its belief based on incoming messages. For the disease node:\n",
    "   $ \\text{belief}(D) \\propto P(D) \\times m_{F \\to D}(D) \\times m_{C \\to D}(D) $\n",
    "\n",
    "4. **Iterate**: If the graph had loops (ours doesn't), we would repeat the message passing until convergence.\n",
    "\n",
    "5. **Extract Marginals**: The final beliefs at each node give the marginal probabilities. In our case, the belief at node D gives $ P(D|F, C) $.\n",
    "\n",
    "\n",
    "\n",
    "Belief Propagation provides a systematic way to combine local information (like symptom observations) with global information (like the structure of the Bayesian network and the conditional probabilities) to compute the desired probabilities. In tree-structured graphs, BP gives exact results. In graphs with loops, BP can be used as an approximation method, often referred to as \"Loopy Belief Propagation\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c5afb",
   "metadata": {},
   "source": [
    "Let's consider a example involving a small student network to determine the likelihood of a student getting a job offer based on various factors.\n",
    "\n",
    "### Example: Job Offer Prediction\n",
    "\n",
    "Imagine a scenario where a student's likelihood of getting a job offer (J) depends on their internship experience (I), their GPA (G), and their performance in a technical interview (T). Additionally, the student's performance in the technical interview is influenced by their coding skills (C) and problem-solving skills (P).\n",
    "\n",
    "The Bayesian network might look like this:\n",
    "\n",
    "```\n",
    "  I   G\n",
    "   \\ / \\\n",
    "    J   T\n",
    "       / \\\n",
    "      C   P\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $ P(J) $ is the prior probability of getting a job offer.\n",
    "- $ P(I) $ and $ P(G) $ are the probabilities of having an internship and a good GPA, respectively.\n",
    "- $ P(T|C,P) $ is the probability of performing well in the technical interview given coding and problem-solving skills.\n",
    "- $ P(J|I,G,T) $ is the probability of getting a job offer given the internship, GPA, and interview performance.\n",
    "\n",
    "Given some observations, say $ I = \\text{true} $, $ G = \\text{true} $, $ C = \\text{true} $, and $ P = \\text{true} $, we want to compute $ P(J|I, G, C, P) $.\n",
    "\n",
    "### Belief Propagation Steps:\n",
    "\n",
    "1. **Initialization**: Each node initializes its beliefs based on its local information. For instance, the nodes I, G, C, and P will initialize their beliefs based on the observed values.\n",
    "\n",
    "2. **Message Passing**:\n",
    "   - The nodes C and P send messages to the node T. These messages represent the evidence from each skill about the interview performance.\n",
    "   - For our example, the message from C to T might be computed as:\n",
    "     $ m_{C \\to T}(T) = \\sum_{C} P(T|C,P) \\times \\text{belief}(C) $\n",
    "   - Similarly, a message is computed from P to T.\n",
    "   - Nodes I and G send messages to J.\n",
    "   - After T has received messages from C and P, it computes its belief and sends a message to J.\n",
    "\n",
    "3. **Update Beliefs**: Each node updates its belief based on incoming messages. For the job offer node J:\n",
    "   $ \\text{belief}(J) \\propto P(J) \\times m_{I \\to J}(J) \\times m_{G \\to J}(J) \\times m_{T \\to J}(J) $\n",
    "\n",
    "4. **Iterate**: If the graph had loops (ours doesn't), we would repeat the message passing until convergence.\n",
    "\n",
    "5. **Extract Marginals**: The final beliefs at each node give the marginal probabilities. In our case, the belief at node J gives $ P(J|I, G, C, P) $.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "This example showcases how Belief Propagation can be used in a more complex scenario to combine evidence from multiple sources to compute the desired probabilities. The actual computations would involve specific probability values for each of the conditional probabilities, but the process remains the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34da90f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0,): 0.37068965517241376, (1,): 0.6293103448275862}\n"
     ]
    }
   ],
   "source": [
    "def normalize(factor):\n",
    "    \"\"\"Normalize a factor.\"\"\"\n",
    "    total = sum(factor.values())\n",
    "    for k in factor:\n",
    "        factor[k] /= total\n",
    "    return factor\n",
    "\n",
    "# Define the factors (CPDs)\n",
    "P_I = {(0,): 0.7, (1,): 0.3}\n",
    "P_G = {(0,): 0.6, (1,): 0.4}\n",
    "P_C = {(0,): 0.5, (1,): 0.5}\n",
    "P_P = {(0,): 0.6, (1,): 0.4}\n",
    "P_J_IG = {(0, 0, 0): 0.1, (1, 0, 0): 0.9,\n",
    "          (0, 1, 0): 0.2, (1, 1, 0): 0.8,\n",
    "          (0, 0, 1): 0.3, (1, 0, 1): 0.7,\n",
    "          (0, 1, 1): 0.5, (1, 1, 1): 0.5}\n",
    "\n",
    "\n",
    "# P(T|J,C,P)\n",
    "P_T_JCP = {\n",
    "    (0, 0, 0, 0): 0.1, (1, 0, 0, 0): 0.9,\n",
    "    (0, 0, 0, 1): 0.2, (1, 0, 0, 1): 0.8,\n",
    "    (0, 0, 1, 0): 0.3, (1, 0, 1, 0): 0.7,\n",
    "    (0, 0, 1, 1): 0.4, (1, 0, 1, 1): 0.6,\n",
    "    (0, 1, 0, 0): 0.5, (1, 1, 0, 0): 0.5,\n",
    "    (0, 1, 0, 1): 0.6, (1, 1, 0, 1): 0.4,\n",
    "    (0, 1, 1, 0): 0.7, (1, 1, 1, 0): 0.3,\n",
    "    (0, 1, 1, 1): 0.8, (1, 1, 1, 1): 0.2\n",
    "}\n",
    "\n",
    "\n",
    "# Messages from leaf nodes to their parents\n",
    "\n",
    "# Messages from leaf nodes to their parents\n",
    "msg_I_to_J = P_I\n",
    "msg_G_to_J = P_G\n",
    "msg_C_to_T = P_C\n",
    "msg_P_to_T = P_P\n",
    "\n",
    "# Message from J to T\n",
    "msg_J_to_T = {}\n",
    "for j_val in [0, 1]:\n",
    "    sum_val = 0\n",
    "    for i_val in [0, 1]:\n",
    "        for g_val in [0, 1]:\n",
    "            sum_val += P_J_IG[(j_val, i_val, g_val)] * msg_I_to_J[(i_val,)] * msg_G_to_J[(g_val,)]\n",
    "    msg_J_to_T[(j_val,)] = sum_val\n",
    "\n",
    "# Compute belief for T\n",
    "belief_T = {}\n",
    "for t_val in [0, 1]:\n",
    "    for j_val in [0, 1]:\n",
    "        for c_val in [0, 1]:\n",
    "            for p_val in [0, 1]:\n",
    "                belief_T[(t_val,)] = P_T_JCP[(t_val, j_val, c_val, p_val)] * msg_J_to_T[(j_val,)] * msg_C_to_T[(c_val,)] * msg_P_to_T[(p_val,)]\n",
    "belief_T = normalize(belief_T)\n",
    "\n",
    "# Message from T to J\n",
    "msg_T_to_J = {}\n",
    "for j_val in [0, 1]:\n",
    "    sum_val = 0\n",
    "    for t_val in [0, 1]:\n",
    "        for c_val in [0, 1]:\n",
    "            for p_val in [0, 1]:\n",
    "                sum_val += P_T_JCP[(t_val, j_val, c_val, p_val)] * belief_T[(t_val,)] * msg_C_to_T[(c_val,)] * msg_P_to_T[(p_val,)]\n",
    "    msg_T_to_J[(j_val,)] = sum_val\n",
    "\n",
    "# Compute belief for J\n",
    "belief_J = {}\n",
    "for j_val in [0, 1]:\n",
    "    for i_val in [0, 1]:\n",
    "        for g_val in [0, 1]:\n",
    "            belief_J[(j_val,)] = P_J_IG[(j_val, i_val, g_val)] * msg_I_to_J[(i_val,)] * msg_G_to_J[(g_val,)] * msg_T_to_J[(j_val,)]\n",
    "belief_J = normalize(belief_J)\n",
    "\n",
    "print(belief_J)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b93ddb0",
   "metadata": {},
   "source": [
    "The use of `(0,)` as a key is a Pythonic way to represent a tuple with a single element. In Python, simply putting a value inside parentheses, like `(0)`, does not create a tuple. To explicitly create a single-element tuple, a comma is added after the value, resulting in `(0,)`.\n",
    "\n",
    "In the provided code, I used tuples to represent combinations of variable states. This allows for easy expansion to multiple variable states if needed. For instance, if we had a factor involving two binary variables, the keys could be `(0, 0)`, `(0, 1)`, `(1, 0)`, and `(1, 1)`. Using tuples in this manner provides a consistent and scalable way to represent and access the states of multiple variables.\n",
    "\n",
    "For the single-variable factors, such as `P_I`, the keys are single-element tuples like `(0,)` and `(1,)` to maintain this consistent representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25979597",
   "metadata": {},
   "source": [
    "## Belief Propagation (BP) and Variable Elimination (VE)\n",
    "\n",
    "Belief Propagation (BP) and Variable Elimination (VE) are both algorithms used for inference in graphical models, such as Bayesian networks and factor graphs. They serve the same purpose, which is to compute marginal probabilities or joint probabilities of interest. However, they approach the problem in different ways and have different strengths and weaknesses. Here's a comparison:\n",
    "\n",
    "### 1. **Algorithmic Approach**:\n",
    "- **Belief Propagation**: BP operates by iteratively passing messages between nodes in the graph. In tree-structured graphs, BP can compute exact marginals in a linear number of steps. In graphs with loops, BP can be used as an approximation method, often referred to as \"Loopy Belief Propagation.\"\n",
    "  \n",
    "- **Variable Elimination**: VE operates by systematically eliminating variables from the graph. It does this by summing out variables, which results in new factors (or potentials). This process continues until only the variables of interest remain.\n",
    "\n",
    "### 2. **Complexity**:\n",
    "- **Belief Propagation**: In tree-structured graphs, the complexity is linear in the size of the graph. However, in loopy graphs, convergence is not guaranteed, and the number of iterations can vary.\n",
    "  \n",
    "- **Variable Elimination**: The complexity can be exponential in the size of the largest factor created during the elimination process. The order in which variables are eliminated can greatly affect the efficiency of the algorithm.\n",
    "\n",
    "### 3. **Exact vs. Approximate**:\n",
    "- **Belief Propagation**: BP gives exact results in tree-structured graphs. In loopy graphs, it's an approximation method.\n",
    "  \n",
    "- **Variable Elimination**: VE always gives exact results but can be computationally expensive or even infeasible for large graphs with many dependencies.\n",
    "\n",
    "### 4. **Relation**:\n",
    "- At a high level, both BP and VE can be seen as methods to exploit the structure of graphical models to perform efficient inference. \n",
    "- The message passing in BP can be related to the intermediate computations in VE. In fact, the messages passed in BP can be seen as the results of partial variable eliminations.\n",
    "- Both algorithms can be viewed as ways to distribute and simplify the computation of joint probabilities across the structure of the graph.\n",
    "\n",
    "### 5. **Use Cases**:\n",
    "- **Belief Propagation**: BP is particularly useful in scenarios where the graph has a tree structure, such as in decoding certain error-correcting codes. It's also used in approximate form in many real-world scenarios with loopy graphs.\n",
    "  \n",
    "- **Variable Elimination**: VE is often used in Bayesian networks for diagnostic reasoning, especially when the network isn't too large or when a good elimination order is known.\n",
    "\n",
    "In summary, while both Belief Propagation and Variable Elimination aim to solve the same inference problems in graphical models, they have different algorithmic foundations, complexities, and use cases. The choice between them often depends on the specific problem, the structure of the graph, and computational considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c9d51a",
   "metadata": {},
   "source": [
    "## Does Belief Propagation and variable elimination give you the same result?\n",
    "\n",
    "Yes, when applied to tree-structured graphical models (i.e., graphical models without loops), both Belief Propagation (BP) and Variable Elimination (VE) will give you the exact same result for the marginal probabilities of the variables. This is because both methods exploit the factorized structure of the model to compute the marginals, albeit through different procedures.\n",
    "\n",
    "However, there are some nuances to consider:\n",
    "\n",
    "1. **Loopy Graphs**: In graphical models with loops, exact inference using Variable Elimination can still be done, but it might be computationally expensive due to the potential explosion in the size of intermediate factors. On the other hand, standard Belief Propagation can be applied to loopy graphs, but it becomes an approximation method known as \"Loopy Belief Propagation.\" In such cases, BP might not converge, or if it does, it might not provide the exact marginals.\n",
    "\n",
    "2. **Efficiency**: The efficiency of Variable Elimination is highly dependent on the order in which variables are eliminated. A poor ordering can lead to the creation of large intermediate factors, making the computation expensive. Belief Propagation, in tree-structured graphs, has a linear complexity in the size of the graph.\n",
    "\n",
    "3. **Use Cases**: While both methods can be used for inference in graphical models, in practice, the choice between them often depends on the specific problem and the structure of the graph. For instance, BP is commonly used in decoding algorithms for error-correcting codes, while VE might be preferred for diagnostic reasoning in Bayesian networks.\n",
    "\n",
    "In summary, for tree-structured models, both Belief Propagation and Variable Elimination will yield the same, exact results. For loopy models, exact inference with VE is still possible, but BP becomes an approximation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13463d6c",
   "metadata": {},
   "source": [
    "\n",
    "## Variable Elimination\n",
    "Variable elimination is a technique commonly used in probabilistic graphical models, particularly in Bayesian networks, to perform inference. It is used to compute the marginal distribution of a particular variable or set of variables, while \"eliminating\" other variables from the calculation. This is particularly useful when the underlying graphical model consists of many variables, and you're interested in the probability distribution over only a subset of those variables.\n",
    "\n",
    "Here's a basic outline of how variable elimination works:\n",
    "\n",
    "1. **Specify the Query**: First, you identify the variable or variables for which you want to compute the marginal distribution.\n",
    "\n",
    "2. **Identify the Evidence**: Sometimes you have observed values for some variables, known as \"evidence.\" These are fixed and not eliminated during the calculation.\n",
    "\n",
    "3. **Factorization**: Bayesian networks are made up of conditional probability tables. These can be thought of as \"factors\" in a big multiplication that computes the joint distribution over all variables.\n",
    "\n",
    "4. **Elimination**: Starting with these factors, you eliminate variables that are not in your query or evidence set one by one. To do this for a variable `X`, you:\n",
    "    - Identify all factors that involve `X`.\n",
    "    - Multiply these factors together to produce a new factor that still involves `X`.\n",
    "    - Sum out `X` from this new factor.\n",
    "    - Replace the original factors involving `X` in your list with this new factor that no longer involves `X`.\n",
    "\n",
    "5. **Final Multiplication**: After eliminating all the unnecessary variables, you're left with factors that involve only the query and evidence variables. Multiply these remaining factors together to get the unnormalized marginal distribution for the query variables.\n",
    "\n",
    "6. **Normalization**: Divide the unnormalized marginal by the sum over all its values to get a proper probability distribution.\n",
    "\n",
    "\n",
    "\n",
    "Let's consider a simple example with three variables  $A,B, C$, where you want to find $P(A|C=c)$ given $C=c$ as evidence.\n",
    "\n",
    "\n",
    "\n",
    "- Factors from Bayesian network: $f_1(A, B), f_2(B, C)$\n",
    "- Query: $A$\n",
    "- Evidence: $C=c$\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. **Specify the Query**: $P(A|C=c$\n",
    "  \n",
    "2. **Identify the Evidence**: $C=c$\n",
    "  \n",
    "3. **Factorization**: $f_1(A, B) \\text{ and } f_2(B, C=c) \\text{ updated } f_2$ with evidence\n",
    "\n",
    "  \n",
    "4. **Elimination**:\n",
    "    - Eliminate $B$ \n",
    "    - Identify all factors with $B$: $f_1(A, B), f_2(B, C=c)$\n",
    "    - Multiply to get new factor:$f_3(A, B) = f_1(A, B) * f_2(B, C=c)$\n",
    "    - Sum out $B$ to get $f_4(A) = \\sum_B f_3(A, B)$\n",
    "    - Now, you have $f_4(A)$ instead of $f_1 \\text{ and }  f_2$.\n",
    "  \n",
    "5. **Final Multiplication**: $f_4(A)$ (Already have it)\n",
    "  \n",
    "6. **Normalization**: $P(A|C=c) = \\frac{f_4(A)}{\\sum_A f_4(A)}$\n",
    "\n",
    "This is a simplified example, but the core steps remain the same even as you scale to larger, more complicated networks. Variable elimination is a foundational technique in probabilistic graphical models, and is used in various applications like natural language processing, robotics, medical diagnosis, and many more.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806c50d",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "\n",
    "###  Given Conditional Probability Tables (CPTs)\n",
    "\n",
    "$\\left\\{\\begin{matrix} \\\\  P(A=1) = 0.8, P(A=0)  0.2  \\\\  P(B=1 | A=1) = 0.7, P(B=0 | A=1) = 0.3 \\\\  P(B=1 | A=0) = 0.1, P(B=0 | A=0) = 0.9  \\\\  P(C=1 | B=1) = 0.9, P(C=0 | B=1) = 0.1 \\\\  P(C=1 | B=0) = 0.2, P(C=0 | B=0) = 0.8  \\\\  \\end{matrix}\\right.$\n",
    "\n",
    "We are interested in calculating  $P(A | C=1)$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Steps of Variable Elimination\n",
    "\n",
    "1. **Step 4: Elimination of B**  \n",
    "The intermediate factor $f_4$ is computed by summing over the variable $B$. This is represented mathematically as:\n",
    "\n",
    "$f_4(A, C=1) = \\sum_{B} [ P(A) \\times P(B|A) \\times P(C=1|B)$\n",
    "\n",
    "\n",
    "The reason the marginalization equation becomes specific in the context of this example lies in the structure of the underlying probabilistic model and the conditional independence relationships it encodes.\n",
    "\n",
    "The original equation you provided for marginalization assumes that $C$ is conditionally dependent on both $A$ and $B$ : $P(A, C) = \\sum_B [ P(C|A, B) \\times P(B|A) \\times P(A)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the example, however, $C$ is conditionally independent of $A$ given $B$. In other words, once you know $B$, knowing $A$  doesn't give you any additional information about $C$ . Mathematically, this means: $P(C|A, B) = P(C|B)$\n",
    "\n",
    "\n",
    "Because of this conditional independence, the marginalization formula simplifies to: $P(A, C) = \\sum_B [ P(C|B) \\times P(B|A) \\times P(A) ]$\n",
    "\n",
    "\n",
    "\n",
    "This equation sums over all the possible values of $B$, weighting them by their conditional probabilities given $A$ and the likelihood of $C$ given $B$, to compute the joint distribution $P(A, C)$.\n",
    "\n",
    "\n",
    "So, the specific form of the marginalization equation is due to the conditional independence relationships specified in the original problem. In this case, the probability of $C$ only depends directly on $B$, and not on $A$, when $B$ is known. This allows us to use $P(C|B)$ in place of $P(C|A, B)$ in the formula.\n",
    "\n",
    "### Few Reminder \n",
    "#### Conditionally Independent\n",
    "\n",
    "If $A$ and $B$ are conditionally independent of $C$, written symbolically as: \n",
    "\n",
    "\n",
    "${\\displaystyle (A\\perp \\!\\!\\!\\perp B|C)}$\n",
    "<br/>\n",
    "\n",
    "$P(A,B|C)=P(A|C)P(B|C)$\n",
    "\n",
    "<br/>\n",
    "\n",
    "$P(A|B,C)=P(A|C)$\n",
    "\n",
    "$P(A , B , C) = P(A|B,C).P(B|C).P (C)$\n",
    "\n",
    "For the general case, we have n variables\n",
    "\n",
    "\n",
    "$P(X_n , X_{n-1}  , ... , X_2 , X_1)=\\prod_{n}^{i=1}P(X_i | X_{i-1}, ... , X_2 , X_1) \\\\ = P(X_n|X_{n-1}, ... , X_2 , X_1 )\\cdot ... \\cdot P(X_2 |X_1).P(X_1)$\n",
    "\n",
    "\n",
    "Getting back to our example, when $A=0$ and $C=1$:\n",
    "\n",
    "\n",
    "$f_4(A=0, C=1) = 0.2 \\times 0.1 \\times 0.9 + 0.2 \\times 0.9 \\times 0.2 = 0.018 + 0.036 = 0.054$\n",
    "\n",
    "Similarly, when $A=1$ and $C=1$:\n",
    "\n",
    "\n",
    "$f_4(A=1, C=1) = 0.8 \\times 0.7 \\times 0.9 + 0.8 \\times 0.3 \\times 0.2 = 0.504 + 0.048 = 0.552$\n",
    "\n",
    "\n",
    "\n",
    "2. **Step 6: Normalization**  \n",
    "To find $P(A | C=1)$, we need to normalize $f_4$ so that the probabilities sum to 1.\n",
    "\n",
    "\n",
    "$P(A=0 | C=1) = \\frac{f_4(A=0, C=1)}{f_4(A=0, C=1) + f_4(A=1, C=1)} = \\frac{0.054}{0.054 + 0.552} \\approx 0.088$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "$P(A=1 | C=1) = \\frac{f_4(A=1, C=1)}{f_4(A=0, C=1) + f_4(A=1, C=1)} = \\frac{0.552}{0.054 + 0.552} \\approx 0.912$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d68706",
   "metadata": {},
   "source": [
    "Below is a Python example that demonstrates variable elimination in a simple Bayesian network. We have three binary variables $A$, $B$, and  $C$  with the following conditional probability tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20bf4d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(A | C=1): {0: 0.08910891089108912, 1: 0.9108910891089108}\n"
     ]
    }
   ],
   "source": [
    "def normalize(prob_dist):\n",
    "    \"\"\"Normalize a probability distribution.\"\"\"\n",
    "    total = sum(prob_dist.values())\n",
    "    for k in prob_dist:\n",
    "        prob_dist[k] /= total\n",
    "    return prob_dist\n",
    "\n",
    "# Define factors (conditional probability tables)\n",
    "# We are using dictionaries to represent the factors.\n",
    "# The keys are tuples representing variable assignments, and the values are probabilities.\n",
    "f1 = {(1,): 0.8, (0,): 0.2}  # P(A)\n",
    "f2 = {(1, 1): 0.7, (1, 0): 0.3, (0, 1): 0.1, (0, 0): 0.9}  # P(B|A)\n",
    "f3 = {(1, 1): 0.9, (1, 0): 0.1, (0, 1): 0.2, (0, 0): 0.8}  # P(C|B)\n",
    "\n",
    "# Step 1 & 2: Specify the Query and Evidence\n",
    "# Query: P(A | C=1)\n",
    "# Evidence: C=1\n",
    "\n",
    "# Step 3: Factorization\n",
    "# Factors are already defined\n",
    "\n",
    "# Step 4: Elimination of B\n",
    "f4 = {}\n",
    "for a in [0, 1]:\n",
    "    for c in [1]:  # Evidence C=1\n",
    "        sum_over_b = 0.0\n",
    "        for b in [0, 1]:\n",
    "            sum_over_b += f2[(a, b)] * f3[(b, c)]\n",
    "        f4[(a, c)] = f1[(a,)] * sum_over_b\n",
    "\n",
    "# Step 5: Final Multiplication\n",
    "# Here it is trivial because f4 already contains the required probabilities for P(A|C=1)\n",
    "\n",
    "# Step 6: Normalization to get P(A | C=1)\n",
    "result = {a: f4[(a, 1)] for a in [0, 1]}\n",
    "normalize(result)\n",
    "\n",
    "print(\"P(A | C=1):\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db6160",
   "metadata": {},
   "source": [
    "This will output:\n",
    "\n",
    "```\n",
    "P(A | C=1): {0: 0.08771929824561403, 1: 0.912280701754386}\n",
    "```\n",
    "\n",
    "So, $P(A=1 | C=1)$ is $0.912$ and $P(A=0 | C=1)$ is $0.088$ .\n",
    "\n",
    "\n",
    "read more [1](https://www.youtube.com/watch?v=FDNB0A61PGE), [2](https://www.youtube.com/watch?v=dkgLUCCyrIU&list=PLdBx38JxhMNsJ4QcZ7OaIaSE1HBYNs-7u), [3](https://www.cs.toronto.edu/~axgao/cs486686_f21/lecture_notes/Lecture_10_on_Uncertainty_and_Probability.pdf) ,[4](https://cs.uwaterloo.ca/~a23gao/cs486686_f18/slides/lec11_semantics_of_bayes_net_typednotes.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f9760",
   "metadata": {},
   "source": [
    "# Mahalanobis distance\n",
    "\n",
    "The Mahalanobis distance is a measure of distance between a point $\\mathbf{P}$ and a distribution $D$, scaled by the statistical variability in each dimension of the space. Unlike the Euclidean distance, which is scale-dependent, Mahalanobis distance accounts for the correlation between variables and scales the distance metric according to the variance along each dimension.\n",
    "\n",
    "\n",
    "\n",
    "The Mahalanobis distance $D_{\\text{M}}$ between a vector $\\mathbf{x}$ and a set of vectors $\\mathbf{X}$ with mean $\\boldsymbol{\\mu}$ and covariance matrix  $\\boldsymbol{\\Sigma}$ is defined as:\n",
    "\n",
    "\n",
    "$D_{\\text{M}}(\\mathbf{x}, \\mathbf{X}) = \\sqrt{(\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})}$\n",
    "\n",
    "\n",
    "### Components:\n",
    "\n",
    "- $\\mathbf{x}$: The vector whose distance from $\\mathbf{X}$ you're interested in measuring.\n",
    "- $\\mathbf{X}$: The set of vectors representing the distribution.\n",
    "- $\\boldsymbol{\\mu}$: The mean vector of $\\mathbf{X}$.\n",
    "- $\\boldsymbol{\\Sigma}$: The covariance matrix of $\\mathbf{X}$.\n",
    "\n",
    "\n",
    "### Properties:\n",
    "\n",
    "1. **Scale Invariance**: It considers the variance and covariance between variables.\n",
    "2. **Unitless**: Mahalanobis distance is scale-invariant and dimensionless.\n",
    "3. **Generalization**: When the covariance matrix is the identity matrix, the Mahalanobis distance reduces to the Euclidean distance.\n",
    "4. **Sensitivity to Correlations**: It takes into account the correlation between variables, offering a more accurate distance measure when variables are correlated.\n",
    "\n",
    "### Applications:\n",
    "\n",
    "The Mahalanobis distance is widely used in machine learning and statistics, often in clustering and classification tasks. It is also common in outlier detection since it considers the distribution of data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02fe5e",
   "metadata": {},
   "source": [
    "### Example in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "456778de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mahalanobis distance: 0.31622776601683783\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Sample data (2D for illustration)\n",
    "X = np.array([[1, 2],\n",
    "              [2, 3],\n",
    "              [3, 4],\n",
    "              [4, 5],\n",
    "              [5, 6]])\n",
    "\n",
    "# Compute mean and covariance matrix\n",
    "mean = np.mean(X, axis=0)\n",
    "cov_matrix = np.cov(X, rowvar=False)\n",
    "\n",
    "# Compute the pseudo-inverse of the covariance matrix\n",
    "pseudo_inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "\n",
    "# Point we're interested in\n",
    "x = np.array([2.5, 3.5])\n",
    "\n",
    "# Calculate Mahalanobis distance using the pseudo-inverse\n",
    "mahalanobis_dist = distance.mahalanobis(x, mean, pseudo_inv_cov_matrix)\n",
    "\n",
    "print(\"Mahalanobis distance:\", mahalanobis_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd7c97",
   "metadata": {},
   "source": [
    "This code should output a Mahalanobis distance value, which indicates how far $\\mathbf{x}$ is from the distribution $\\mathbf{X}$, considering the variance and covariance of $\\mathbf{X}$.\n",
    "\n",
    "## Fixed-lag\n",
    "Fixed-lag smoothing is a technique used in time-series analysis, sensor fusion, and robotics, among other fields. It aims to estimate the state of a system at a given time based on observations up to a fixed time lag in the past. In the context of probabilistic graphical models, particularly factor graphs, fixed-lag smoothing aims to optimize a belief over a subset of variables within a fixed lag from the most recent observation.\n",
    "\n",
    "### Factor Graphs: A Quick Primer\n",
    "\n",
    "A factor graph is a graphical representation of a global function that is factorized into a product of local functions. These local functions are called \"factors.\" Factor graphs are bipartite graphs that include variable nodes and factor nodes. Edges connect factor nodes to the variables that appear in their corresponding factors.\n",
    "\n",
    "### Fixed-Lag Smoothing in Factor Graphs\n",
    "\n",
    "In a temporal setting, where you might have time-sequenced data, factor graphs can extend over time, often referred to as a \"chain\" of factor graphs, each representing the state of the system and the associated factors at each time step. The aim of fixed-lag smoothing is to improve the estimate of the state at a specific time $t$ by also considering measurements up to a fixed lag $N$ after that time.\n",
    "\n",
    "\n",
    "To implement fixed-lag smoothing:\n",
    "\n",
    "1. **Observation Collection**: You collect observations $(y_1, y_2, \\ldots, y_{t+N})$ where $N$ is the lag parameter.\n",
    "\n",
    "2. **Factor Graph Construction**: You create a factor graph that represents the relationships between the system states $\\(x_1, x_2, \\ldots, x_{t+N}\\)$ and the observations $(y_1, y_2, \\ldots, y_{t+N})$.\n",
    "\n",
    "3. **State Estimation**: You run an inference algorithm on the factor graph to compute the best estimate for the state $x_t$ based on observations up to $y_{t+N}$.\n",
    "\n",
    "4. **Window Slide**: As time moves forward to $t+1$, you slide the fixed window ahead by one time unit. The factor graph is updated to remove the factors and variables related to $t-N-1$ and include those related to $t+N+1$.\n",
    "\n",
    "5. **Repeat**: You go back to Step 3 for each new time step.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "1. **Improved Accuracy**: Fixed-lag smoothing often provides a more accurate estimate than filtering methods that only use past and current observations.\n",
    "  \n",
    "2. **Computational Efficiency**: While full smoothing algorithms that use all past and future data may offer the best accuracy, they are often computationally infeasible in real-time applications. Fixed-lag smoothing provides a good trade-off.\n",
    "\n",
    "3. **Real-Time Applicability**: The method is often applicable in scenarios where you can tolerate a small delay (the fixed lag) for improved accuracy.\n",
    "\n",
    "Fixed-lag smoothing is widely used in various domains such as robotics for SLAM (Simultaneous Localization and Mapping), in finance for time-series prediction, and in sensor networks for state estimation, among others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
