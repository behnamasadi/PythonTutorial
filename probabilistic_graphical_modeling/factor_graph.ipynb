{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24affba9",
   "metadata": {},
   "source": [
    "# Factor Graph\n",
    "A **Factor Graph** is a bipartite graph that represents the factorization of a function. A bipartite graph is a graph in which the vertex set, $V$, can be partitioned into two subsets, $X$ and $Y$, such that each edge of the graph has one vertex in X and one vertex in Y . In a factor graph, there are two types of nodes:\n",
    "\n",
    "1. **Variable Nodes**: Represent variables in your model, $x, y, z $ are the variables.\n",
    "2. **Factor Nodes**: Represent factors or functions that operate on one or more of these variables, $f_1$  is a factor involving variables $x$ and $y$, $f_2$  is a factor involving variables $y$ and $z$.\n",
    " \n",
    "Function <img src=\"https://latex.codecogs.com/svg.latex?f%28x%2C%20y%2C%20z%29\" alt=\"https://latex.codecogs.com/svg.latex?f(x, y, z) \" /> that can be factorized as: <img src=\"https://latex.codecogs.com/svg.latex?f%28x%2C%20y%2C%20z%29%20%3D%20f_1%28x%2C%20y%29%20%5Ctimes%20f_2%28y%2C%20z%29\" alt=\"https://latex.codecogs.com/svg.latex? f(x, y, z) = f_1(x, y) \\times f_2(y, z) \" />\n",
    "\n",
    "The **variables** represent the **unknown random** variables in the estimation problem, whereas the **factors** represent **probabilistic constraints** on those variables, derived from measurements or prior knowledge.\n",
    "\n",
    "The factor graph will have three variable nodes for $x, y, z$, and two factor nodes for $f_1, f_2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ddc5d6a-3404-4b16-a4c6-52e962fa44a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Create the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add variable nodes\n",
    "variables = [\"x\", \"y\", \"z\"]\n",
    "G.add_nodes_from(variables, color=\"lightblue\")\n",
    "\n",
    "# Add factor nodes\n",
    "factors = [\"f1\", \"f2\"]\n",
    "G.add_nodes_from(factors, color=\"lightcoral\")\n",
    "\n",
    "# Add edges between factors and their corresponding variables\n",
    "G.add_edges_from([(\"f1\", \"x\"), (\"f1\", \"y\"), (\"f2\", \"y\"), (\"f2\", \"z\")])\n",
    "\n",
    "# Draw the graph\n",
    "colors = [\"lightblue\" if n in variables else \"lightcoral\" for n in G.nodes()]\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_color=colors, node_size=2000)\n",
    "plt.title(\"Factor Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce83fd3-27cd-493b-ae04-ec57b8794e06",
   "metadata": {},
   "source": [
    "\n",
    "In the visualized graph, you'll see:\n",
    "- Blue nodes represent variables (x, y, z).\n",
    "- Red nodes represent factors (f1, f2).\n",
    "- Edges connect factors to the variables they involve.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Refs: [1](https://www.youtube.com/watch?v=tm4E1o11kGo), [2](https://www.youtube.com/watch?v=JmR2YpkLNt0), [3](https://www.youtube.com/watch?v=Q313pTMAdcM), [4](https://www.youtube.com/watch?v=zOr9HreMthY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7075376-028e-45df-b36c-b5f7886d2ba3",
   "metadata": {},
   "source": [
    "# **Algorithms in Factor Graphs**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6289df-a7c9-419b-9c33-4c3f3e028e76",
   "metadata": {},
   "source": [
    "## **1.Optimization Algorithms**:\n",
    "   Factor graphs are often solved by minimizing a cost function derived from negative log-likelihoods.\n",
    "   - **Gauss-Newton Method**: Iterative nonlinear optimization for least-squares problems.\n",
    "   - **Levenberg-Marquardt Algorithm (LMA)**: Combines Gauss-Newton with gradient descent for robustness.\n",
    "   - **Gradient Descent**: For large-scale problems or when derivatives are difficult to compute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c12858-d605-440c-914b-e1a2411d6edc",
   "metadata": {},
   "source": [
    "In factor graphs, **optimization** aims to find the best estimate of the variables (e.g., robot poses, landmark positions) that maximizes the posterior probability of the model given the measurements or minimizes a cost function derived from the factor graph.\n",
    "\n",
    "---\n",
    "\n",
    "#### **What Are We Optimizing?**\n",
    "We optimize the **posterior probability** of the variables $ X $ given the measurements $ Z $:\n",
    "$\n",
    "p(X | Z) \\propto p(Z | X) p(X)\n",
    "$\n",
    "Where:\n",
    "- $ p(Z | X) $: Likelihood of the measurements given the variables (constraints derived from sensor data, odometry, etc.).\n",
    "- $ p(X) $: Prior probability of the variables (e.g., initial pose or smoothness assumptions).\n",
    "\n",
    "This is equivalent to **minimizing the negative log-posterior**:\n",
    "$\n",
    "\\hat{X} = \\arg\\min_X -\\log p(X | Z)\n",
    "$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcef623-4133-4916-83b4-d8029b384c3c",
   "metadata": {},
   "source": [
    "\n",
    "#### **Factor Graph Representation**\n",
    "The factor graph represents the joint probability distribution as a product of factors:\n",
    "$\n",
    "p(X, Z) \\propto \\prod_i \\phi_i(X)\n",
    "$\n",
    "Each factor $ \\phi_i(X) $ models a constraint:\n",
    "- Measurements (e.g., sensor readings).\n",
    "- Priors (e.g., initial pose).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Optimization Objective**\n",
    "The negative log of the posterior can be expressed as a **nonlinear least-squares problem**:\n",
    "$\n",
    "\\hat{X} = \\arg\\min_X \\sum_i \\| r_i(X) \\|^2_{\\Sigma_i}\n",
    "$\n",
    "Where:\n",
    "- $ r_i(X) $: Residual or error term of the $ i $-th factor.\n",
    "- $ \\Sigma_i $: Covariance (weight) of the $ i $-th factor.\n",
    "- $ \\| r_i(X) \\|^2_{\\Sigma_i} = r_i(X)^T \\Sigma_i^{-1} r_i(X) $: Weighted squared error.\n",
    "\n",
    "#### **Residuals in SLAM**\n",
    "Residuals $ r_i(X) $ quantify the difference between predicted and observed values for each constraint:\n",
    "1. **Odometry Factor** (robot motion model):\n",
    "   $\n",
    "   r_{\\text{odom}}(X_k, X_{k+1}) = \\text{odometry measurement} - \\text{predicted relative pose}\n",
    "   $\n",
    "\n",
    "2. **Landmark Measurement Factor**:\n",
    "   $\n",
    "   r_{\\text{landmark}}(X_k, L_j) = \\text{observed landmark position} - \\text{predicted position from robot pose}\n",
    "   $\n",
    "\n",
    "3. **Prior Factor**:\n",
    "   $\n",
    "   r_{\\text{prior}}(X_0) = \\text{initial pose} - \\text{predicted pose}\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "#### **Complete Cost Function**\n",
    "Combining all residuals, the total cost function is:\n",
    "$ J(X) = \\sum_k \\| r_{\\text{odom}}(X_k, X_{k+1}) \\|^2_{\\Sigma_{\\text{odom}}} + \\sum_j \\| r_{\\text{landmark}}(X_k, L_j) \\|^2_{\\Sigma_{\\text{landmark}}}\n",
    "+ \\| r_{\\text{prior}}(X_0) \\|^2_{\\Sigma_{\\text{prior}}} $\n",
    "The optimization minimizes $ J(X) $, giving the best estimates for $ X $ (poses and landmarks).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Example: 2D SLAM**\n",
    "##### Problem Setup:\n",
    "- **Variables**: Robot poses $ X_k = [x_k, y_k, \\theta_k] $ and landmarks $ L_j = [l_{x,j}, l_{y,j}] $.\n",
    "- **Factors**:\n",
    "  - Odometry measurements: $ Z_{\\text{odom},k} $.\n",
    "  - Landmark observations: $ Z_{\\text{landmark},j} $.\n",
    "  - Prior on the initial pose.\n",
    "\n",
    "##### Residuals:\n",
    "1. **Odometry Factor**:\n",
    "   $\n",
    "   r_{\\text{odom},k} = \\begin{bmatrix} x_{k+1} - (x_k + \\Delta x) \\\\ y_{k+1} - (y_k + \\Delta y) \\\\ \\theta_{k+1} - (\\theta_k + \\Delta \\theta) \\end{bmatrix}\n",
    "   $\n",
    "   Where $ \\Delta x, \\Delta y, \\Delta \\theta $ are derived from $ Z_{\\text{odom},k} $.\n",
    "\n",
    "2. **Landmark Observation**:\n",
    "   $\n",
    "   r_{\\text{landmark},j} = \\begin{bmatrix} l_{x,j} - (x_k + d \\cos(\\theta_k + \\alpha)) \\\\ l_{y,j} - (y_k + d \\sin(\\theta_k + \\alpha)) \\end{bmatrix}\n",
    "   $\n",
    "   Where $ d, \\alpha $ are derived from $ Z_{\\text{landmark},j} $.\n",
    "\n",
    "3. **Prior**:\n",
    "   $\n",
    "   r_{\\text{prior}} = X_0 - \\text{prior value}\n",
    "   $\n",
    "\n",
    "##### Cost Function:\n",
    "$\n",
    "J(X) = \\sum_k \\| r_{\\text{odom},k} \\|^2 + \\sum_j \\| r_{\\text{landmark},j} \\|^2 + \\| r_{\\text{prior}} \\|^2\n",
    "$\n",
    "\n",
    "##### Optimization:\n",
    "- Solve using iterative methods like Gauss-Newton or Levenberg-Marquardt.\n",
    "- Linearize residuals and solve the linearized system to update $ X $ iteratively.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077cd7ae-9f5b-4a82-afd2-48df8e8a2eaf",
   "metadata": {},
   "source": [
    "A **simple numerical example** of optimizing a factor graph with a few states and landmarks using the principles mentioned above.\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem Setup**\n",
    "#### Variables:\n",
    "- Robot poses: $ X_0 = [0, 0, 0] $, $ X_1 = [1, 0, 0] $, $ X_2 = [2, 0, 0] $ (initial guess).\n",
    "- Landmarks: $ L_1 = [1, 1] $, $ L_2 = [3, 1] $ (initial guess).\n",
    "\n",
    "#### Measurements:\n",
    "1. **Odometry Measurements**:\n",
    "   - $ Z_{\\text{odom}, 0\\rightarrow1} = [1, 0] $ (robot moves 1 unit forward).\n",
    "   - $ Z_{\\text{odom}, 1\\rightarrow2} = [1, 0] $.\n",
    "\n",
    "2. **Landmark Observations**:\n",
    "   - At $ X_1 $: $ Z_{\\text{landmark}, L_1} = [1, 1] $.\n",
    "   - At $ X_2 $: $ Z_{\\text{landmark}, L_2} = [1, 1] $.\n",
    "\n",
    "3. **Prior on Initial Pose**:\n",
    "   - $ X_0 = [0, 0, 0] $.\n",
    "\n",
    "#### Covariance Matrices:\n",
    "- For simplicity, all covariances $ \\Sigma = I $ (identity matrix).\n",
    "\n",
    "---\n",
    "\n",
    "### **Formulating Residuals**\n",
    "1. **Odometry Residuals**:\n",
    "   $\n",
    "   r_{\\text{odom}, 0\\rightarrow1} = X_1 - (X_0 + Z_{\\text{odom}, 0\\rightarrow1})\n",
    "   $\n",
    "   $\n",
    "   r_{\\text{odom}, 1\\rightarrow2} = X_2 - (X_1 + Z_{\\text{odom}, 1\\rightarrow2})\n",
    "   $\n",
    "\n",
    "2. **Landmark Residuals**:\n",
    "   $\n",
    "   r_{\\text{landmark}, L_1} = L_1 - (X_1 + Z_{\\text{landmark}, L_1})\n",
    "   $\n",
    "   $\n",
    "   r_{\\text{landmark}, L_2} = L_2 - (X_2 + Z_{\\text{landmark}, L_2})\n",
    "   $\n",
    "\n",
    "3. **Prior Residual**:\n",
    "   $\n",
    "   r_{\\text{prior}} = X_0 - [0, 0, 0]\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### **Initial Residuals**\n",
    "Using the initial guesses:\n",
    "\n",
    "1. **Odometry Residuals**:\n",
    "   $\n",
    "   r_{\\text{odom}, 0\\rightarrow1} = [1, 0] - ([0, 0] + [1, 0]) = [0, 0]\n",
    "   $\n",
    "   $\n",
    "   r_{\\text{odom}, 1\\rightarrow2} = [2, 0] - ([1, 0] + [1, 0]) = [0, 0]\n",
    "   $\n",
    "\n",
    "2. **Landmark Residuals**:\n",
    "   $\n",
    "   r_{\\text{landmark}, L_1} = [1, 1] - ([1, 0] + [1, 1]) = [0, 0]\n",
    "   $\n",
    "   $\n",
    "   r_{\\text{landmark}, L_2} = [3, 1] - ([2, 0] + [1, 1]) = [0, 0]\n",
    "   $\n",
    "\n",
    "3. **Prior Residual**:\n",
    "   $\n",
    "   r_{\\text{prior}} = [0, 0] - [0, 0] = [0, 0]\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### **Cost Function**\n",
    "The total cost function is:\n",
    "$\n",
    "J(X, L) = \\|r_{\\text{odom}, 0\\rightarrow1}\\|^2 + \\|r_{\\text{odom}, 1\\rightarrow2}\\|^2 + \\|r_{\\text{landmark}, L_1}\\|^2 + \\|r_{\\text{landmark}, L_2}\\|^2 + \\|r_{\\text{prior}}\\|^2\n",
    "$\n",
    "\n",
    "At the initial guess:\n",
    "$\n",
    "J(X, L) = 0\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimization**\n",
    "To introduce some error for optimization, let’s modify the initial guesses:\n",
    "- New initial guess: $ X_2 = [2.1, 0] $, $ L_2 = [3.2, 1] $.\n",
    "\n",
    "1. **Updated Residuals**:\n",
    "   - $ r_{\\text{odom}, 1\\rightarrow2} = [2.1, 0] - ([1, 0] + [1, 0]) = [0.1, 0] $.\n",
    "   - $ r_{\\text{landmark}, L_2} = [3.2, 1] - ([2.1, 0] + [1, 1]) = [0.1, 0] $.\n",
    "\n",
    "2. **Updated Cost**:\n",
    "   $\n",
    "   J(X, L) = 0.1^2 + 0.1^2 = 0.02\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimization Steps**\n",
    "Using Gauss-Newton or Levenberg-Marquardt:\n",
    "1. **Linearize Residuals**:\n",
    "   Compute Jacobians for each residual.\n",
    "\n",
    "2. **Solve Linear System**:\n",
    "   Update $ X_2 $ and $ L_2 $ to minimize $ J(X, L) $.\n",
    "\n",
    "3. **Iterate**:\n",
    "   Repeat until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Estimate**\n",
    "After a few iterations, the optimizer adjusts $ X_2 $ and $ L_2 $ to minimize the residuals:\n",
    "$\n",
    "X_2 = [2, 0], \\quad L_2 = [3, 1]\n",
    "$\n",
    "Resulting in $ J(X, L) = 0 $, achieving the best-fit estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed79b11-f84c-4198-963c-497cfcdaa022",
   "metadata": {},
   "source": [
    "\n",
    "## **2.Belief Propagation (BP)**:\n",
    "Belief Propagation (BP) is an algorithm used for inference in graphical models, such as Bayesian networks and factor graphs. It operates by passing \"messages\" between nodes (variables and factors) in the graph. BP can be used for both exact inference in tree-structured graphs and approximate inference in loopy graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3406a6c",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/gaussian_belief_propagation.gif\" width=\"30%\" height=\"30%\" />\n",
    "\n",
    "The best explanation [here](https://gaussianbp.github.io/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d933845f-7835-4a96-9dc8-a249133aba4e",
   "metadata": {},
   "source": [
    "**Belief Propagation Steps:**\n",
    "\n",
    "1. **Initialization**: Each node initializes its beliefs based on its local information. For instance, the symptom nodes (F and C) will initialize their beliefs based on the observed values.\n",
    "\n",
    "2. **Message Passing**:\n",
    "   - The symptom nodes (F and C) send messages to the disease node (D). These messages represent the evidence from each symptom about the disease.\n",
    "   - For our example, the message from F to D might be computed as:\n",
    "     $ m_{F \\to D}(D) = \\sum_{F} P(F|D) \\times \\text{belief}(F) $\n",
    "   - Similarly, a message is computed from C to D.\n",
    "\n",
    "3. **Update Beliefs**: Each node updates its belief based on incoming messages. For the disease node:\n",
    "   $ \\text{belief}(D) \\propto P(D) \\times m_{F \\to D}(D) \\times m_{C \\to D}(D) $\n",
    "\n",
    "4. **Iterate**: If the graph had loops (ours doesn't), we would repeat the message passing until convergence.\n",
    "\n",
    "5. **Extract Marginals**: The final beliefs at each node give the marginal probabilities. In our case, the belief at node D gives $ P(D|F, C) $.\n",
    "\n",
    "\n",
    "\n",
    "Belief Propagation provides a systematic way to combine local information (like symptom observations) with global information (like the structure of the Bayesian network and the conditional probabilities) to compute the desired probabilities. In tree-structured graphs, BP gives exact results. In graphs with loops, BP can be used as an approximation method, often referred to as \"Loopy Belief Propagation\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801896f",
   "metadata": {},
   "source": [
    "**Example: Disease Diagnosis**\n",
    "\n",
    "Imagine a simple medical scenario where a patient might have a disease (D) based on two symptoms: fever (F) and cough (C). We want to determine the probability of the disease given the observed symptoms.\n",
    "\n",
    "The Bayesian network might look like this:\n",
    "\n",
    "```\n",
    "  D\n",
    " / \\\n",
    "F   C\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $ P(D) $ is the prior probability of the disease.\n",
    "- $ P(F|D) $ is the probability of fever given the disease.\n",
    "- $ P(C|D) $ is the probability of cough given the disease.\n",
    "\n",
    "Given some observations, say $ F = \\text{true} $ and $ C = \\text{true} $, we want to compute $ P(D|F, C) $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c5afb",
   "metadata": {},
   "source": [
    "**Example: Job Offer Prediction**\n",
    "\n",
    "Imagine a scenario where a student's likelihood of getting a job offer (J) depends on their internship experience (I), their GPA (G), and their performance in a technical interview (T). Additionally, the student's performance in the technical interview is influenced by their coding skills (C) and problem-solving skills (P).\n",
    "\n",
    "The Bayesian network might look like this:\n",
    "\n",
    "```\n",
    "  I   G\n",
    "   \\ / \\\n",
    "    J   T\n",
    "       / \\\n",
    "      C   P\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $ P(J) $ is the prior probability of getting a job offer.\n",
    "- $ P(I) $ and $ P(G) $ are the probabilities of having an internship and a good GPA, respectively.\n",
    "- $ P(T|C,P) $ is the probability of performing well in the technical interview given coding and problem-solving skills.\n",
    "- $ P(J|I,G,T) $ is the probability of getting a job offer given the internship, GPA, and interview performance.\n",
    "\n",
    "Given some observations, say $ I = \\text{true} $, $ G = \\text{true} $, $ C = \\text{true} $, and $ P = \\text{true} $, we want to compute $ P(J|I, G, C, P) $.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e078e9-e983-4a91-8e5b-0e8ae3587c79",
   "metadata": {},
   "source": [
    "## **3.Incremental Solvers**:\n",
    "   - Used in real-time applications like SLAM for updating solutions as new data arrives.\n",
    "   - Example: **iSAM2 (Incremental Smoothing and Mapping)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424d160-74e3-4731-b2ed-f3123eb77ecd",
   "metadata": {},
   "source": [
    "## **4.Marginalization**:\n",
    "   - Integrating out certain variables to simplify the factor graph while preserving key information.\n",
    "   - Often used to reduce the size of the graph (e.g., in SLAM).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfef413-61d5-4bcc-aa2d-8dbee4de8a44",
   "metadata": {},
   "source": [
    "### **Marginalization in Factor Graphs for SLAM**\n",
    "\n",
    "Marginalization in SLAM is a technique used to simplify a factor graph by reducing the number of variables while retaining key information about the system. This is particularly useful for long-term SLAM systems where the factor graph can grow excessively large, making optimization computationally expensive.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts of Marginalization**\n",
    "1. **Objective**:\n",
    "   - Reduce the size of the factor graph by removing variables (e.g., old robot poses or landmarks) while preserving their influence on the remaining graph.\n",
    "\n",
    "2. **Types of Marginalization**:\n",
    "   - **Landmark Marginalization**: Marginalizing out map landmarks no longer observed to reduce the graph's complexity.\n",
    "   - **Pose Marginalization**: Removing older robot poses from the graph to focus on the most recent state.\n",
    "\n",
    "3. **Mathematical Basis**:\n",
    "   - Marginalization involves integrating out variables from the joint probability distribution:\n",
    "     $\n",
    "     p(X_r, X_m) = \\int p(X_r, X_m, X_o) dX_o\n",
    "     $\n",
    "     Where:\n",
    "     - $ X_r $: Retained variables (e.g., recent poses, active landmarks).\n",
    "     - $ X_m $: Measurements or observations.\n",
    "     - $ X_o $: Variables to be marginalized (e.g., old poses, inactive landmarks).\n",
    "\n",
    "---\n",
    "\n",
    "### **How Marginalization Works in Factor Graphs**\n",
    "1. **Start with the Joint Distribution**:\n",
    "   - The factor graph represents the joint probability of all variables:\n",
    "     $\n",
    "     p(X) \\propto \\prod_i \\phi_i(X)\n",
    "     $\n",
    "     where $ \\phi_i $ are the factors relating variables and measurements.\n",
    "\n",
    "2. **Marginalization**:\n",
    "   - Select variables to marginalize out (e.g., old poses).\n",
    "   - Remove these variables by integrating over their distributions.\n",
    "   - The result is a reduced graph with new factors that summarize the influence of the marginalized variables.\n",
    "\n",
    "3. **New Factors**:\n",
    "   - The marginalization process introduces **dense factors** that couple the retained variables.\n",
    "   - These dense factors encode the summarized constraints of the removed variables.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps in Marginalization for SLAM**\n",
    "1. **Identify Variables to Remove**:\n",
    "   - For instance, older poses in a fixed-lag smoother or inactive landmarks.\n",
    "\n",
    "2. **Linearize the Factor Graph**:\n",
    "   - Linearize the nonlinear factors around the current estimate.\n",
    "   - Express the system in terms of a Gaussian distribution.\n",
    "\n",
    "3. **Apply Schur Complement**:\n",
    "   - The marginalization process involves the Schur complement to compute the effect of marginalized variables on the retained variables.\n",
    "   - In matrix form, consider the linearized system:\n",
    "     $ \\begin{bmatrix}\n",
    "     H_{rr} & H_{ro} \\\\\n",
    "     H_{or} & H_{oo}\n",
    "     \\end{bmatrix}\n",
    "     \\begin{bmatrix}\n",
    "     \\delta X_r \\\\\n",
    "     \\delta X_o\n",
    "     \\end{bmatrix} =  \\begin{bmatrix}      b_r \\\\      b_o      \\end{bmatrix}  $\n",
    "     Where $ H_{rr}, H_{ro}, H_{or}, H_{oo} $ are the block components of the Hessian matrix, and $ \\delta X_r, \\delta X_o $ are the updates to retained and marginalized variables.\n",
    "\n",
    "   - Marginalizing $ \\delta X_o $ yields a reduced system:\n",
    "     $\n",
    "     H'_{rr} = H_{rr} - H_{ro} H_{oo}^{-1} H_{or}\n",
    "     $\n",
    "     $\n",
    "     b'_r = b_r - H_{ro} H_{oo}^{-1} b_o\n",
    "     $\n",
    "     This produces a new, smaller system for $ \\delta X_r $.\n",
    "\n",
    "4. **Update the Factor Graph**:\n",
    "   - Replace the factors involving marginalized variables with a single dense factor on the retained variables.\n",
    "\n",
    "---\n",
    "\n",
    "### **Challenges of Marginalization**\n",
    "1. **Dense Factors**:\n",
    "   - Marginalization introduces dense factors that increase computational cost for subsequent optimization.\n",
    "\n",
    "2. **Numerical Stability**:\n",
    "   - Inverting the sub-matrix $ H_{oo} $ during the Schur complement can lead to numerical issues.\n",
    "\n",
    "3. **Information Loss**:\n",
    "   - Marginalization approximates the influence of removed variables, potentially losing fine-grained details.\n",
    "\n",
    "4. **Nonlinearity**:\n",
    "   - Marginalization is typically performed on a linearized graph, which may lead to inaccuracies if the graph is highly nonlinear.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications in SLAM**\n",
    "1. **Fixed-Lag Smoothing**:\n",
    "   - Keep a sliding window of recent poses and landmarks.\n",
    "   - Marginalize out variables outside the sliding window.\n",
    "\n",
    "2. **Pose Graph Optimization**:\n",
    "   - Remove older poses while maintaining their influence on the trajectory.\n",
    "\n",
    "3. **Efficient Long-Term SLAM**:\n",
    "   - Reduce memory and computational costs by pruning the graph.\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Example**\n",
    "Consider a robot performing SLAM:\n",
    "- **Before Marginalization**: The graph contains all robot poses ($ X_1, X_2, ..., X_n $) and landmarks ($ L_1, L_2, ... $).\n",
    "- **After Marginalization**: Older poses ($ X_1, X_2 $) and inactive landmarks ($ L_1 $) are removed, replaced by a dense factor that summarizes their influence on the remaining graph.\n",
    "\n",
    "This ensures efficient optimization while preserving essential constraints.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bae2fe-2161-42fe-b95c-ed38fed9f38e",
   "metadata": {},
   "source": [
    "## **5.Variable Elimination**:\n",
    "\n",
    "Variable elimination is a technique commonly used in probabilistic graphical models, particularly in Bayesian networks, to perform inference. It is used to compute the marginal distribution of a particular variable or set of variables, while \"eliminating\" other variables from the calculation. This is particularly useful when the underlying graphical model consists of many variables, and you're interested in the probability distribution over only a subset of those variables.\n",
    "\n",
    "Here's a basic outline of how variable elimination works:\n",
    "\n",
    "1. **Specify the Query**: First, you identify the variable or variables for which you want to compute the marginal distribution.\n",
    "\n",
    "2. **Identify the Evidence**: Sometimes you have observed values for some variables, known as \"evidence.\" These are fixed and not eliminated during the calculation.\n",
    "\n",
    "3. **Factorization**: Bayesian networks are made up of conditional probability tables. These can be thought of as \"factors\" in a big multiplication that computes the joint distribution over all variables.\n",
    "\n",
    "4. **Elimination**: Starting with these factors, you eliminate variables that are not in your query or evidence set one by one. To do this for a variable `X`, you:\n",
    "    - Identify all factors that involve `X`.\n",
    "    - Multiply these factors together to produce a new factor that still involves `X`.\n",
    "    - Sum out `X` from this new factor.\n",
    "    - Replace the original factors involving `X` in your list with this new factor that no longer involves `X`.\n",
    "\n",
    "5. **Final Multiplication**: After eliminating all the unnecessary variables, you're left with factors that involve only the query and evidence variables. Multiply these remaining factors together to get the unnormalized marginal distribution for the query variables.\n",
    "\n",
    "6. **Normalization**: Divide the unnormalized marginal by the sum over all its values to get a proper probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13463d6c",
   "metadata": {},
   "source": [
    "**Variable Elimination Example** \n",
    "\n",
    "\n",
    "three variables  $A,B, C$, where you want to find $P(A|C=c)$ given $C=c$ as evidence.\n",
    "\n",
    "\n",
    "\n",
    "- Factors from Bayesian network: $f_1(A, B), f_2(B, C)$\n",
    "- Query: $A$\n",
    "- Evidence: $C=c$\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. **Specify the Query**: $P(A|C=c$\n",
    "  \n",
    "2. **Identify the Evidence**: $C=c$\n",
    "  \n",
    "3. **Factorization**: $f_1(A, B) \\text{ and } f_2(B, C=c) \\text{ updated } f_2$ with evidence\n",
    "\n",
    "  \n",
    "4. **Elimination**:\n",
    "    - Eliminate $B$ \n",
    "    - Identify all factors with $B$: $f_1(A, B), f_2(B, C=c)$\n",
    "    - Multiply to get new factor:$f_3(A, B) = f_1(A, B) * f_2(B, C=c)$\n",
    "    - Sum out $B$ to get $f_4(A) = \\sum_B f_3(A, B)$\n",
    "    - Now, you have $f_4(A)$ instead of $f_1 \\text{ and }  f_2$.\n",
    "  \n",
    "5. **Final Multiplication**: $f_4(A)$ (Already have it)\n",
    "  \n",
    "6. **Normalization**: $P(A|C=c) = \\frac{f_4(A)}{\\sum_A f_4(A)}$\n",
    "\n",
    "This is a simplified example, but the core steps remain the same even as you scale to larger, more complicated networks. Variable elimination is a foundational technique in probabilistic graphical models, and is used in various applications like natural language processing, robotics, medical diagnosis, and many more.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806c50d",
   "metadata": {},
   "source": [
    "Given Conditional Probability Tables (CPTs)\n",
    "\n",
    "$\\left\\{\\begin{matrix} \\\\  P(A=1) = 0.8, P(A=0)  0.2  \\\\  P(B=1 | A=1) = 0.7, P(B=0 | A=1) = 0.3 \\\\  P(B=1 | A=0) = 0.1, P(B=0 | A=0) = 0.9  \\\\  P(C=1 | B=1) = 0.9, P(C=0 | B=1) = 0.1 \\\\  P(C=1 | B=0) = 0.2, P(C=0 | B=0) = 0.8  \\\\  \\end{matrix}\\right.$\n",
    "\n",
    "We are interested in calculating  $P(A | C=1)$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Steps of Variable Elimination\n",
    "\n",
    "1. **Step 4: Elimination of B**  \n",
    "The intermediate factor $f_4$ is computed by summing over the variable $B$. This is represented mathematically as:\n",
    "\n",
    "$f_4(A, C=1) = \\sum_{B} [ P(A) \\times P(B|A) \\times P(C=1|B)$\n",
    "\n",
    "\n",
    "The reason the marginalization equation becomes specific in the context of this example lies in the structure of the underlying probabilistic model and the conditional independence relationships it encodes.\n",
    "\n",
    "The original equation you provided for marginalization assumes that $C$ is conditionally dependent on both $A$ and $B$ : $P(A, C) = \\sum_B [ P(C|A, B) \\times P(B|A) \\times P(A)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the example, however, $C$ is conditionally independent of $A$ given $B$. In other words, once you know $B$, knowing $A$  doesn't give you any additional information about $C$ . Mathematically, this means: $P(C|A, B) = P(C|B)$\n",
    "\n",
    "\n",
    "Because of this conditional independence, the marginalization formula simplifies to: $P(A, C) = \\sum_B [ P(C|B) \\times P(B|A) \\times P(A) ]$\n",
    "\n",
    "\n",
    "\n",
    "This equation sums over all the possible values of $B$, weighting them by their conditional probabilities given $A$ and the likelihood of $C$ given $B$, to compute the joint distribution $P(A, C)$.\n",
    "\n",
    "\n",
    "So, the specific form of the marginalization equation is due to the conditional independence relationships specified in the original problem. In this case, the probability of $C$ only depends directly on $B$, and not on $A$, when $B$ is known. This allows us to use $P(C|B)$ in place of $P(C|A, B)$ in the formula.\n",
    "\n",
    "**Few Reminder** \n",
    "- Conditionally Independent:\n",
    "\n",
    "If $A$ and $B$ are conditionally independent of $C$, written symbolically as: \n",
    "\n",
    "\n",
    "${\\displaystyle (A\\perp \\!\\!\\!\\perp B|C)}$\n",
    "<br/>\n",
    "\n",
    "$P(A,B|C)=P(A|C)P(B|C)$\n",
    "\n",
    "<br/>\n",
    "\n",
    "$P(A|B,C)=P(A|C)$\n",
    "\n",
    "$P(A , B , C) = P(A|B,C).P(B|C).P (C)$\n",
    "\n",
    "For the general case, we have n variables\n",
    "\n",
    "\n",
    "$P(X_n , X_{n-1}  , ... , X_2 , X_1)=\\prod_{n}^{i=1}P(X_i | X_{i-1}, ... , X_2 , X_1) \\\\ = P(X_n|X_{n-1}, ... , X_2 , X_1 )\\cdot ... \\cdot P(X_2 |X_1).P(X_1)$\n",
    "\n",
    "\n",
    "Getting back to our example, when $A=0$ and $C=1$:\n",
    "\n",
    "\n",
    "$f_4(A=0, C=1) = 0.2 \\times 0.1 \\times 0.9 + 0.2 \\times 0.9 \\times 0.2 = 0.018 + 0.036 = 0.054$\n",
    "\n",
    "Similarly, when $A=1$ and $C=1$:\n",
    "\n",
    "\n",
    "$f_4(A=1, C=1) = 0.8 \\times 0.7 \\times 0.9 + 0.8 \\times 0.3 \\times 0.2 = 0.504 + 0.048 = 0.552$\n",
    "\n",
    "\n",
    "\n",
    "2. **Step 6: Normalization**  \n",
    "To find $P(A | C=1)$, we need to normalize $f_4$ so that the probabilities sum to 1.\n",
    "\n",
    "\n",
    "$P(A=0 | C=1) = \\frac{f_4(A=0, C=1)}{f_4(A=0, C=1) + f_4(A=1, C=1)} = \\frac{0.054}{0.054 + 0.552} \\approx 0.088$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "$P(A=1 | C=1) = \\frac{f_4(A=1, C=1)}{f_4(A=0, C=1) + f_4(A=1, C=1)} = \\frac{0.552}{0.054 + 0.552} \\approx 0.912$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f9760",
   "metadata": {},
   "source": [
    "# Mahalanobis distance\n",
    "\n",
    "The Mahalanobis distance is a measure of distance between a point $\\mathbf{P}$ and a distribution $D$, scaled by the statistical variability in each dimension of the space. Unlike the Euclidean distance, which is scale-dependent, Mahalanobis distance accounts for the correlation between variables and scales the distance metric according to the variance along each dimension.\n",
    "\n",
    "\n",
    "\n",
    "The Mahalanobis distance $D_{\\text{M}}$ between a vector $\\mathbf{x}$ and a set of vectors $\\mathbf{X}$ with mean $\\boldsymbol{\\mu}$ and covariance matrix  $\\boldsymbol{\\Sigma}$ is defined as:\n",
    "\n",
    "\n",
    "$D_{\\text{M}}(\\mathbf{x}, \\mathbf{X}) = \\sqrt{(\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})}$\n",
    "\n",
    "\n",
    "### Components:\n",
    "\n",
    "- $\\mathbf{x}$: The vector whose distance from $\\mathbf{X}$ you're interested in measuring.\n",
    "- $\\mathbf{X}$: The set of vectors representing the distribution.\n",
    "- $\\boldsymbol{\\mu}$: The mean vector of $\\mathbf{X}$.\n",
    "- $\\boldsymbol{\\Sigma}$: The covariance matrix of $\\mathbf{X}$.\n",
    "\n",
    "\n",
    "### Properties:\n",
    "\n",
    "1. **Scale Invariance**: It considers the variance and covariance between variables.\n",
    "2. **Unitless**: Mahalanobis distance is scale-invariant and dimensionless.\n",
    "3. **Generalization**: When the covariance matrix is the identity matrix, the Mahalanobis distance reduces to the Euclidean distance.\n",
    "4. **Sensitivity to Correlations**: It takes into account the correlation between variables, offering a more accurate distance measure when variables are correlated.\n",
    "\n",
    "### Applications:\n",
    "\n",
    "The Mahalanobis distance is widely used in machine learning and statistics, often in clustering and classification tasks. It is also common in outlier detection since it considers the distribution of data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02fe5e",
   "metadata": {},
   "source": [
    "### Example in Python:\n",
    "This code should output a Mahalanobis distance value, which indicates how far $\\mathbf{x}$ is from the distribution $\\mathbf{X}$, considering the variance and covariance of $\\mathbf{X}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456778de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mahalanobis distance: 0.31622776601683783\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Sample data (2D for illustration)\n",
    "X = np.array([[1, 2],\n",
    "              [2, 3],\n",
    "              [3, 4],\n",
    "              [4, 5],\n",
    "              [5, 6]])\n",
    "\n",
    "# Compute mean and covariance matrix\n",
    "mean = np.mean(X, axis=0)\n",
    "cov_matrix = np.cov(X, rowvar=False)\n",
    "\n",
    "# Compute the pseudo-inverse of the covariance matrix\n",
    "pseudo_inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "\n",
    "# Point we're interested in\n",
    "x = np.array([2.5, 3.5])\n",
    "\n",
    "# Calculate Mahalanobis distance using the pseudo-inverse\n",
    "mahalanobis_dist = distance.mahalanobis(x, mean, pseudo_inv_cov_matrix)\n",
    "\n",
    "print(\"Mahalanobis distance:\", mahalanobis_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd7c97",
   "metadata": {},
   "source": [
    "\n",
    "## Fixed-lag\n",
    "Fixed-lag smoothing is a technique used in time-series analysis, sensor fusion, and robotics, among other fields. It aims to estimate the state of a system at a given time based on observations up to a fixed time lag in the past. In the context of probabilistic graphical models, particularly factor graphs, fixed-lag smoothing aims to optimize a belief over a subset of variables within a fixed lag from the most recent observation.\n",
    "\n",
    "### Fixed-Lag Smoothing in Factor Graphs\n",
    "\n",
    "In a temporal setting, where you might have time-sequenced data, factor graphs can extend over time, often referred to as a \"chain\" of factor graphs, each representing the state of the system and the associated factors at each time step. The aim of fixed-lag smoothing is to improve the estimate of the state at a specific time $t$ by also considering measurements up to a fixed lag $N$ after that time.\n",
    "\n",
    "\n",
    "To implement fixed-lag smoothing:\n",
    "\n",
    "1. **Observation Collection**: You collect observations $(y_1, y_2, \\ldots, y_{t+N})$ where $N$ is the lag parameter.\n",
    "\n",
    "2. **Factor Graph Construction**: You create a factor graph that represents the relationships between the system states $x_1, x_2, \\ldots, x_{t+N}$ and the observations $(y_1, y_2, \\ldots, y_{t+N})$.\n",
    "\n",
    "3. **State Estimation**: You run an inference algorithm on the factor graph to compute the best estimate for the state $x_t$ based on observations up to $y_{t+N}$.\n",
    "\n",
    "4. **Window Slide**: As time moves forward to $t+1$, you slide the fixed window ahead by one time unit. The factor graph is updated to remove the factors and variables related to $t-N-1$ and include those related to $t+N+1$.\n",
    "\n",
    "5. **Repeat**: You go back to Step 3 for each new time step.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "1. **Improved Accuracy**: Fixed-lag smoothing often provides a more accurate estimate than filtering methods that only use past and current observations.\n",
    "  \n",
    "2. **Computational Efficiency**: While full smoothing algorithms that use all past and future data may offer the best accuracy, they are often computationally infeasible in real-time applications. Fixed-lag smoothing provides a good trade-off.\n",
    "\n",
    "3. **Real-Time Applicability**: The method is often applicable in scenarios where you can tolerate a small delay (the fixed lag) for improved accuracy.\n",
    "\n",
    "Fixed-lag smoothing is widely used in various domains such as robotics for SLAM (Simultaneous Localization and Mapping), in finance for time-series prediction, and in sensor networks for state estimation, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5104a368-7eaa-4f28-8025-053f81d97d18",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Factor Graph Equation\n",
    "\n",
    "A **factor graph** is a bipartite graph that represents the factorization of a probability distribution. It is widely used in robotics for solving the **SLAM (Simultaneous Localization and Mapping)** problem, where we estimate both the robot's trajectory and the positions of landmarks in the environment.\n",
    "\n",
    "In general, the problem can be formulated as minimizing a cost function that combines multiple constraints:\n",
    "\n",
    "$\n",
    "\\mathbf{x}^* = \\arg \\min_{\\mathbf{x}} \\sum_{i} \\|\\mathbf{h}_i(\\mathbf{x}) - \\mathbf{z}_i\\|^2_{\\mathbf{\\Sigma}_i^{-1}}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{x}$: The set of all variables to estimate (e.g., robot poses and landmark positions).\n",
    "- $\\mathbf{h}_i(\\mathbf{x})$: The measurement model for factor $i$, which predicts the observation.\n",
    "- $\\mathbf{z}_i$: The actual measurement corresponding to factor $i$.\n",
    "- $\\mathbf{\\Sigma}_i^{-1}$: The information matrix (inverse of the covariance) for the measurement noise.\n",
    "\n",
    "The factor graph minimizes the sum of squared errors weighted by their uncertainty, resulting in the maximum a posteriori (MAP) estimate.\n",
    "\n",
    "---\n",
    "\n",
    "### Numerical Example: A Robot with 3 Poses and 2 Landmarks\n",
    "\n",
    "#### Scenario:\n",
    "1. The robot has 3 poses: $ \\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3 $.\n",
    "2. The robot observes 2 landmarks: $ \\mathbf{l}_1, \\mathbf{l}_2 $.\n",
    "3. The robot has odometry constraints between consecutive poses and landmark observations from each pose.\n",
    "\n",
    "#### Factors:\n",
    "1. **Odometry factors**: $ \\mathbf{z}_{12}, \\mathbf{z}_{23} $, connecting $ \\mathbf{x}_1 \\to \\mathbf{x}_2 $ and $ \\mathbf{x}_2 \\to \\mathbf{x}_3 $.\n",
    "2. **Landmark observation factors**: $ \\mathbf{z}_{1l1}, \\mathbf{z}_{2l1}, \\mathbf{z}_{3l2} $, connecting poses to landmarks.\n",
    "\n",
    "#### Cost Function:\n",
    "$\n",
    "\\mathbf{x}^* = \\arg \\min_{\\mathbf{x}} \\left( \\|\\mathbf{h}_{12}(\\mathbf{x}_1, \\mathbf{x}_2) - \\mathbf{z}_{12}\\|^2_{\\Sigma_{12}^{-1}} + \\|\\mathbf{h}_{23}(\\mathbf{x}_2, \\mathbf{x}_3) - \\mathbf{z}_{23}\\|^2_{\\Sigma_{23}^{-1}} + \\|\\mathbf{h}_{1l1}(\\mathbf{x}_1, \\mathbf{l}_1) - \\mathbf{z}_{1l1}\\|^2_{\\Sigma_{1l1}^{-1}} + \\|\\mathbf{h}_{2l1}(\\mathbf{x}_2, \\mathbf{l}_1) - \\mathbf{z}_{2l1}\\|^2_{\\Sigma_{2l1}^{-1}} + \\|\\mathbf{h}_{3l2}(\\mathbf{x}_3, \\mathbf{l}_2) - \\mathbf{z}_{3l2}\\|^2_{\\Sigma_{3l2}^{-1}} \\right)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Numerical Example\n",
    "\n",
    "#### Measurements:\n",
    "- Initial pose: $ \\mathbf{x}_1 = (0, 0) $.\n",
    "- Odometry measurements:\n",
    "  - $ \\mathbf{z}_{12} = (2, 0) $ (robot moves 2 units along x-axis from $ \\mathbf{x}_1 $).\n",
    "  - $ \\mathbf{z}_{23} = (2, 0) $ (robot moves 2 more units along x-axis from $ \\mathbf{x}_2 $).\n",
    "- Landmark observations:\n",
    "  - From $ \\mathbf{x}_1 $, landmark $ \\mathbf{l}_1 $ is observed at $ (1, 1) $.\n",
    "  - From $ \\mathbf{x}_2 $, landmark $ \\mathbf{l}_1 $ is observed at $ (1, 1) $.\n",
    "  - From $ \\mathbf{x}_3 $, landmark $ \\mathbf{l}_2 $ is observed at $ (5, 1) $.\n",
    "\n",
    "#### Initial Guess:\n",
    "- Poses: $ \\mathbf{x}_2 = (2, 0), \\mathbf{x}_3 = (4, 0) $.\n",
    "- Landmarks: $ \\mathbf{l}_1 = (1, 1), \\mathbf{l}_2 = (5, 1) $.\n",
    "\n",
    "---\n",
    "\n",
    "#### Solving:\n",
    "1. Use the initial guess and compute the residuals for each factor.\n",
    "2. Apply an optimization method (e.g., Gauss-Newton or Levenberg-Marquardt) to iteratively update the variables ($ \\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{l}_1, \\mathbf{l}_2 $) to minimize the cost function.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d597a3fb-50e5-4418-a000-83a8acbbdaff",
   "metadata": {},
   "source": [
    "In the factor graph representation, we factorize the joint probability of all variables ($\\mathbf{x}$) into a product of factors, each of which encodes constraints or measurements. The goal is to find the maximum a posteriori (MAP) estimate of $\\mathbf{x}$.\n",
    "\n",
    "### Variables in the Example\n",
    "- **Poses**: $ \\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3 $\n",
    "- **Landmarks**: $ \\mathbf{l}_1, \\mathbf{l}_2 $\n",
    "- **Measurements**:\n",
    "  - Odometry: $ \\mathbf{z}_{12}, \\mathbf{z}_{23} $\n",
    "  - Observations: $ \\mathbf{z}_{1l1}, \\mathbf{z}_{2l1}, \\mathbf{z}_{3l2} $\n",
    "\n",
    "---\n",
    "\n",
    "### Factorization of the Joint Probability\n",
    "The joint probability of all variables ($\\mathbf{x}$) given the measurements ($\\mathbf{z}$) is:\n",
    "\n",
    "$\n",
    "P(\\mathbf{x} \\mid \\mathbf{z}) \\propto \\prod_{i=1}^{N} \\phi_i(\\mathbf{x})\n",
    "$\n",
    "\n",
    "where $\\phi_i(\\mathbf{x})$ represents each factor. For this problem:\n",
    "\n",
    "$\n",
    "P(\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{l}_1, \\mathbf{l}_2 \\mid \\mathbf{z}_{12}, \\mathbf{z}_{23}, \\mathbf{z}_{1l1}, \\mathbf{z}_{2l1}, \\mathbf{z}_{3l2}) \\propto \\psi_{\\text{prior}}(\\mathbf{x}_1) \\cdot \\psi_{\\text{odom12}}(\\mathbf{x}_1, \\mathbf{x}_2) \\cdot \\psi_{\\text{odom23}}(\\mathbf{x}_2, \\mathbf{x}_3) \\cdot \\psi_{\\text{obs1l1}}(\\mathbf{x}_1, \\mathbf{l}_1) \\cdot \\psi_{\\text{obs2l1}}(\\mathbf{x}_2, \\mathbf{l}_1) \\cdot \\psi_{\\text{obs3l2}}(\\mathbf{x}_3, \\mathbf{l}_2)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Factors:\n",
    "1. **Prior Factor** ($ \\psi_{\\text{prior}} $):\n",
    "   - Encodes the prior information about the robot's initial pose $ \\mathbf{x}_1 $:\n",
    "   $\n",
    "   \\psi_{\\text{prior}}(\\mathbf{x}_1) = \\exp\\left(-\\frac{1}{2} \\|\\mathbf{x}_1 - \\mathbf{z}_{\\text{prior}}\\|^2_{\\Sigma_{\\text{prior}}^{-1}}\\right)\n",
    "   $\n",
    "   Here, $ \\mathbf{z}_{\\text{prior}} = (0, 0) $.\n",
    "\n",
    "2. **Odometry Factors** ($ \\psi_{\\text{odom12}}, \\psi_{\\text{odom23}} $):\n",
    "   - Relate consecutive poses $ \\mathbf{x}_1 \\to \\mathbf{x}_2 $ and $ \\mathbf{x}_2 \\to \\mathbf{x}_3 $:\n",
    "   $\n",
    "   \\psi_{\\text{odom12}}(\\mathbf{x}_1, \\mathbf{x}_2) = \\exp\\left(-\\frac{1}{2} \\|\\mathbf{x}_2 - \\mathbf{x}_1 - \\mathbf{z}_{12}\\|^2_{\\Sigma_{12}^{-1}}\\right)\n",
    "   $\n",
    "   $\n",
    "   \\psi_{\\text{odom23}}(\\mathbf{x}_2, \\mathbf{x}_3) = \\exp\\left(-\\frac{1}{2} \\|\\mathbf{x}_3 - \\mathbf{x}_2 - \\mathbf{z}_{23}\\|^2_{\\Sigma_{23}^{-1}}\\right)\n",
    "   $\n",
    "\n",
    "3. **Observation Factors** ($ \\psi_{\\text{obs1l1}}, \\psi_{\\text{obs2l1}}, \\psi_{\\text{obs3l2}} $):\n",
    "   - Relate poses to observed landmarks:\n",
    "   $\n",
    "   \\psi_{\\text{obs1l1}}(\\mathbf{x}_1, \\mathbf{l}_1) = \\exp\\left(-\\frac{1}{2} \\|\\mathbf{l}_1 - (\\mathbf{x}_1 + \\mathbf{z}_{1l1})\\|^2_{\\Sigma_{1l1}^{-1}}\\right)\n",
    "   $\n",
    "   $\n",
    "   \\psi_{\\text{obs2l1}}(\\mathbf{x}_2, \\mathbf{l}_1) = \\exp\\left(-\\frac{1}{2} \\|\\mathbf{l}_1 - (\\mathbf{x}_2 + \\mathbf{z}_{2l1})\\|^2_{\\Sigma_{2l1}^{-1}}\\right)\n",
    "   $\n",
    "   $\n",
    "   \\psi_{\\text{obs3l2}}(\\mathbf{x}_3, \\mathbf{l}_2) = \\exp\\left(-\\frac{1}{2} \\|\\mathbf{l}_2 - (\\mathbf{x}_3 + \\mathbf{z}_{3l2})\\|^2_{\\Sigma_{3l2}^{-1}}\\right)\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### Final Factor Graph Representation\n",
    "The joint probability is now represented as:\n",
    "\n",
    "$\n",
    "P(\\mathbf{x}) \\propto \\psi_{\\text{prior}}(\\mathbf{x}_1) \\cdot \\psi_{\\text{odom12}}(\\mathbf{x}_1, \\mathbf{x}_2) \\cdot \\psi_{\\text{odom23}}(\\mathbf{x}_2, \\mathbf{x}_3) \\cdot \\psi_{\\text{obs1l1}}(\\mathbf{x}_1, \\mathbf{l}_1) \\cdot \\psi_{\\text{obs2l1}}(\\mathbf{x}_2, \\mathbf{l}_1) \\cdot \\psi_{\\text{obs3l2}}(\\mathbf{x}_3, \\mathbf{l}_2)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "1. **Optimization**: Convert this factorization into a sum of squared residuals and use iterative solvers like Gauss-Newton or Levenberg-Marquardt.\n",
    "2. **Visualization**: Draw the factor graph with nodes for $ \\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{l}_1, \\mathbf{l}_2 $ and factors connecting them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd90ca60-91ab-4f5f-a13b-a9ffe6a8203d",
   "metadata": {},
   "source": [
    "Let’s fully expand the equation for $\\psi_{\\text{obs1l1}}$ and explain the components, including $\\Sigma_{3l2}^{-1}$.\n",
    "\n",
    "---\n",
    "\n",
    "### Fully Expanded Equation for $\\psi_{\\text{obs1l1}}(\\mathbf{x}_1, \\mathbf{l}_1)$\n",
    "\n",
    "The factor $\\psi_{\\text{obs1l1}}(\\mathbf{x}_1, \\mathbf{l}_1)$ models the error between the observed position of the landmark ($\\mathbf{z}_{1l1}$) relative to the robot pose ($\\mathbf{x}_1$) and the estimated position of the landmark ($\\mathbf{l}_1$).\n",
    "\n",
    "$\n",
    "\\psi_{\\text{obs1l1}}(\\mathbf{x}_1, \\mathbf{l}_1) = \\exp\\left(-\\frac{1}{2} \\mathbf{e}_{1l1}^\\top \\Sigma_{1l1}^{-1} \\mathbf{e}_{1l1}\\right)\n",
    "$\n",
    "\n",
    "#### Components:\n",
    "1. **Error Term** ($\\mathbf{e}_{1l1}$):\n",
    "   $\n",
    "   \\mathbf{e}_{1l1} = \\mathbf{l}_1 - (\\mathbf{x}_1 + \\mathbf{z}_{1l1})\n",
    "   $\n",
    "   - $\\mathbf{l}_1$: Estimated position of the landmark in the global frame.\n",
    "   - $\\mathbf{x}_1$: Robot's estimated pose in the global frame.\n",
    "   - $\\mathbf{z}_{1l1}$: Measured relative position of the landmark from the robot's pose.\n",
    "\n",
    "2. **Information Matrix** ($\\Sigma_{1l1}^{-1}$):\n",
    "   - The **information matrix** is the inverse of the measurement noise covariance matrix ($\\Sigma_{1l1}$):\n",
    "     $\n",
    "     \\Sigma_{1l1}^{-1} = \\text{Cov}(\\mathbf{z}_{1l1})^{-1}\n",
    "     $\n",
    "   - $\\Sigma_{1l1}$ is a $2 \\times 2$ (for 2D) or $3 \\times 3$ (for 3D) matrix that models the uncertainty in the observation $\\mathbf{z}_{1l1}$. For example:\n",
    "     $\n",
    "     \\Sigma_{1l1} =\n",
    "     \\begin{bmatrix}\n",
    "     \\sigma_x^2 & 0 \\\\\n",
    "     0 & \\sigma_y^2\n",
    "     \\end{bmatrix}\n",
    "     $\n",
    "     where $\\sigma_x^2$ and $\\sigma_y^2$ represent the variances in the x and y directions, respectively.\n",
    "\n",
    "   - If the measurement noise is isotropic (equal in all directions), the matrix simplifies to $\\Sigma_{1l1} = \\sigma^2 I$, and $\\Sigma_{1l1}^{-1} = \\frac{1}{\\sigma^2} I$.\n",
    "\n",
    "3. **Weighted Error Norm**:\n",
    "   The term $-\\frac{1}{2} \\mathbf{e}_{1l1}^\\top \\Sigma_{1l1}^{-1} \\mathbf{e}_{1l1}$ computes the Mahalanobis distance, which accounts for the uncertainty in the measurements.\n",
    "\n",
    "---\n",
    "\n",
    "### Fully Expanded $\\psi_{\\text{obs1l1}}(\\mathbf{x}_1, \\mathbf{l}_1)$\n",
    "\n",
    "Substituting everything, the equation becomes:\n",
    "\n",
    "$\n",
    "\\psi_{\\text{obs1l1}}(\\mathbf{x}_1, \\mathbf{l}_1) = \\exp\\left(-\\frac{1}{2} \\left(\\mathbf{l}_1 - (\\mathbf{x}_1 + \\mathbf{z}_{1l1})\\right)^\\top \\Sigma_{1l1}^{-1} \\left(\\mathbf{l}_1 - (\\mathbf{x}_1 + \\mathbf{z}_{1l1})\\right)\\right)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding $\\Sigma_{3l2}^{-1}$ in $\\psi_{\\text{obs3l2}}(\\mathbf{x}_3, \\mathbf{l}_2)$\n",
    "\n",
    "$\\Sigma_{3l2}^{-1}$ is the **information matrix** for the observation of the second landmark ($\\mathbf{l}_2$) from the third robot pose ($\\mathbf{x}_3$).\n",
    "\n",
    "1. **Measurement Noise Covariance** ($\\Sigma_{3l2}$):\n",
    "   - This models the uncertainty in the observation $\\mathbf{z}_{3l2}$ (e.g., sensor noise when detecting $\\mathbf{l}_2$).\n",
    "   - Example covariance matrix in 2D:\n",
    "     $\n",
    "     \\Sigma_{3l2} =\n",
    "     \\begin{bmatrix}\n",
    "     \\sigma_x^2 & \\rho \\\\\n",
    "     \\rho & \\sigma_y^2\n",
    "     \\end{bmatrix}\n",
    "     $\n",
    "     where $\\rho$ is the covariance between x and y.\n",
    "\n",
    "2. **Information Matrix**:\n",
    "   - The inverse of $\\Sigma_{3l2}$:\n",
    "     $\n",
    "     \\Sigma_{3l2}^{-1} = \\text{Cov}(\\mathbf{z}_{3l2})^{-1}\n",
    "     $\n",
    "\n",
    "3. **Impact**:\n",
    "   - If the sensor is very noisy ($ \\Sigma_{3l2} $ has high variance), the weight ($ \\Sigma_{3l2}^{-1} $) will be low, reducing the factor's influence on the optimization.\n",
    "   - Conversely, precise measurements will have a higher influence.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "The information matrix ($\\Sigma_{3l2}^{-1}$) is critical for weighting the influence of each factor in the optimization process. Expanding $\\psi_{\\text{obs1l1}}$ and others involves substituting the error term, computing the Mahalanobis distance, and incorporating the measurement noise.\n",
    "\n",
    "Would you like a numerical example using actual values for $\\Sigma$, $\\mathbf{x}$, and $\\mathbf{l}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c93c38-75a1-4d25-8b2f-07c9db1a9472",
   "metadata": {},
   "source": [
    "Let's fully expand the equation for $\\psi_{\\text{odom12}}(\\mathbf{x}_1, \\mathbf{x}_2)$ and explain all its components.\n",
    "\n",
    "---\n",
    "\n",
    "### Fully Expanded Equation for $\\psi_{\\text{odom12}}(\\mathbf{x}_1, \\mathbf{x}_2)$\n",
    "\n",
    "The factor $\\psi_{\\text{odom12}}(\\mathbf{x}_1, \\mathbf{x}_2)$ encodes the error between the measured odometry ($\\mathbf{z}_{12}$) and the difference in the estimated poses of the robot ($\\mathbf{x}_1$ and $\\mathbf{x}_2$).\n",
    "\n",
    "$\n",
    "\\psi_{\\text{odom12}}(\\mathbf{x}_1, \\mathbf{x}_2) = \\exp\\left(-\\frac{1}{2} \\mathbf{e}_{12}^\\top \\Sigma_{12}^{-1} \\mathbf{e}_{12}\\right)\n",
    "$\n",
    "\n",
    "#### Components:\n",
    "1. **Error Term** ($\\mathbf{e}_{12}$):\n",
    "   $\n",
    "   \\mathbf{e}_{12} = (\\mathbf{x}_2 - \\mathbf{x}_1) - \\mathbf{z}_{12}\n",
    "   $\n",
    "   - $\\mathbf{x}_1, \\mathbf{x}_2$: Estimated poses of the robot at steps 1 and 2.\n",
    "   - $\\mathbf{z}_{12}$: Measured relative displacement of the robot from $\\mathbf{x}_1$ to $\\mathbf{x}_2$.\n",
    "\n",
    "2. **Information Matrix** ($\\Sigma_{12}^{-1}$):\n",
    "   - This is the inverse of the covariance matrix $\\Sigma_{12}$, which models the uncertainty in the odometry measurement $\\mathbf{z}_{12}$.\n",
    "   - For example, in a 2D odometry measurement with $x$- and $y$-displacement and heading angle $\\theta$, $\\Sigma_{12}$ might look like:\n",
    "     $\n",
    "     \\Sigma_{12} =\n",
    "     \\begin{bmatrix}\n",
    "     \\sigma_x^2 & 0 & 0 \\\\\n",
    "     0 & \\sigma_y^2 & 0 \\\\\n",
    "     0 & 0 & \\sigma_\\theta^2\n",
    "     \\end{bmatrix}\n",
    "     $\n",
    "     where $\\sigma_x^2, \\sigma_y^2, \\sigma_\\theta^2$ are variances for displacement in $x$, $y$, and heading, respectively.\n",
    "\n",
    "   - The information matrix is:\n",
    "     $\n",
    "     \\Sigma_{12}^{-1} =\n",
    "     \\begin{bmatrix}\n",
    "     \\frac{1}{\\sigma_x^2} & 0 & 0 \\\\\n",
    "     0 & \\frac{1}{\\sigma_y^2} & 0 \\\\\n",
    "     0 & 0 & \\frac{1}{\\sigma_\\theta^2}\n",
    "     \\end{bmatrix}\n",
    "     $\n",
    "\n",
    "3. **Weighted Error Norm**:\n",
    "   - The term $-\\frac{1}{2} \\mathbf{e}_{12}^\\top \\Sigma_{12}^{-1} \\mathbf{e}_{12}$ computes the Mahalanobis distance, which accounts for the uncertainty in the odometry measurement.\n",
    "\n",
    "---\n",
    "\n",
    "### Fully Expanded $\\psi_{\\text{odom12}}(\\mathbf{x}_1, \\mathbf{x}_2)$\n",
    "\n",
    "Substituting everything, the equation becomes:\n",
    "\n",
    "$\n",
    "\\psi_{\\text{odom12}}(\\mathbf{x}_1, \\mathbf{x}_2) = \\exp\\left(-\\frac{1}{2} \\left[(\\mathbf{x}_2 - \\mathbf{x}_1) - \\mathbf{z}_{12}\\right]^\\top \\Sigma_{12}^{-1} \\left[(\\mathbf{x}_2 - \\mathbf{x}_1) - \\mathbf{z}_{12}\\right]\\right)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Example in 2D\n",
    "Suppose:\n",
    "- $\\mathbf{x}_1 = (0, 0, 0)$ (robot's pose at step 1: $(x, y, \\theta)$).\n",
    "- $\\mathbf{x}_2 = (2, 1, 0.1)$ (robot's pose at step 2).\n",
    "- $\\mathbf{z}_{12} = (2, 1, 0.1)$ (measured relative odometry: 2m forward, 1m right, and 0.1 rad rotation).\n",
    "- Covariance matrix for odometry noise:\n",
    "  $\n",
    "  \\Sigma_{12} =\n",
    "  \\begin{bmatrix}\n",
    "  0.1^2 & 0 & 0 \\\\\n",
    "  0 & 0.1^2 & 0 \\\\\n",
    "  0 & 0 & 0.05^2\n",
    "  \\end{bmatrix}\n",
    "  $\n",
    "\n",
    "#### Steps:\n",
    "1. Compute the error:\n",
    "   $\n",
    "   \\mathbf{e}_{12} = (\\mathbf{x}_2 - \\mathbf{x}_1) - \\mathbf{z}_{12} = (2, 1, 0.1) - (2, 1, 0.1) = (0, 0, 0)\n",
    "   $\n",
    "\n",
    "2. Compute the Mahalanobis distance:\n",
    "   Since $\\mathbf{e}_{12} = (0, 0, 0)$, the Mahalanobis distance is:\n",
    "   $\n",
    "   \\mathbf{e}_{12}^\\top \\Sigma_{12}^{-1} \\mathbf{e}_{12} = 0\n",
    "   $\n",
    "\n",
    "3. Evaluate the factor:\n",
    "   $\n",
    "   \\psi_{\\text{odom12}}(\\mathbf{x}_1, \\mathbf{x}_2) = \\exp\\left(-\\frac{1}{2} \\cdot 0\\right) = 1\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### Key Points\n",
    "- If the error $\\mathbf{e}_{12}$ is small, the factor's value ($\\psi_{\\text{odom12}}$) will be close to 1, indicating high consistency with the measurement.\n",
    "- If $\\mathbf{e}_{12}$ is large (e.g., if the robot’s estimated poses deviate significantly from the measured odometry), $\\psi_{\\text{odom12}}$ will be much smaller, penalizing the inconsistent estimate.\n",
    "\n",
    "Would you like to extend this example to include numerical optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144aab9f-c089-416e-a672-c19fbfcdec7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
