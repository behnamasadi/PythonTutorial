{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "parallel-jewelry",
   "metadata": {},
   "source": [
    "# KL divergence\n",
    "The most common type of variational Bayes uses the Kullbackâ€“Leibler divergence, which makes this minimization tractable.\n",
    "\n",
    "\n",
    "${\\displaystyle D_{\\mathrm {KL} }(Q\\parallel P)\\triangleq \\sum _{\\mathbf {Z} }Q(\\mathbf {Z} )\\log {\\frac {Q(\\mathbf {Z} )}{P(\\mathbf {Z} \\mid \\mathbf {X} )}}.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-developer",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "|x |\t0   |\t1   |   2   |\n",
    "|---|-------|-------|-------|\n",
    "|Distribution P(x)| 9/25| 12/25|4/25|\n",
    "|Distribution Q(x)|1/3| 1/3|1/3|\n",
    "\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}D_{\\text{KL}}(P\\parallel Q)&=\\sum _{x\\in {\\mathcal {X}}}P(x)\\ln \\left({\\frac {P(x)}{Q(x)}}\\right)\\\\&={\\frac {9}{25}}\\ln \\left({\\frac {9/25}{1/3}}\\right)+{\\frac {12}{25}}\\ln \\left({\\frac {12/25}{1/3}}\\right)+{\\frac {4}{25}}\\ln \\left({\\frac {4/25}{1/3}}\\right)\\\\&={\\frac {1}{25}}\\left(32\\ln(2)+55\\ln(3)-50\\ln(5)\\right)\\approx 0.0852996\\end{aligned}}}$\n",
    "\n",
    "\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}D_{\\text{KL}}(Q\\parallel P)&=\\sum _{x\\in {\\mathcal {X}}}Q(x)\\ln \\left({\\frac {Q(x)}{P(x)}}\\right)\\\\&={\\frac {1}{3}}\\ln \\left({\\frac {1/3}{9/25}}\\right)+{\\frac {1}{3}}\\ln \\left({\\frac {1/3}{12/25}}\\right)+{\\frac {1}{3}}\\ln \\left({\\frac {1/3}{4/25}}\\right)\\\\&={\\frac {1}{3}}\\left(-4\\ln(2)-6\\ln(3)+6\\ln(5)\\right)\\approx 0.097455\\end{aligned}}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-cancer",
   "metadata": {},
   "source": [
    "## KL divergence between two univariate Gaussians\n",
    "\n",
    "$\\begin{align}\n",
    "KL(p, q) &= - \\int p(x) \\log q(x) dx + \\int p(x) \\log p(x) dx\\\\\\\\\n",
    "&=\\frac{1}{2} \\log (2 \\pi \\sigma_2^2) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2} - \\frac{1}{2} (1 + \\log 2 \\pi \\sigma_1^2)\\\\\\\\\n",
    "&= \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2} - \\frac{1}{2}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-adobe",
   "metadata": {},
   "source": [
    "## KL divergence between two multivariate Gaussians\n",
    "$\\begin{aligned}\n",
    "KL &= \\int \\left[ \\frac{1}{2} \\log\\frac{|\\Sigma_2|}{|\\Sigma_1|} - \\frac{1}{2} (x-\\mu_1)^T\\Sigma_1^{-1}(x-\\mu_1) + \\frac{1}{2} (x-\\mu_2)^T\\Sigma_2^{-1}(x-\\mu_2) \\right] \\times p(x) dx \\\\\n",
    "&= \\frac{1}{2} \\log\\frac{|\\Sigma_2|}{|\\Sigma_1|} - \\frac{1}{2} \\text{tr}\\ \\left\\{E[(x - \\mu_1)(x - \\mu_1)^T] \\ \\Sigma_1^{-1} \\right\\} + \\frac{1}{2} E[(x - \\mu_2)^T \\Sigma_2^{-1} (x - \\mu_2)] \\\\\n",
    "&= \\frac{1}{2} \\log\\frac{|\\Sigma_2|}{|\\Sigma_1|} - \\frac{1}{2} \\text{tr}\\ \\{I_d \\} + \\frac{1}{2} (\\mu_1 - \\mu_2)^T \\Sigma_2^{-1} (\\mu_1 - \\mu_2) + \\frac{1}{2} \\text{tr} \\{ \\Sigma_2^{-1} \\Sigma_1 \\} \\\\\n",
    "&= \\frac{1}{2}\\left[\\log\\frac{|\\Sigma_2|}{|\\Sigma_1|} - d + \\text{tr} \\{ \\Sigma_2^{-1}\\Sigma_1 \\} + (\\mu_2 - \\mu_1)^T \\Sigma_2^{-1}(\\mu_2 - \\mu_1)\\right].\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-utility",
   "metadata": {},
   "source": [
    "Refs [1](https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians), [2](https://stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
