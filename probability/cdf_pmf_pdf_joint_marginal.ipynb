{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optional-viking",
   "metadata": {},
   "source": [
    "## 1. Probability Density Function\n",
    "### 1.1. Discrete Random Variable PMF\n",
    " \n",
    "For a discrete random variable $X$ that takes on a finite or countably infinite number of possible values, we determined  for all of the possible values of , and called it the probability mass function **p.m.f.**\n",
    "p.m.f. gives the probability that a discrete random variable is exactly equal to some value.\n",
    " \n",
    "### Example of p.m.f\n",
    "we recorded the sequence of heads and tails in two tosses of a fair coin. The sample space for this random experiment is given by: $S = \\{hh, ht, th, tt\\}.\\notag$\n",
    "\n",
    "Suppose we are only interested in tosses that result in heads. We can define a random variable  $X$  that tracks the number of heads obtained in an outcome.\n",
    "\n",
    "\n",
    "$ X: S  \\rightarrow \\mathbb{R} $\n",
    "\n",
    "\n",
    "$\\begin{align*} \n",
    " S\\ &\\stackrel{\\text{function:}\\ X}{\\longrightarrow}\\ \\text{outputs:}\\ \\mathbb{R} \\\\ \n",
    "hh &\\quad\\stackrel{X}{\\mapsto}\\quad 2 \\\\ \n",
    "th &\\quad\\stackrel{X}{\\mapsto}\\quad 1 \\\\ \n",
    "ht &\\quad\\stackrel{X}{\\mapsto}\\quad 1 \\\\ \n",
    "tt &\\quad\\stackrel{X}{\\mapsto}\\quad 0 \n",
    "\\end{align*}$\n",
    "\n",
    "\n",
    "we compute the probability that the random variable  $X$  equals  1. There are two outcomes that lead to  $X$  taking the value 1, namely  $ht$  and  $th$\n",
    "\n",
    "$X(hh) = 2,\\quad X(ht) = X(th) = 1,\\quad X(tt) = 0.\\notag$\n",
    "\n",
    "$ P(X=1) = P(\\{ht, th\\}) = \\frac{\\text{# outcomes in}\\ \\{ht, th\\}}{\\text{# outcomes in}\\ S} = \\frac{2}{4} = 0.5\\notag $\n",
    "\n",
    "\n",
    "$\\begin{align*} \n",
    "p(0) &= P(X=0) = P(\\{tt\\}) = 0.25 \\\\ \n",
    "p(2) &= P(X=2) = P(\\{hh\\}) = 0.25 \n",
    "\\end{align*}$\n",
    "\n",
    "We can represent probability mass functions numerically with:\n",
    "- Table\n",
    "- Graphically with a histogram, \n",
    "- Analytically with a formula.\n",
    "\n",
    "<img src='images/pmf_table_histogram.png'>\n",
    "\n",
    "Refs: <a href=\"https://stats.libretexts.org/Courses/Saint_Mary's_College_Notre_Dame/MATH_345__-_Probability_(Kuter)/3%3A_Discrete_Random_Variables/3.2%3A_Probability_Mass_Functions_(PMFs)_and_Cumulative_Distribution_Functions_(CDFs)_for_Discrete_Random_Variables#:~:text=In%20Example%203.2.,we%20found%20to%20be%200.5.\">1</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-trademark",
   "metadata": {},
   "source": [
    "### 1.2. Continuous Random Variable PDF\n",
    "For continuous random variables $X$, the probability that $X$ takes on any particular value  is 0. That is, finding $P(X=x)$ for a continuous random variable. Instead, we'll need to find the probability that  falls in some interval $(a,b)$ , that is, we'll need to find $P(a<X<b)$. We'll do that using a probability density function **p.d.f.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-boxing",
   "metadata": {},
   "source": [
    "#### Examples of continuous p.d.f\n",
    "\n",
    "\n",
    "1) suppose $X$ is uniformly distributed on the unit interval ${\\displaystyle [a,b]}$.\n",
    "\n",
    "${\\displaystyle {\\begin{cases}{\\frac {1}{b-a}}&{\\text{for }}x\\in [a,b]\\\\0&{\\text{otherwise}}\\end{cases}}}$\n",
    "\n",
    "\n",
    "2) Suppose $X$ is exponential distributed. Then the p.d.f of $X$ is given by\n",
    "\n",
    "$\t{\\displaystyle \\lambda e^{-\\lambda x}}$\n",
    "\n",
    "3) Suppose $X$ is normal distributed. Then the p.d.f of $X$ is given by\n",
    "\n",
    "${\\displaystyle {\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-shanghai",
   "metadata": {},
   "source": [
    "## 2. Cumulative Distribution Function c.d.f\n",
    "\n",
    "The cumulative distribution function, **c.d.f.** of a random variable $X$ is a function on the real numbers that is denoted as  𝐹  and is given by:\n",
    "\n",
    "$F(x) = P(X\\leq x),\\quad \\text{for any}\\ x\\in\\mathbb{R}. \\label{cdf}$\n",
    "\n",
    "${\\displaystyle F:\\mathbb {R} \\rightarrow [0,1]}$ \n",
    "\n",
    " \n",
    "${\\displaystyle \\lim _{x\\rightarrow -\\infty }F(x)=0}$   \n",
    "\n",
    "\n",
    "${\\displaystyle \\lim _{x\\rightarrow \\infty }F(x)=1}$\n",
    " \n",
    "\n",
    "${\\displaystyle \\operatorname {P} (a<X\\leq b)=F_{X}(b)-F_{X}(a)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-evanescence",
   "metadata": {},
   "source": [
    "### 2.1. Discrete Random Variable c.d.f\n",
    "If $X$ is a purely discrete random variable, then it attains values ${\\displaystyle x_{1},x_{2},\\ldots }$   with probability ${\\displaystyle p_{i}=p(x_{i})}$, and the **c.d.f** of $X$ will be discontinuous at the points $x_{i}$:\n",
    "\n",
    "${\\displaystyle F_{X}(x)=\\operatorname {P} (X\\leq x)=\\sum _{x_{i}\\leq x}\\operatorname {P} (X=x_{i})=\\sum _{x_{i}\\leq x}p(x_{i}).} $\n",
    "\n",
    "#### Examples of discrete c.d.f\n",
    "1. In the tossing a fair coin example, we have: \n",
    "\n",
    "\n",
    "$\\begin{align*} \n",
    "F(0) &= P(X\\leq0) = P(X=0) = 0.25 \\\\ \n",
    "F(1) &= P(X\\leq1) = P(X=0\\ \\text{or}\\ 1) = p(0) + p(1) = 0.75 \\\\ \n",
    "F(2) &= P(X\\leq2) = P(X=0\\ \\text{or}\\ 1\\ \\text{or}\\ 2) = p(0) + p(1) + p(2) = 1 \n",
    "\\end{align*}$\n",
    "\n",
    "\n",
    "$F(x) = \\left\\{\\begin{array}{l l} \n",
    "0, & \\text{for}\\ x<0 \\\\ \n",
    "0.25 & \\text{for}\\ 0\\leq x <1 \\\\ \n",
    "0.75 & \\text{for}\\ 1\\leq x <2 \\\\ \n",
    "1 & \\text{for}\\ x\\geq 2. \n",
    "\\end{array}\\right.\\notag$\n",
    "\n",
    "\n",
    "2. A random variable  $X$  has a Bernoulli distribution with parameter  $p$ , where  $0≤𝑝≤1$ , if it has only two possible values, typically denoted  0  and  1. \n",
    "\n",
    "Bernoulli distribution **p.m.f.**:\n",
    "\n",
    "\n",
    "$\\begin{align*} \n",
    "p(0) &= P(X=0) = 1-p,\\\\ \n",
    "p(1) &= P(X=1) = p. \n",
    "\\end{align*}$\n",
    "\n",
    "\n",
    "Bernoulli distribution **c.d.f.**:\n",
    "\n",
    "$F(x) = \\left\\{\\begin{array}{r r} \n",
    "0, & x<0 \\\\ \n",
    "1-p, & 0\\leq x<1, \\\\ \n",
    "1, & x\\geq1. \n",
    "\\end{array}\\right.\\label{Berncdf}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-singapore",
   "metadata": {},
   "source": [
    "### 2.2. Continuous Random Variable \n",
    "The **c.d.f** of a continuous random variable $X$ can be expressed as follows:\n",
    "\n",
    "\n",
    "$F(x) = P(X\\leq x) = \\int\\limits^x_{-\\infty}\\! f(t)\\, dt, \\quad\\text{for}\\ x\\in\\mathbb{R}.\\notag$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-therapist",
   "metadata": {},
   "source": [
    "#### Examples of continuous c.d.f\n",
    "1) suppose $X$ is uniformly distributed on the unit interval ${\\displaystyle [a,b]}$.\n",
    "\n",
    "${\\displaystyle {\\begin{cases}0&{\\text{for }}x<a\\\\{\\frac {x-a}{b-a}}&{\\text{for }}x\\in [a,b]\\\\1&{\\text{for }}x>b\\end{cases}}}$\n",
    "\n",
    "\n",
    "2) Suppose $X$ is exponential distributed. Then the c.d.f of $X$ is given by\n",
    "\n",
    "${\\displaystyle F_{X}(x;\\lambda )={\\begin{cases}1-e^{-\\lambda x}&x\\geq 0,\\\\0&x<0.\\end{cases}}}$\n",
    "\n",
    "3) Suppose $X$ is normal distributed. Then the c.d.f of $X$ is given by\n",
    "\n",
    "${\\displaystyle F(x;\\mu ,\\sigma )={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}\\int _{-\\infty }^{x}\\exp \\left(-{\\frac {(t-\\mu )^{2}}{2\\sigma ^{2}}}\\ \\right)\\,dt}={\\displaystyle {\\frac {1}{2}}\\left[1+\\operatorname {erf} \\left({\\frac {x-\\mu }{\\sigma {\\sqrt {2}}}}\\right)\\right]}$\n",
    "\n",
    "4) Suppose $X$ is binomial distributed. Then the c.d.f of $X$ is given by\n",
    "\n",
    "${\\displaystyle F(k;n,p)=\\Pr(X\\leq k)=\\sum _{i=0}^{\\lfloor k\\rfloor }{n \\choose i}p^{i}(1-p)^{n-i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-syndicate",
   "metadata": {},
   "source": [
    "### 2.3. Relationship between PDF and CDF for a Continuous Random Variable\n",
    "\n",
    "Let  $X$  be a continuous random variable with pdf $f$  and cdf  $F$:\n",
    "- The cdf is found by integrating the pdf: $F(x) = \\int\\limits^x_{-\\infty}\\! f(t)\\, dt\\notag$\n",
    "- The pdf can be found by differentiating the cdf: $f(x) = \\frac{d}{dx}\\left[F(x)\\right]\\notag$\n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "Lets say your you have the following p.d.f\n",
    "\n",
    "$f(x) = \\left\\{\\begin{array}{l l} \n",
    "x, & \\text{for}\\ 0\\leq x\\leq 1 \\\\ \n",
    "2-x, & \\text{for}\\ 1< x\\leq 2 \\\\ \n",
    "0, & \\text{otherwise} \n",
    "\\end{array}\\right.\\notag$\n",
    "\n",
    "The c.d.f would be the following:\n",
    "\n",
    "\n",
    "$F(x) = \\left\\{\\begin{array}{l l} \n",
    "0, & \\text{for}\\ x<0 \\\\ \n",
    "\\frac{x^2}{2}, & \\text{for}\\ 0\\leq x \\leq 1 \\\\ \n",
    "2x - \\frac{x^2}{2} - 1, & \\text{for}\\ 1< x\\leq 2 \\\\ \n",
    "1, & \\text{for}\\ x>2 \n",
    "\\end{array}\\right.\\notag$\n",
    "\n",
    "Refs: <a href=\"https://stats.libretexts.org/Courses/Saint_Mary's_College_Notre_Dame/MATH_345__-_Probability_(Kuter)/4%3A_Continuous_Random_Variables/4.1%3A_Probability_Density_Functions_(PDFs)_and_Cumulative_Distribution_Functions_(CDFs)_for_Continuous_Random_Variables#Example_.5C(.5CPageIndex.7B1.7D.5C)\"> 1</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-neighborhood",
   "metadata": {},
   "source": [
    "## 3. Joint Probability Distribution\n",
    "\n",
    "Given random variables ${\\displaystyle X,Y,\\ldots }$, that are defined on a probability space, the joint probability distribution for ${\\displaystyle X,Y,\\ldots }$ is a probability distribution that gives the probability that each of ${\\displaystyle X,Y,\\ldots }$ falls in any particular range or discrete set of values specified for that variable.\n",
    "\n",
    "\n",
    "\n",
    "The joint probability mass function of two discrete random variables $X,Y$ is:\n",
    "\n",
    "${\\displaystyle p_{X,Y}(x,y)=\\mathrm {P} (X=x\\ \\mathrm {and} \\ Y=y)}$\n",
    "\n",
    "\n",
    "or written in terms of conditional distributions\n",
    "\n",
    "${\\displaystyle p_{X,Y}(x,y)=\\mathrm {P} (Y=y\\mid X=x)\\cdot \\mathrm {P} (X=x)=\\mathrm {P} (X=x\\mid Y=y)\\cdot \\mathrm {P} (Y=y)}$\n",
    "\n",
    "\n",
    "where ${\\displaystyle \\mathrm {P} (Y=y\\mid X=x)}\\mathrm {P} $ is the probability of ${\\displaystyle Y=y}$ given that ${\\displaystyle X=x}$.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the tossing of a fair die and let $A = 1$ if the number is even (i.e., 2, 4, or 6) and $A = 0$ otherwise. Furthermore, let $B = 1$ if the number is prime (i.e., 2, 3, or 5) and $B = 0$ otherwise.\n",
    "\n",
    "\n",
    "|   |1\t|2\t|3\t|4\t|5\t|6  |\n",
    "|---|---|---|---|---|---|---|\n",
    "|A\t|0\t|1\t|0\t|1\t|0\t|1  |\n",
    "|B\t|0\t|1\t|1\t|0\t|1\t|0  |\n",
    "\n",
    "Then, the joint distribution of $A$ and $B$, expressed as a probability mass function, is:\n",
    "\n",
    "$P(A=0,B=0)=P\\{1\\}=\\frac{1}{6}$\n",
    "\n",
    "$P(A=0,B=1)=P\\{3,5\\}=\\frac{2}{6}$\n",
    "\n",
    "$P(A=1,B=0)=P\\{4,6\\}=\\frac{2}{6}$\n",
    "\n",
    "$P(A=1,B=1)=P\\{2\\}=\\frac{1}{6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea4007-a831-4ba1-9917-feb3ccccedba",
   "metadata": {},
   "source": [
    "# 4. Marginal Distributions\n",
    "jointly Gaussian random vectors $ \\mathbf{x} $ and $ \\mathbf{y} $, where $ [\\mathbf{x}, \\mathbf{y}] $ denotes the joint vector formed by stacking $ \\mathbf{x} $ and $ \\mathbf{y} $. The joint vector follows a multivariate normal (Gaussian) distribution with a mean vector and a covariance matrix structured as given:\n",
    "\n",
    "$\n",
    "[\\mathbf{x}, \\mathbf{y}] \\sim \\mathcal{N}\\left(\\begin{bmatrix} \\mu_{\\mathbf{x}} \\\\ \\mu_{\\mathbf{y}} \\end{bmatrix}, \\begin{bmatrix} A & C \\\\ C^T & B \\end{bmatrix}\\right)\n",
    "$\n",
    "\n",
    "### Definitions:\n",
    "\n",
    "1. **Mean Vectors**:\n",
    "   - $ \\mu_{\\mathbf{x}} $ is the mean vector of the random vector $ \\mathbf{x} $.\n",
    "   - $ \\mu_{\\mathbf{y}} $ is the mean vector of the random vector $ \\mathbf{y} $.\n",
    "\n",
    "2. **Covariance Matrix**:\n",
    "   - $ A $ is the covariance matrix of $ \\mathbf{x} $.\n",
    "   - $ B $ is the covariance matrix of $ \\mathbf{y} $.\n",
    "   - $ C $ is the covariance matrix representing the covariance between $ \\mathbf{x} $ and $ \\mathbf{y} $.\n",
    "\n",
    "### Understanding the Joint and Marginal Distributions:\n",
    "\n",
    "**Joint Distribution**:\n",
    "The joint distribution of $ \\mathbf{x} $ and $ \\mathbf{y} $ as specified tells us how the vectors $ \\mathbf{x} $ and $ \\mathbf{y} $ vary together. The matrix $ \\begin{bmatrix} A & C \\\\ C^T & B \\end{bmatrix} $ fully specifies:\n",
    "- How $ \\mathbf{x} $ varies with itself (through $ A $),\n",
    "- How $ \\mathbf{y} $ varies with itself (through $ B $), and\n",
    "- How $ \\mathbf{x} $ and $ \\mathbf{y} $ co-vary (through $ C $ and $ C^T $).\n",
    "\n",
    "**Marginal Distribution of $ \\mathbf{x} $**:\n",
    "The marginal distribution of $ \\mathbf{x} $ refers to the distribution of $ \\mathbf{x} $ irrespective of $ \\mathbf{y} $. It is derived from the joint distribution by considering only the elements related to $ \\mathbf{x} $, which in the case of the given joint distribution are:\n",
    "\n",
    "$\n",
    "\\mathbf{x} \\sim \\mathcal{N}(\\mu_{\\mathbf{x}}, A)\n",
    "$\n",
    "\n",
    "Here, $ A $ is the covariance matrix of $ \\mathbf{x} $, reflecting how $ \\mathbf{x} $ varies with itself, independently of $ \\mathbf{y} $. The mean $ \\mu_{\\mathbf{x}} $ remains the same as in the joint distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ce657-04ed-48bd-b0e3-86776824cbb1",
   "metadata": {},
   "source": [
    "Consider a dataset with two variables, $ X $ and $ Y $. The joint distribution gives the probabilities of all possible combinations of $ X $ and $ Y $. The marginal distribution of $ X $ is found by summing (or integrating, in the case of continuous variables) the joint probabilities over all possible values of $ Y $.\n",
    "\n",
    "### Numerical Example\n",
    "\n",
    "Let's take a simple example with discrete variables.\n",
    "\n",
    "#### Joint Probability Distribution\n",
    "\n",
    "Assume we have the following joint probability distribution of $ X $ and $ Y $:\n",
    "\n",
    "| $ X $ | $ Y = 1 $ | $ Y = 2 $ | $ Y = 3 $ | Marginal Distribution of $ X $ |\n",
    "|:------:|:-----------:|:-----------:|:-----------:|:---------------------------------:|\n",
    "|   1    |     0.1     |     0.2     |     0.1     |                ?                  |\n",
    "|   2    |     0.05    |     0.1     |     0.05    |                ?                  |\n",
    "|   3    |     0.2     |     0.1     |     0.1     |                ?                  |\n",
    "\n",
    "To find the marginal distribution of $ X $, we sum the joint probabilities over all values of $ Y $ for each $ X $.\n",
    "\n",
    "#### Calculation\n",
    "\n",
    "For $ X = 1 $:\n",
    "$ P(X = 1) = P(X = 1, Y = 1) + P(X = 1, Y = 2) + P(X = 1, Y = 3) = 0.1 + 0.2 + 0.1 = 0.4 $\n",
    "\n",
    "For $ X = 2 $:\n",
    "$ P(X = 2) = P(X = 2, Y = 1) + P(X = 2, Y = 2) + P(X = 2, Y = 3) = 0.05 + 0.1 + 0.05 = 0.2 $\n",
    "\n",
    "For $ X = 3 $:\n",
    "$ P(X = 3) = P(X = 3, Y = 1) + P(X = 3, Y = 2) + P(X = 3, Y = 3) = 0.2 + 0.1 + 0.1 = 0.4 $\n",
    "\n",
    "#### Marginal Distribution of $ X $\n",
    "\n",
    "Now we can update our table with the marginal distributions:\n",
    "\n",
    "| $ X $ | $ Y = 1 $ | $ Y = 2 $ | $ Y = 3 $ | Marginal Distribution of $ X $ |\n",
    "|:------:|:-----------:|:-----------:|:-----------:|:---------------------------------:|\n",
    "|   1    |     0.1     |     0.2     |     0.1     |               0.4                 |\n",
    "|   2    |     0.05    |     0.1     |     0.05    |               0.2                 |\n",
    "|   3    |     0.2     |     0.1     |     0.1     |               0.4                 |\n",
    "\n",
    "So, the marginal distribution of $ X $ is:\n",
    "- $ P(X = 1) = 0.4 $\n",
    "- $ P(X = 2) = 0.2 $\n",
    "- $ P(X = 3) = 0.4 $\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The marginal distribution of $ X $ provides the probabilities of $ X $ values regardless of $ Y $. It is derived by summing the joint probabilities across all possible values of $ Y $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1806425-1f03-49b5-905c-fc3c4a732fe5",
   "metadata": {},
   "source": [
    "# The Mean and Covariance of Conditional \n",
    "To find the conditional distribution of $ \\mathbf{x} $ given $ \\mathbf{y} $ when both are part of a joint Gaussian distribution, we start with the assumption that the joint distribution of $ \\mathbf{x} $ and $ \\mathbf{y} $ is multivariate normal. \n",
    "\n",
    "Let $ \\mathbf{z} $ be the concatenation of $ \\mathbf{x} $ and $ \\mathbf{y} $:\n",
    "$ \\mathbf{z} = \\begin{bmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\end{bmatrix} $\n",
    "\n",
    "Assume $ \\mathbf{z} $ follows a multivariate normal distribution:\n",
    "$ \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{\\mu_z}, \\mathbf{\\Sigma_z}) $\n",
    "where\n",
    "$ \\mathbf{\\mu_z} = \\begin{bmatrix} \\mathbf{\\mu_x} \\\\ \\mathbf{\\mu_y} \\end{bmatrix}, \\quad \\mathbf{\\Sigma_z} = \\begin{bmatrix} \\mathbf{\\Sigma_{xx}} & \\mathbf{\\Sigma_{xy}} \\\\ \\mathbf{\\Sigma_{yx}} & \\mathbf{\\Sigma_{yy}} \\end{bmatrix} $\n",
    "\n",
    "Given this structure, we aim to find the conditional distribution of $ \\mathbf{x} $ given $ \\mathbf{y} $. The result is a conditional normal distribution, which we can derive as follows.\n",
    "\n",
    "### Conditional Mean and Covariance\n",
    "\n",
    "1. **Conditional Mean:**\n",
    "   The conditional mean of $ \\mathbf{x} $ given $ \\mathbf{y} $ is:\n",
    "   $ \\mathbf{\\mu_{x|y}} = \\mathbf{\\mu_x} + \\mathbf{\\Sigma_{xy}} \\mathbf{\\Sigma_{yy}}^{-1} (\\mathbf{y} - \\mathbf{\\mu_y}) $\n",
    "\n",
    "2. **Conditional Covariance:**\n",
    "   The conditional covariance of $ \\mathbf{x} $ given $ \\mathbf{y} $ is:\n",
    "   $ \\mathbf{\\Sigma_{x|y}} = \\mathbf{\\Sigma_{xx}} - \\mathbf{\\Sigma_{xy}} \\mathbf{\\Sigma_{yy}}^{-1} \\mathbf{\\Sigma_{yx}} $\n",
    "\n",
    "### Resulting Conditional Distribution\n",
    "\n",
    "Thus, the conditional distribution of $ \\mathbf{x} $ given $ \\mathbf{y} $ is:\n",
    "$ \\mathbf{x} | \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{\\mu_{x|y}}, \\mathbf{\\Sigma_{x|y}}) $\n",
    "where:\n",
    "$ \\mathbf{\\mu_{x|y}} = \\mathbf{\\mu_x} + \\mathbf{\\Sigma_{xy}} \\mathbf{\\Sigma_{yy}}^{-1} (\\mathbf{y} - \\mathbf{\\mu_y}) $\n",
    "$ \\mathbf{\\Sigma_{x|y}} = \\mathbf{\\Sigma_{xx}} - \\mathbf{\\Sigma_{xy}} \\mathbf{\\Sigma_{yy}}^{-1} \\mathbf{\\Sigma_{yx}} $\n",
    "\n",
    "### Summary\n",
    "- The mean of the conditional distribution $ \\mathbf{x} | \\mathbf{y} $ shifts from $ \\mathbf{\\mu_x} $ to account for the information provided by $ \\mathbf{y} $.\n",
    "- The covariance of the conditional distribution is reduced, reflecting the decreased uncertainty about $ \\mathbf{x} $ given $ \\mathbf{y} $.\n",
    "\n",
    "This approach leverages the properties of the multivariate normal distribution, ensuring that the resulting conditional distribution is also Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ae5660-b0ae-4140-aeba-99490eb186eb",
   "metadata": {},
   "source": [
    "## Poof\n",
    "\n",
    "Certainly! Let's derive the equations for the conditional mean and conditional covariance of $\\mathbf{x}$ given $\\mathbf{y}$ in the context of a joint Gaussian distribution.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Consider the joint Gaussian distribution of the random vectors $\\mathbf{x}$ and $\\mathbf{y}$:\n",
    "$ \\mathbf{z} = \\begin{bmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\end{bmatrix} \\sim \\mathcal{N}\\left( \\begin{bmatrix} \\mathbf{\\mu_x} \\\\ \\mathbf{\\mu_y} \\end{bmatrix}, \\begin{bmatrix} \\mathbf{\\Sigma_{xx}} & \\mathbf{\\Sigma_{xy}} \\\\ \\mathbf{\\Sigma_{yx}} & \\mathbf{\\Sigma_{yy}} \\end{bmatrix} \\right) $\n",
    "\n",
    "Here:\n",
    "- $\\mathbf{x}$ is an $n$-dimensional random vector.\n",
    "- $\\mathbf{y}$ is a $p$-dimensional random vector.\n",
    "- $\\mathbf{\\mu_x}$ and $\\mathbf{\\mu_y}$ are the means of $\\mathbf{x}$ and $\\mathbf{y}$, respectively.\n",
    "- $\\mathbf{\\Sigma_{xx}}$ is the covariance matrix of $\\mathbf{x}$.\n",
    "- $\\mathbf{\\Sigma_{yy}}$ is the covariance matrix of $\\mathbf{y}$.\n",
    "- $\\mathbf{\\Sigma_{xy}}$ is the cross-covariance matrix between $\\mathbf{x}$ and $\\mathbf{y}$.\n",
    "- $\\mathbf{\\Sigma_{yx}} = \\mathbf{\\Sigma_{xy}}^\\top$.\n",
    "\n",
    "### Derivation of Conditional Mean\n",
    "\n",
    "The goal is to find the conditional distribution of $\\mathbf{x}$ given $\\mathbf{y} = \\mathbf{y_0}$.\n",
    "\n",
    "The joint Gaussian distribution can be written as:\n",
    "$ f(\\mathbf{z}) = f\\left( \\begin{bmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\end{bmatrix} \\right) = \\frac{1}{(2\\pi)^{\\frac{n+p}{2}} |\\mathbf{\\Sigma_z}|^{\\frac{1}{2}}} \\exp \\left( -\\frac{1}{2} (\\mathbf{z} - \\mathbf{\\mu_z})^\\top \\mathbf{\\Sigma_z}^{-1} (\\mathbf{z} - \\mathbf{\\mu_z}) \\right) $\n",
    "\n",
    "Using properties of the multivariate normal distribution, the conditional distribution of $\\mathbf{x}$ given $\\mathbf{y}$ is also normal:\n",
    "$ \\mathbf{x} | \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{\\mu_{x|y}}, \\mathbf{\\Sigma_{x|y}}) $\n",
    "\n",
    "#### Conditional Mean\n",
    "\n",
    "The conditional mean $\\mathbf{\\mu_{x|y}}$ is given by:\n",
    "$ \\mathbf{\\mu_{x|y}} = \\mathbf{\\mu_x} + \\mathbf{\\Sigma_{xy}} \\mathbf{\\Sigma_{yy}}^{-1} (\\mathbf{y} - \\mathbf{\\mu_y}) $\n",
    "\n",
    "#### Derivation:\n",
    "\n",
    "1. Consider the partitioned form of the joint covariance matrix:\n",
    "   $ \\mathbf{\\Sigma_z} = \\begin{bmatrix} \\mathbf{\\Sigma_{xx}} & \\mathbf{\\Sigma_{xy}} \\\\ \\mathbf{\\Sigma_{yx}} & \\mathbf{\\Sigma_{yy}} \\end{bmatrix} $\n",
    "\n",
    "2. The inverse of the partitioned covariance matrix $\\mathbf{\\Sigma_z}^{-1}$ can be expressed using block matrix inversion formulas:\n",
    "   $\n",
    "   \\mathbf{\\Sigma_z}^{-1} = \\begin{bmatrix}\n",
    "   \\mathbf{A} & \\mathbf{B} \\\\\n",
    "   \\mathbf{C} & \\mathbf{D}\n",
    "   \\end{bmatrix}\n",
    "   $\n",
    "   where:\n",
    "   $\n",
    "   \\mathbf{A} = \\mathbf{\\Sigma_{xx}}^{-1} - \\mathbf{\\Sigma_{xx}}^{-1} \\mathbf{\\Sigma_{xy}} (\\mathbf{\\Sigma_{yy}} - \\mathbf{\\Sigma_{yx}} \\mathbf{\\Sigma_{xx}}^{-1} \\mathbf{\\Sigma_{xy}})^{-1} \\mathbf{\\Sigma_{yx}} \\mathbf{\\Sigma_{xx}}^{-1}\n",
    "   $\n",
    "   \n",
    "   \n",
    "   \n",
    "   $\n",
    "   \\mathbf{B} = -\\mathbf{\\Sigma_{xx}}^{-1} \\mathbf{\\Sigma_{xy}} (\\mathbf{\\Sigma_{yy}} - \\mathbf{\\Sigma_{yx}} \\mathbf{\\Sigma_{xx}}^{-1} \\mathbf{\\Sigma_{xy}})^{-1}\n",
    "   $\n",
    "   \n",
    "   \n",
    "   \n",
    "   $\n",
    "   \\mathbf{C} = -(\\mathbf{\\Sigma_{yy}} - \\mathbf{\\Sigma_{yx}} \\mathbf{\\Sigma_{xx}}^{-1} \\mathbf{\\Sigma_{xy}})^{-1} \\mathbf{\\Sigma_{yx}} \\mathbf{\\Sigma_{xx}}^{-1}\n",
    "   $\n",
    "   $\n",
    "   \\mathbf{D} = (\\mathbf{\\Sigma_{yy}} - \\mathbf{\\Sigma_{yx}} \\mathbf{\\Sigma_{xx}}^{-1} \\mathbf{\\Sigma_{xy}})^{-1}\n",
    "   $\n",
    "\n",
    "3. The conditional mean formula comes from the linear property of the multivariate normal distribution and can be derived by completing the square in the exponent of the joint Gaussian density function. After completion, it is evident that the mean shifts by the term involving the covariance matrices and the deviation of $\\mathbf{y}$ from its mean.\n",
    "\n",
    "### Derivation of Conditional Covariance\n",
    "\n",
    "The conditional covariance $\\mathbf{\\Sigma_{x|y}}$ is given by:\n",
    "$ \\mathbf{\\Sigma_{x|y}} = \\mathbf{\\Sigma_{xx}} - \\mathbf{\\Sigma_{xy}} \\mathbf{\\Sigma_{yy}}^{-1} \\mathbf{\\Sigma_{yx}} $\n",
    "\n",
    "#### Derivation:\n",
    "\n",
    "1. The conditional covariance matrix can be derived from the Schur complement of $\\mathbf{\\Sigma_{yy}}$ in $\\mathbf{\\Sigma_z}$.\n",
    "\n",
    "2. Intuitively, the conditional covariance matrix represents the reduction in uncertainty about $\\mathbf{x}$ after observing $\\mathbf{y}$. It accounts for the correlation between $\\mathbf{x}$ and $\\mathbf{y}$ and adjusts the variance accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e2223-b7ff-4a7d-b436-e45223eb0a37",
   "metadata": {},
   "source": [
    "To find the conditional distribution of $ \\mathbf{x} $ given $ \\mathbf{y} $ when both are part of a joint Gaussian distribution, we utilize the properties of multivariate normal distributions. Given that:\n",
    "\n",
    "$\n",
    "[\\mathbf{x}, \\mathbf{y}] \\sim \\mathcal{N}\\left(\\begin{bmatrix} \\mu_{\\mathbf{x}} \\\\ \\mu_{\\mathbf{y}} \\end{bmatrix}, \\begin{bmatrix} A & C \\\\ C^T & B \\end{bmatrix}\\right)\n",
    "$\n",
    "\n",
    "The conditional distribution $ \\mathbf{x} | \\mathbf{y} $ is also normally distributed where the mean and the covariance are calculated as follows:\n",
    "\n",
    "### Mean of $ \\mathbf{x} | \\mathbf{y} $:\n",
    "\n",
    "$\n",
    "\\text{Mean} = \\mu_{\\mathbf{x}} + C B^{-1} (\\mathbf{y} - \\mu_{\\mathbf{y}})\n",
    "$\n",
    "\n",
    "This equation represents the expected value of $ \\mathbf{x} $ given $ \\mathbf{y} $, where:\n",
    "- $ \\mu_{\\mathbf{x}} $ and $ \\mu_{\\mathbf{y}} $ are the mean vectors of $ \\mathbf{x} $ and $ \\mathbf{y} $ respectively.\n",
    "- $ C $ is the covariance matrix between $ \\mathbf{x} $ and $ \\mathbf{y} $.\n",
    "- $ B $ is the covariance matrix of $ \\mathbf{y} $, and $ B^{-1} $ is its inverse.\n",
    "- $ \\mathbf{y} $ is the observed value of the random vector $ \\mathbf{y} $.\n",
    "\n",
    "### Covariance of $ \\mathbf{x} | \\mathbf{y} $:\n",
    "\n",
    "$\n",
    "\\text{Covariance} = A - C B^{-1} C^T\n",
    "$\n",
    "\n",
    "This formula represents the covariance of $ \\mathbf{x} $ conditional on $ \\mathbf{y} $ and indicates how $ \\mathbf{x} $ varies around its new mean given $ \\mathbf{y} $:\n",
    "- $ A $ is the covariance matrix of $ \\mathbf{x} $.\n",
    "- $ C $, $ B $, and $ C^T $ are as defined above.\n",
    "\n",
    "### Conditional Distribution Expression\n",
    "\n",
    "Thus, the conditional distribution of $ \\mathbf{x} $ given $ \\mathbf{y} $ is:\n",
    "\n",
    "$\n",
    "\\mathbf{x} | \\mathbf{y} \\sim \\mathcal{N}(\\mu_{\\mathbf{x}} + C B^{-1} (\\mathbf{y} - \\mu_{\\mathbf{y}}), A - C B^{-1} C^T)\n",
    "$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "This result highlights a fundamental property of Gaussian vectors: the conditional distribution of a subset of the vector given the other subset is also Gaussian. The conditional mean $ \\mu_{\\mathbf{x}} + C B^{-1} (\\mathbf{y} - \\mu_{\\mathbf{y}}) $ adjusts the mean $ \\mu_{\\mathbf{x}} $ based on the deviation of $ \\mathbf{y} $ from its mean $ \\mu_{\\mathbf{y}} $, weighted by the covariance between $ \\mathbf{x} $ and $ \\mathbf{y} $ relative to the variance of $ \\mathbf{y} $. The conditional covariance $ A - C B^{-1} C^T $ reduces the uncertainty in $ \\mathbf{x} $ due to the knowledge of $ \\mathbf{y} $, reflecting less variability in $ \\mathbf{x} $ once $ \\mathbf{y} $ is known."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f52ee-be8a-4c85-b91e-899a525b86f2",
   "metadata": {},
   "source": [
    "# Physical Example in the context of robotics\n",
    "\n",
    "\n",
    "Let's create a physical example in the context of robotics that illustrates the conditional distribution of one variable given another when they follow a joint Gaussian distribution.\n",
    "\n",
    "### Scenario: Robot Localization\n",
    "\n",
    "Imagine a robot navigating a two-dimensional space, equipped with a GPS sensor and a compass. The robot's state can be described by two variables:\n",
    "1. $ x $: The robot's position along the x-axis.\n",
    "2. $ y $: The robot's heading angle (orientation) measured by the compass.\n",
    "\n",
    "The robot's state vector is:\n",
    "$ \\mathbf{z} = \\begin{bmatrix} x \\\\ y \\end{bmatrix} $\n",
    "\n",
    "Due to sensor noise and environmental factors, both $ x $ and $ y $ are random variables and are jointly Gaussian distributed.\n",
    "\n",
    "### Joint Gaussian Distribution\n",
    "\n",
    "Assume the robot's state follows this joint Gaussian distribution:\n",
    "$ \\mathbf{z} \\sim \\mathcal{N} \\left( \\begin{bmatrix} 5 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 2 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix} \\right) $\n",
    "\n",
    "- Mean position ($ x $): 5 meters along the x-axis.\n",
    "- Mean heading ($ y $): 0 radians (pointing straight forward).\n",
    "- Variance in position: 2 $(\\text{meters}^2)$\n",
    "- Variance in heading: 1 $(\\text{radians}^2)$\n",
    "- Covariance between position and heading: 0.5\n",
    "\n",
    "### Problem\n",
    "\n",
    "Given a specific heading measurement, $ y = y_0 $, we want to find the conditional distribution of the robot's position $ x $.\n",
    "\n",
    "### Conditional Distribution Calculation\n",
    "\n",
    "#### 1. Extract Parameters\n",
    "\n",
    "From the joint distribution:\n",
    "$ \\mathbf{\\mu_z} = \\begin{bmatrix} 5 \\\\ 0 \\end{bmatrix} $\n",
    "$ \\mathbf{\\Sigma_z} = \\begin{bmatrix} 2 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix} $\n",
    "\n",
    "#### 2. Conditional Mean\n",
    "\n",
    "The conditional mean of $ x $ given $ y = y_0 $:\n",
    "$ \\mu_{x|y} = \\mu_x + \\Sigma_{xy} \\Sigma_{yy}^{-1} (y_0 - \\mu_y) $\n",
    "\n",
    "Plugging in the values:\n",
    "- $\\mu_x = 5$\n",
    "- $\\mu_y = 0$\n",
    "- $\\Sigma_{xy} = 0.5$\n",
    "- $\\Sigma_{yy} = 1$\n",
    "\n",
    "$ \\mu_{x|y} = 5 + 0.5 \\cdot 1^{-1} (y_0 - 0) = 5 + 0.5 \\cdot y_0 $\n",
    "\n",
    "#### 3. Conditional Covariance\n",
    "\n",
    "The conditional covariance of $ x $ given $ y $:\n",
    "$ \\Sigma_{x|y} = \\Sigma_{xx} - \\Sigma_{xy} \\Sigma_{yy}^{-1} \\Sigma_{yx} $\n",
    "\n",
    "Plugging in the values:\n",
    "- $\\Sigma_{xx} = 2$\n",
    "- $\\Sigma_{xy} = 0.5$\n",
    "- $\\Sigma_{yy} = 1$\n",
    "\n",
    "$ \\Sigma_{x|y} = 2 - 0.5 \\cdot 1^{-1} \\cdot 0.5 = 2 - 0.25 = 1.75 $\n",
    "\n",
    "### Conditional Distribution\n",
    "\n",
    "Given $ y = y_0 $, the conditional distribution of $ x $ is:\n",
    "$ x | y = y_0 \\sim \\mathcal{N} \\left( 5 + 0.5 y_0, 1.75 \\right) $\n",
    "\n",
    "### Physical Interpretation\n",
    "\n",
    "1. **Prior Distribution:**\n",
    "   - Before any heading measurement, the robot's position $ x $ is normally distributed with mean 5 meters and variance 2 $(\\text{meters}^2)$.\n",
    "   - The heading $ y $ is normally distributed with mean 0 radians and variance 1 $(\\text{radians}^2)$.\n",
    "\n",
    "2. **Conditional Distribution:**\n",
    "   - Once the robot measures its heading $ y = y_0 $, it updates its belief about its position $ x $.\n",
    "   - The new mean position $ x $ is adjusted based on the measured heading $ y_0 $, specifically by the amount $ 0.5 y_0 $.\n",
    "   - The uncertainty (variance) about the position $ x $ is reduced to 1.75 $(\\text{meters}^2)$.\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "Suppose the robot measures its heading to be $ y_0 = 2 $ radians.\n",
    "\n",
    "1. **Conditional Mean:**\n",
    "   $ \\mu_{x|y} = 5 + 0.5 \\cdot 2 = 5 + 1 = 6 $\n",
    "\n",
    "2. **Conditional Covariance:**\n",
    "   $ \\Sigma_{x|y} = 1.75 $\n",
    "\n",
    "Given this heading measurement, the robot's updated belief about its position is:\n",
    "$ x | y = 2 \\sim \\mathcal{N} \\left( 6, 1.75 \\right) $\n",
    "\n",
    "This means the robot now believes it is centered around 6 meters along the x-axis, with a reduced uncertainty compared to before the heading measurement.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Joint Distribution:** $\\mathbf{z} = \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} 5 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 2 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix} \\right)$\n",
    "- **Conditional Distribution (given $ y = 2 $):** $ x | y = 2 \\sim \\mathcal{N} \\left( 6, 1.75 \\right) $\n",
    "\n",
    "This example shows how a robot can use joint Gaussian properties to update its position estimate based on a heading measurement, reducing uncertainty in its localization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30e83d-65cc-40c2-a8df-1b42a0b00f18",
   "metadata": {},
   "source": [
    "# Example of Weight \n",
    "\n",
    "\n",
    "Refs: [1](https://online.stat.psu.edu/stat414/lesson/21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-bedroom",
   "metadata": {},
   "source": [
    "### 3.1. Joint Probability Mass Function (joint pmf)\n",
    "\n",
    "If discrete random variables  $X$  and  $Y$  are defined on the same sample space  $S$ , then their joint probability mass function (joint pmf) is given by\n",
    "$p(x,y) = P(X=x\\ \\ \\text{and}\\ \\ Y=y),\\notag$\n",
    " \n",
    "\n",
    "where  $(x,y)$  is a pair of possible values for the pair of random variables  $(x,y)$ , and  $p(x,y)$  satisfies the following conditions:\n",
    "\n",
    "- $0 \\leq p(x,y) \\leq 1$ \n",
    "- $\\displaystyle{\\mathop{\\sum\\sum}_{(x,y)}p(x,y) = 1}$\n",
    "- $\\displaystyle{P\\left((X,Y)\\in A\\right)) = \\mathop{\\sum\\sum}_{(x,y)\\in A} p(x,y)}$\n",
    "\n",
    "\n",
    "Refs: [1](https://stats.libretexts.org/Courses/Saint_Mary's_College_Notre_Dame/MATH_345__-_Probability_(Kuter)/5%3A_Probability_Distributions_for_Combinations_of_Random_Variables/5.1%3A_Joint_Distributions_of_Discrete_Random_Variables#:~:text=Suppose%20that%20X%20and%20Y,p(x%2Cy).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-singing",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Consider example of tossing a coin three times, random variable $X$  denote the number of heads obtained, random variable  $Y$  denote the winnings earned in a single play,\n",
    "\n",
    "\n",
    "\n",
    "- $\\$1$ if first  $h$  occurs on the first toss $\\{hhh,htt,hht,hth\\}$\n",
    "- $\\$2$ if first $h$ occurs on the second toss $\\{thh,tht\\}$\n",
    "- $\\$3$ if first $h$ occurs on the third toss $\\{tth\\}$\n",
    "- $\\$-1$ if no $h$ occur $\\{ttt\\}$\n",
    "\n",
    "\n",
    "Note that the possible values of $X$ are  $x=0,1,2,3$ , and the possible values of  $Y$  are  $y=−1,1,2,3$ . \n",
    "\n",
    "The joint pmf table for our above exmple would be:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>p(x,y)</th>\n",
    "            <th class=\"mt-align-center\" colspan=\"4\" rowspan=\"1\" scope=\"row\">\\(X\\)</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th class=\"mt-align-center\" scope=\"row\">\\(Y\\)</th>\n",
    "            <th class=\"mt-align-center\">0</th>\n",
    "            <th class=\"mt-align-center\">1</th>\n",
    "            <th class=\"mt-align-center\">2</th>\n",
    "            <th class=\"mt-align-center\">3</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th class=\"mt-align-center\" scope=\"row\">-1</th>\n",
    "            <td class=\"mt-align-center\"><span class=\"mt-color-2ecc71\">1/8</span></td>\n",
    "            <td class=\"mt-align-center\">0</td>\n",
    "            <td class=\"mt-align-center\">0</td>\n",
    "            <td class=\"mt-align-center\">0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th class=\"mt-align-center\" scope=\"row\">1</th>\n",
    "            <td class=\"mt-align-center\">0</td>\n",
    "            <td class=\"mt-align-center\"><span class=\"mt-color-e67e22\">1/8</span></td>\n",
    "            <td class=\"mt-align-center\"><span class=\"mt-color-3498db\">2/8</span></td>\n",
    "            <td class=\"mt-align-center\"><span class=\"mt-color-8e44ad\">1/8</span></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th class=\"mt-align-center\" scope=\"row\">2</th>\n",
    "            <td class=\"mt-align-center\">0</td>\n",
    "            <td class=\"mt-align-center\"><span class=\"mt-color-e67e22\">1/8</span></td>\n",
    "            <td class=\"mt-align-center\"><span class=\"mt-color-3498db\">1/8</span></td>\n",
    "            <td class=\"mt-align-center\">0</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th class=\"mt-align-center\" scope=\"row\">3</th>\n",
    "            <td class=\"mt-align-center\">0</td>\n",
    "            <td class=\"mt-align-center\"><span class=\"mt-color-e67e22\">1/8</span></td>\n",
    "            <td class=\"mt-align-center\">0</td>\n",
    "            <td class=\"mt-align-center\">0</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "$S = \\{{ttt}, {htt}, {tht}, {tth}, {hht}, {hth}, {thh}, {hhh}\\}\\notag$\n",
    "\n",
    "$p(0,-1) = P(X=0\\ \\text{and}\\ Y=-1) = P(ttt) = \\frac{1}{8}.\\notag$\n",
    "\n",
    "\n",
    "$p(1,1) = P(X=1\\ \\text{and}\\ Y=1) = P(htt) = \\frac{1}{8}.\\notag$\n",
    "\n",
    "\n",
    "$p(2,1) = P(X=2\\ \\text{and}\\ Y=1) = P(\\text{tht or thh} ) = \\frac{2}{8}.\\notag$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-routine",
   "metadata": {},
   "source": [
    "### 3.2. Joint Cumulative Distribution function (joint cdf)\n",
    "In the discrete case, we can obtain the joint cumulative distribution function (joint cdf) of  $X$  and  $Y$  by summing the joint pmf:\n",
    "\n",
    "$F(x,y) = P(X\\leq x\\ \\text{and}\\ Y\\leq y) = \\sum_{x_i \\leq x} \\sum_{y_j \\leq y} p(x_i, y_j)\\notag$\n",
    "\n",
    "#### Example\n",
    "\n",
    "The joint cdf table for our above exmple would be:\n",
    "\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th class=\"mt-align-center\" scope=\"row\">F(x,y)</th>\n",
    "            <th class=\"mt-align-center\" colspan=\"4\" rowspan=\"1\" scope=\"col\">\\(X\\)</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th class=\"mt-align-center\" scope=\"row\">Y</th>\n",
    "            <th class=\"mt-align-center\">0</th>\n",
    "            <th class=\"mt-align-center\">1</th>\n",
    "            <th class=\"mt-align-center\">2</th>\n",
    "            <th class=\"mt-align-center\">3</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th class=\"mt-align-center\" scope=\"row\">-1</th>\n",
    "            <td class=\"mt-align-center\">1/8</td>\n",
    "            <td class=\"mt-align-center\">1/8</td>\n",
    "            <td class=\"mt-align-center\">1/8</td>\n",
    "            <td class=\"mt-align-center\">1/8</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th class=\"mt-align-center\" scope=\"row\">1</th>\n",
    "            <td class=\"mt-align-center\">1/8</td>\n",
    "            <td class=\"mt-align-center\">1/4</td>\n",
    "            <td class=\"mt-align-center\">1/2</td>\n",
    "            <td class=\"mt-align-center\">5/8</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th class=\"mt-align-center\" scope=\"row\">2</th>\n",
    "            <td class=\"mt-align-center\">1/8</td>\n",
    "            <td class=\"mt-align-center\">3/8</td>\n",
    "            <td class=\"mt-align-center\">3/4</td>\n",
    "            <td class=\"mt-align-center\">7/8</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th class=\"mt-align-center\" scope=\"row\">3</th>\n",
    "            <td class=\"mt-align-center\">1/8</td>\n",
    "            <td class=\"mt-align-center\">1/2</td>\n",
    "            <td class=\"mt-align-center\">7/8</td>\n",
    "            <td class=\"mt-align-center\">1</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "$F(1,1) = P(X\\leq1\\ \\text{and}\\ Y\\leq1) = \\sum_{x\\leq1}\\sum_{y\\leq1} p(x,y) = p(0,-1) + p(0,1) + p(-1,1) + p(1,1) = \\frac{1}{4}\\notag$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-savage",
   "metadata": {},
   "source": [
    "### Semicolon notation in joint probability\n",
    "\n",
    "In $p_{\\theta} (x|z, y) = f(x; z, y, \\theta)$, \n",
    "\n",
    "$f(x; z, y, \\theta)$\n",
    "\n",
    "is a function of $x$ with \"parameters\" $y,x,\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-seattle",
   "metadata": {},
   "source": [
    "## 4. Marginal Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-embassy",
   "metadata": {},
   "source": [
    "### 4.1.1. Marginal probability mass functions (Marginal pmf)\n",
    "\n",
    "\n",
    "Suppose that discrete random variables  $X$  and  $Y$  have joint pmf  $p(x,y)$. Let  $y_1, y_2, \\ldots, y_j, \\ldots$  denote the possible values of  $Y$ , and let  $x_1, x_2, \\ldots, x_i, \\ldots$  denote the possible values of  $X$ . The marginal probability mass functions (marginal pmf's) of  $X$  and  $Y$  are respectively given by the following:\n",
    "\n",
    "\n",
    "$\\begin{align*} \n",
    "p_X(x) &= \\sum_j p(x, y_j) \\quad(\\text{fix a value of}\\ X\\ \\text{and sum over possible values of}\\ Y) \\\\ \n",
    "p_Y(y) &= \\sum_i p(x_i, y) \\quad(\\text{fix a value of}\\ Y\\ \\text{and sum over possible values of}\\ X) \n",
    "\\end{align*}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-myrtle",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th  scope=\"col\">x</th>\n",
    "            <th  scope=\"col\">pₓ(x)</th>\n",
    "            <th  scope=\"col\">y</th>\n",
    "            <th  scope=\"col\">pᵧ(y)</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td >0</td>\n",
    "            <td >1/8</td>\n",
    "            <td >-1</td>\n",
    "            <td >1/8</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td >1</td>\n",
    "            <td >3/8</td>\n",
    "            <td >1</td>\n",
    "            <td >1/2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td >2</td>\n",
    "            <td >3/8</td>\n",
    "            <td >2</td>\n",
    "            <td >1/4</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td >3</td>\n",
    "            <td >1/8</td>\n",
    "            <td >3</td>\n",
    "            <td >1/8</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "According to the joint **pmf** table for our above exmple, we would have:\n",
    "\n",
    "$p_X(0) = \\sum_j p(X=0,y_j)= p(X=0,Y=-1)+p(X=0,Y=1)+p(X=0,Y=2)+p(X=0,Y=3) = \\frac{1}{8}+0+0+0.\\notag$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-partner",
   "metadata": {},
   "source": [
    "### 4.1.2. Marginal probability density functions (Marginal pdf)\n",
    "\n",
    "\n",
    "The marginal probability density functions of the continuous random variables $X$ and $Y$ are given, respectively, by:\n",
    "\n",
    "$f_X(x)=\\int_{-\\infty}^\\infty f(x,y)dy,\\qquad x\\in S_1$\n",
    "\n",
    "$f_Y(y)=\\int_{-\\infty}^\\infty f(x,y)dx,\\qquad y\\in S_2$\n",
    "\n",
    "#### Example\n",
    "Let $X$ and $Y$ have joint probability density function:\n",
    "\n",
    "\n",
    "$f(x,y) = \\left\\{\\begin{array}{l l} \n",
    "4xy &  0<x<1 , 0<y<1 \\\\ \n",
    "0 & \\text{otherwise}. \n",
    "\\end{array}\\right.\\notag$\n",
    "\n",
    "\n",
    "$f_X(x)=\\int_0^1 4xy dy=4x\\left[\\dfrac{y^2}{2}\\right]_{y=0}^{y=1}=2x, \\qquad 0<x<1$\n",
    "\n",
    "$f_Y(y)=\\int_0^1 4xy dx=4y\\left[\\dfrac{x^2}{2}\\right]_{x=0}^{x=1}=2y, \\qquad 0<y<1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-hamburg",
   "metadata": {},
   "source": [
    "### 4.2. Marginal cumulative distribution functio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-excuse",
   "metadata": {},
   "source": [
    "### 4.3. Marginal distribution vs. conditional distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-palace",
   "metadata": {},
   "source": [
    "### 4.4. Marginal Probability  and Expected Value\n",
    "\n",
    "A marginal probability can always be written as an expected value:\n",
    "\n",
    "${\\displaystyle p_{X}(x)=\\int _{y}p_{X\\mid Y}(x\\mid y)\\,p_{Y}(y)\\,\\mathrm {d} y=\\operatorname {E} _{Y}[p_{X\\mid Y}(x\\mid y)]\\;.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-virtue",
   "metadata": {},
   "source": [
    "### 4.5. Marginal likelihood\n",
    "\n",
    "A marginal likelihood function (integrated likelihood), is a likelihood function in which some parameter variables have been marginalized. \n",
    "\n",
    "#### In the context of Bayesian statistics\n",
    "Given a set of independent identically distributed data points ${\\displaystyle \\mathbf {X} =(x_{1},\\ldots ,x_{n}),}$, where $x_{i}\\sim p(x_{i}|\\theta )$ according to some probability distribution parameterized by $\\theta$ , where $\\theta$  itself is a random variable described by a distribution, i.e. ${\\displaystyle \\theta \\sim p(\\theta \\mid \\alpha ),}$ the marginal likelihood in general asks what the probability ${\\displaystyle p(\\mathbf {X} \\mid \\alpha )}$ is, where $\\theta$  has been marginalized out (integrated out): \n",
    "\n",
    "\n",
    "${\\displaystyle p(\\mathbf {X} \\mid \\alpha )=\\int _{\\theta }p(\\mathbf {X} \\mid \\theta )\\,p(\\theta \\mid \\alpha )\\ \\operatorname {d} \\!\\theta }$\n",
    "\n",
    "####  In classical statistics\n",
    "In In classical statistics, the concept of marginal likelihood occurs instead in the context of a joint parameter ${\\displaystyle \\theta =(\\psi ,\\lambda )}$, where $\\psi$  is the actual parameter of interest, and $\\lambda$  is a non-interesting nuisance parameter.\n",
    "\n",
    "\n",
    "We know that:\n",
    "\n",
    "$P(B|C)=\\sum_{i} P(B|A_i,C)P(A_i|C) $\n",
    "\n",
    "And we also know \n",
    "\n",
    "${\\mathcal {L}}(\\theta|X)=p(X|\\theta)=p_{\\theta }(X)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "by marginalizing out $\\lambda$ :\n",
    "\n",
    "${\\displaystyle {\\mathcal {L}}(\\psi ;\\mathbf {X} )=p(\\mathbf {X} \\mid \\psi )=\\int _{\\lambda }p(\\mathbf {X} \\mid \\lambda ,\\psi )\\,p(\\lambda \\mid \\psi )\\ \\operatorname {d} \\!\\lambda }$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-microwave",
   "metadata": {},
   "source": [
    "## Marginalization of conditional probability\n",
    "\n",
    "$P(E=e|A=a)=\\frac{P(E=e,A=a)}{P(A=a)}=\\frac{\\sum_{c}P(E=e,C=c,A=a)}{P(A=a)}$\n",
    "using the definition of conditional probability, this is equal to:\n",
    "$\\sum_{c}P(E=e,C=c|A=a)$\n",
    "\n",
    "Refs: [1](https://stats.stackexchange.com/questions/256271/marginalization-of-conditional-probability)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
