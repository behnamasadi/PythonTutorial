{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f98fbb0-58a4-4833-9dc3-55f0b113d50f",
   "metadata": {},
   "source": [
    "Steve is very shy and withdrawn, with little interest in people and needs for order and structure, is he more likely to be a librarian or a farmer?\n",
    "\n",
    "At first, it seems more like the description for the librarian, but you should incorporate the ratio of librarians to farmers\n",
    "1. In USA, the ratio of librarians to farmers is 1 to 20. \n",
    "<img src=\"images/librarians_to_farmers_ratio.png\" width=\"50%\" height=\"50%\" />\n",
    "\n",
    "2. The above description would fit 40 percent of librarians and 10 percent of farmers.\n",
    "<img src=\"images/description_fit.png\" width=\"50%\" height=\"50%\" />\n",
    "\n",
    "With that we can conclude:\n",
    "\n",
    "<img src=\"images/stat.png\" width=\"50%\" height=\"50%\" />\n",
    "\n",
    "\n",
    "Here our **Evidence** is **Description** and being **Librarians** is our **Hypothesis**\n",
    "\n",
    "\n",
    "$P(L|D)=\\frac{4}{4+24}$\n",
    "\n",
    "\n",
    "<img src=\"images/bayes_theorem_1.png\" width=\"50%\" height=\"50%\" />\n",
    "\n",
    "\n",
    "Although the chance that that description describes a librarian is 4 times more than the being a farmer, the new evidence shroud update your belief, it should update teh prior.\n",
    "\n",
    "<img src=\"images/bayes_theorem_2.png\" width=\"50%\" height=\"50%\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-inspection",
   "metadata": {},
   "source": [
    "## Conditional probability tables\n",
    "When the variables are discrete  we may think of the factors $p(x_i\\mid x_{A_i})$ as probability tables. rows correspond to assignments to $x_{A_i}$ and columns correspond to values of $x_i$. the entries contain the actual probabilities $p(x_i\\mid x_{A_i})$\n",
    "\n",
    "## Bayes' Theorem \n",
    "1. **Prior Probability $P(H)$**: This represents our initial belief about the probability of a hypothesis $H$ before we see any new data or evidence. It reflects what we know about the situation prior to the latest information.\n",
    "\n",
    "2. **Likelihood $P(E|H)$**: This is the probability of observing the evidence $E$ given that the hypothesis $H$ is true. It measures how likely we are to observe the evidence if the hypothesis holds.\n",
    "\n",
    "3. **Evidence $P(E)$**: Also known as the marginal likelihood, this is the probability of observing the evidence under all possible hypotheses. It can be calculated by considering all possible states of the world (all hypotheses) and summing or integrating over them.\n",
    "\n",
    "4. **Posterior Probability $P(H|E)$**: This is what Bayesian inference aims to compute - the probability of the hypothesis $H$ after taking into account the new evidence $E$ and our prior belief. This represents our updated belief about the hypothesis in light of the new evidence.\n",
    "\n",
    "Bayes' theorem provides the mathematical formula for computing the posterior probability:\n",
    "\n",
    "$ P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)} $\n",
    "\n",
    "\n",
    "\n",
    "${\\displaystyle {\\text{posterior}}={\\frac {{\\text{likelihood}}\\times {\\text{prior}}}{\\text{evidence}}}\\,}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-diesel",
   "metadata": {},
   "source": [
    "## Conditionally Independent\n",
    "\n",
    "If $A$ and $B$ are conditionally independent of $C$, written symbolically as: ${\\displaystyle (A\\perp \\!\\!\\!\\perp B|C)}$\n",
    "\n",
    "$P(A,B|C)=P(A|C)P(B|C)$\n",
    "\n",
    "$P(A|B,C)=P(A|C)$\n",
    "\n",
    "\n",
    "The concept of conditional independence can be visually represented using probability trees, Venn diagrams, or Bayesian networks. However, let's consider a simple Venn diagram with two events $A$ and $B$, and given that a third event $ C$ has occurred.\n",
    "\n",
    "1. **Event A**: The shaded area for A represents $ P(A | C) $\n",
    "2. **Event B**: The shaded area for B represents $ P(B | C) $\n",
    "3. **Event C**: The presence of C as the bounding box indicates we are looking at probabilities conditional on C.\n",
    "\n",
    "Assuming $ A$ and $ B $ are conditionally independent given $ C $, then the following holds:\n",
    "$\n",
    "P(A \\cap B | C) = P(A | C) \\times P(B | C)\n",
    "$\n",
    "\n",
    "You would represent this by showing that the overlap between A and B, given $C$, can be calculated as the product of the individual conditional probabilities.\n",
    "\n",
    "### Diagram\n",
    "\n",
    "Imagine a bounding box for event C; inside this box, we have two overlapping circles, one for event A and another for event B. \n",
    "\n",
    "```\n",
    "                      ---------------\n",
    "                     |       C         |\n",
    "                     |  -----------    |\n",
    "                     | |     A    |    |\n",
    "                     | | ---------|    |\n",
    "                     | | |   A∩B  |  | |\n",
    "                     | | |________|  | |\n",
    "                     | |     B       | |\n",
    "                     | --------------- |\n",
    "                      -----------------\n",
    "```\n",
    "\n",
    "### Calculating Area\n",
    "\n",
    "If we treat these shapes as geometric areas, we could say:\n",
    "\n",
    "- Area of $ C = 1 $ (because we're looking at conditional probabilities)\n",
    "- Area of $ A $ inside $ C = P(A | C) $\n",
    "- Area of $ B $ inside $ C = P(B | C) $\n",
    "- Overlapping Area of $ A $ and $ B $ inside $ C = P(A \\cap B | C) $\n",
    "\n",
    "Since $ A $ and $ B $ are conditionally independent given $ C $:\n",
    "$\n",
    "\\text{Area of \\( A \\cap B \\) inside \\( C \\)} = \\text{Area of \\( A \\) inside \\( C \\)} \\times \\text{Area of \\( B \\) inside \\( C \\)}\n",
    "$\n",
    "\n",
    "That would mean $ P(A \\cap B | C) = P(A | C) \\times P(B | C) $.\n",
    "\n",
    "In this case, the area representing $ A \\cap B $ within the boundary of $ C $ can be directly calculated by the product of the conditional probabilities of $ A $ and $ B $, given $ C $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-cathedral",
   "metadata": {},
   "source": [
    "## Independent Event\n",
    "\n",
    "Two events $A,B$ are said to be statistically independent if and only if \n",
    "\n",
    "$P(A,B)=P(A)P(B)$\n",
    "\n",
    "$P(A|B)=\\frac{P(A,B)}{P(B)}=\\frac{P(A)P(B)}{P(B)}=P(A)$\n",
    "\n",
    "Also $\\bar{B}$ and $A$ are independent, $P(A,\\bar{B})=P(A)P(\\bar{B})$\n",
    "\n",
    "If $X$ and $Y$ are independent random variables, then the expectation operator $\\operatorname {E}$  has the property\n",
    "\n",
    "${\\displaystyle \\operatorname {E} [XY]=\\operatorname {E} [X]\\operatorname {E} [Y]}$\n",
    "\n",
    "\n",
    "and the covariance ${\\displaystyle \\operatorname {cov} [X,Y]}$ is zero, as follows from\n",
    "\n",
    "${\\displaystyle \\operatorname {cov} [X,Y]=\\operatorname {E} [XY]-\\operatorname {E} [X]\\operatorname {E} [Y].}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcec375-893c-42aa-bc1d-8a812a7295eb",
   "metadata": {},
   "source": [
    "# Conditional independence\n",
    "\n",
    "Conditional independence is a fundamental concept in probability theory and statistics, which helps to understand the relationships between random variables. The example you provided from Wikipedia illustrates this concept through a simple scenario involving colored boxes.\n",
    "\n",
    "Here’s a summary of the example and explanation:\n",
    "\n",
    "### Example Explanation\n",
    "\n",
    "Consider three events related to a process of selecting boxes:\n",
    "\n",
    "1. **Event A**: \"The box is red.\"\n",
    "2. **Event B**: \"The box is blue.\"\n",
    "3. **Event C**: \"The box is small.\"\n",
    "\n",
    "We want to understand the relationships between these events, particularly whether Event A and Event B are conditionally independent given Event C.\n",
    "\n",
    "### Definitions\n",
    "\n",
    "- **Independent Events**: Two events $A$ and $B$ are independent if the occurrence of $A$ does not affect the probability of $B$ occurring, i.e., $P(A \\cap B) = P(A)P(B)$.\n",
    "\n",
    "- **Conditional Independence**: Two events $A$ and $B$ are conditionally independent given a third event $C$ if the occurrence of $A$ and $B$ are independent when $C$ is known to occur, i.e., $P(A \\cap B | C) = P(A | C)P(B | C)$.\n",
    "\n",
    "### Applying Conditional Independence\n",
    "\n",
    "To apply this concept, we need to analyze the probability distributions involved:\n",
    "\n",
    "- $P(A | C)$: The probability that the box is red given that the box is small.\n",
    "- $P(B | C)$: The probability that the box is blue given that the box is small.\n",
    "- $P(A \\cap B | C)$: The probability that the box is both red and blue given that the box is small.\n",
    "\n",
    "For conditional independence to hold, the following must be true:\n",
    "$ P(A \\cap B | C) = P(A | C) P(B | C) $\n",
    "\n",
    "### Detailed Analysis\n",
    "\n",
    "Suppose we have data about the boxes:\n",
    "\n",
    "- The total number of boxes: $N$.\n",
    "- The number of red boxes: $N_R$.\n",
    "- The number of blue boxes: $N_B$.\n",
    "- The number of small boxes: $N_S$.\n",
    "- The number of red small boxes: $N_{R \\cap S}$.\n",
    "- The number of blue small boxes: $N_{B \\cap S}$.\n",
    "- The number of red and blue small boxes: $N_{R \\cap B \\cap S}$.\n",
    "\n",
    "From this data, we can calculate the required probabilities:\n",
    "\n",
    "$ P(A | C) = \\frac{N_{R \\cap S}}{N_S} $\n",
    "$ P(B | C) = \\frac{N_{B \\cap S}}{N_S} $\n",
    "$ P(A \\cap B | C) = \\frac{N_{R \\cap B \\cap S}}{N_S} $\n",
    "\n",
    "To check conditional independence, we verify if:\n",
    "\n",
    "$ \\frac{N_{R \\cap B \\cap S}}{N_S} = \\left(\\frac{N_{R \\cap S}}{N_S}\\right) \\left(\\frac{N_{B \\cap S}}{N_S}\\right) $\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$ N_{R \\cap B \\cap S} = \\frac{N_{R \\cap S} \\cdot N_{B \\cap S}}{N_S} $\n",
    "\n",
    "If the above equation holds, then events A and B are conditionally independent given event C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3e7bc",
   "metadata": {},
   "source": [
    "# Conditional Distribution of Y Given X\n",
    "Refs: [1](https://online.stat.psu.edu/stat414/lesson/21/21.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ee4fa-9bd9-437b-87e6-a03a125c5b11",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"images/Conditional_independence2.svg.png\" alt=\"\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee462f-76f6-4a47-9bda-9b173a528a3a",
   "metadata": {},
   "source": [
    "### Practical Interpretation\n",
    "\n",
    "If our analysis shows that knowing whether the box is small does not change the relationship between the box being red and blue, then we have demonstrated conditional independence. This concept can be extended to more complex scenarios in probability and statistics, such as Bayesian networks and Markov models.\n",
    "\n",
    "By understanding this example, we can grasp how conditional independence helps simplify the analysis of probabilistic models by allowing us to break down complex dependencies into simpler, more manageable components.\n",
    "\n",
    "Refs: [1](https://en.wikipedia.org/wiki/Conditional_independence#Coloured_boxes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
