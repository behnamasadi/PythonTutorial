{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "executive-simulation",
   "metadata": {},
   "source": [
    "## Likelihood\n",
    "Now if you take a random sample (an observation) from the above graph, let say at $x=-2$, then the $y=0.6$ is the likelihood that this observation generated from the this graph ($\\mu=4$ and $\\sigma=2$). The value $y=0.6$ is relatively small, which make sense, because if take a random sample from a gaussian distribution the chance that we get closer to the mean is bigger than the chance of being away from that.\n",
    "\n",
    "$Likelihood(\\mu=4,\\sigma=2|Observation=-2)$\n",
    "\n",
    "Refs: [1](https://www.youtube.com/watch?v=pYxNSUDSFH4&t=182s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-current",
   "metadata": {},
   "source": [
    "<img src='images/likelihood.png'>\n",
    "\n",
    "The red ares=$P(5<x<10|\\mu=4,\\sigma=2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-collectible",
   "metadata": {},
   "source": [
    "## Log Likelihood\n",
    "Since we assume the obeservations are independent of each other and for independent probabilites we have:\n",
    "\n",
    "$P(x_1,x_2|\\theta)=P(x_1|\\theta).P(x_2|\\theta)$\n",
    "\n",
    "$x_i\\perp x_j \\text{indipendent}$ \n",
    "\n",
    "Therefore for likelihood we would also have:\n",
    "$L(distribution|observeation1,observeation2)=L(distribution|observeation1)L(distribution|observeation2)$\n",
    "\n",
    "Since Likelihood values are small and multiplying them will make them smaller which might may round to zero in  computation, we use the **log** function. The **log** function will also truns multipication into summation and power into multipication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-presentation",
   "metadata": {},
   "source": [
    "# Likelihood ratio\n",
    "We usually don't use likelihood function but we compare them :\n",
    "$L(\\theta1|x)>L(\\theta2|x)?$\n",
    "\n",
    "Since **log** function is a monotonically increasig function, when the likelihood increase/ decrease, the log will also increase/ decrease, so we compare log of likelihood \n",
    "# Odds ratio\n",
    "Odds provide a measure of the likelihood of a particular outcome. The odds of rolling a 6 is 1:5, The odds of rolling either a 5 or 6 is 2:4.\n",
    "\n",
    "\n",
    "$odds=\\frac{P(event)}{1-P(event)}$\n",
    "# Log odds\n",
    "\n",
    "The logit function or the log-odds is the logarithm of the odds. If $p$ is a probability, \n",
    "\n",
    "${\\displaystyle \\operatorname {logit} (p)=\\log \\left({\\frac {p}{1-p}}\\right)=\\log(p)-\\log(1-p)=-\\log \\left({\\frac {1}{p}}-1\\right)\\,.}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-shield",
   "metadata": {},
   "source": [
    "## Marginal likelihood\n",
    "\n",
    "A marginal likelihood function (integrated likelihood), is a likelihood function in which some parameter variables have been marginalized. \n",
    "\n",
    "### In the context of Bayesian statistics\n",
    "Given a set of independent identically distributed data points ${\\displaystyle \\mathbf {X} =(x_{1},\\ldots ,x_{n}),}$, where $x_{i}\\sim p(x_{i}|\\theta )$ according to some probability distribution parameterized by $\\theta$ , where $\\theta$  itself is a random variable described by a distribution, i.e. ${\\displaystyle \\theta \\sim p(\\theta \\mid \\alpha ),}$ the marginal likelihood in general asks what the probability ${\\displaystyle p(\\mathbf {X} \\mid \\alpha )}$ is, where $\\theta$  has been marginalized out (integrated out): \n",
    "\n",
    "\n",
    "${\\displaystyle p(\\mathbf {X} \\mid \\alpha )=\\int _{\\theta }p(\\mathbf {X} \\mid \\theta )\\,p(\\theta \\mid \\alpha )\\ \\operatorname {d} \\!\\theta }$\n",
    "\n",
    "###  In classical statistics\n",
    "In In classical statistics, the concept of marginal likelihood occurs instead in the context of a joint parameter ${\\displaystyle \\theta =(\\psi ,\\lambda )}$, where $\\psi$  is the actual parameter of interest, and $\\lambda$  is a non-interesting nuisance parameter.\n",
    "\n",
    "\n",
    "We know that:\n",
    "\n",
    "$P(B|C)=\\sum_{i} P(B|A_i,C)P(A_i|C) $\n",
    "\n",
    "And we also know \n",
    "\n",
    "${\\mathcal {L}}(\\theta|X)=p(X|\\theta)=p_{\\theta }(X)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "by marginalizing out $\\lambda$ :\n",
    "\n",
    "${\\displaystyle {\\mathcal {L}}(\\psi ;\\mathbf {X} )=p(\\mathbf {X} \\mid \\psi )=\\int _{\\lambda }p(\\mathbf {X} \\mid \\lambda ,\\psi )\\,p(\\lambda \\mid \\psi )\\ \\operatorname {d} \\!\\lambda }$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
