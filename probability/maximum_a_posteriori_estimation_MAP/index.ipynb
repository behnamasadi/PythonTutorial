{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40b4b82",
   "metadata": {},
   "source": [
    "## Maximum a Posteriori (MAP) Estimation\n",
    "Maximum a Posteriori Estimation (MAP) is a statistical method used to estimate an unknown quantity based on observed data. It's particularly useful in Bayesian inference, where it incorporates prior knowledge or beliefs about the parameter to be estimated, along with the likelihood of the observed data.\n",
    "\n",
    "### The MAP Equation\n",
    "\n",
    "The MAP estimate of a parameter $\\theta$ given data $D$ can be formulated using Bayes' theorem. The theorem relates the conditional and marginal probabilities of random events, and in the context of MAP, it is used to update the probability of a hypothesis as more information becomes available.\n",
    "\n",
    "The equation for MAP is given by:\n",
    "\n",
    "$\n",
    "\\hat{\\theta}_{\\text{MAP}} = \\arg \\max_{\\theta} P(\\theta | D) = \\arg \\max_{\\theta} \\frac{P(D | \\theta) P(\\theta)}{P(D)}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $\\hat{\\theta}_{\\text{MAP}}$ is the MAP estimate of the parameter $\\theta$.\n",
    "- $P(\\theta | D)$ is the posterior probability of $\\theta$ given data $D$.\n",
    "- $P(D | \\theta)$ is the likelihood of data $D$ given $\\theta$.\n",
    "- $P(\\theta)$ is the prior probability of $\\theta$, representing our initial belief about $\\theta$ before observing the data.\n",
    "- $P(D)$ is the marginal likelihood of data $D$, also known as the evidence, which acts as a normalizing constant.\n",
    "\n",
    "Since $P(D)$ is constant for all values of $\\theta$, it does not affect the arg max operation, and the MAP estimation can be simplified to:\n",
    "\n",
    "$\n",
    "\\hat{\\theta}_{\\text{MAP}} = \\arg \\max_{\\theta} P(D | \\theta) P(\\theta)\n",
    "$\n",
    "\n",
    "### Numerical Example\n",
    "\n",
    "Imagine you're trying to estimate the bias (probability of heads, $\\theta$) of a coin based on observing 10 flips, of which 7 are heads and 3 are tails. You have a prior belief that the coin is slightly biased towards heads, modeled as a Beta distribution with parameters $\\alpha = 3$ and $\\beta = 2$, representing previous knowledge or belief about the distribution of $\\theta$.\n",
    "\n",
    "The likelihood function for a binomial distribution, given $k$ successes out of $n$ trials, is:\n",
    "\n",
    "$\n",
    "P(D | \\theta) = \\binom{n}{k} \\theta^k (1 - \\theta)^{n-k}\n",
    "$\n",
    "\n",
    "For our coin flip example, $k = 7$ heads out of $n = 10$ flips.\n",
    "\n",
    "The prior distribution is the Beta distribution, which is:\n",
    "\n",
    "$\n",
    "P(\\theta) = \\frac{\\theta^{\\alpha-1} (1 - \\theta)^{\\beta-1}}{B(\\alpha, \\beta)}\n",
    "$\n",
    "\n",
    "For $\\alpha = 3$ and $\\beta = 2$, and ignoring the normalizing constant $B(\\alpha, \\beta)$ since it doesn't affect the arg max operation.\n",
    "\n",
    "The MAP estimate is obtained by maximizing the product of these two:\n",
    "\n",
    "$\n",
    "\\hat{\\theta}_{\\text{MAP}} = \\arg \\max_{\\theta} \\theta^{7+3-1} (1 - \\theta)^{10-7+2-1}\n",
    "$\n",
    "\n",
    "This can be solved analytically for simple cases like this, or numerically for more complex scenarios. Let's calculate the MAP estimate for this example.\n",
    "\n",
    "The Maximum a Posteriori (MAP) estimate of the bias ($\\theta$) of the coin, based on observing 7 heads out of 10 flips and incorporating our prior belief that the coin is slightly biased towards heads (with a Beta prior of $\\alpha = 3$ and $\\beta = 2$), is approximately $0.692$. This means that, given the observed data and our prior belief, the estimated probability of the coin landing on heads is around 69.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39e7226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import beta\n",
    "import numpy as np\n",
    "\n",
    "# Prior parameters\n",
    "alpha_prior = 3\n",
    "beta_prior = 2\n",
    "\n",
    "# Observations\n",
    "heads = 7\n",
    "tails = 3\n",
    "\n",
    "# Update parameters with observations\n",
    "alpha_post = alpha_prior + heads\n",
    "beta_post = beta_prior + tails\n",
    "\n",
    "# Compute MAP estimate: mode of the posterior Beta distribution\n",
    "# For Beta distribution, mode = (alpha - 1) / (alpha + beta - 2)\n",
    "theta_map = (alpha_post - 1) / (alpha_post + beta_post - 2)\n",
    "\n",
    "theta_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3020ffd0",
   "metadata": {},
   "source": [
    "##  Maximum a Posteriori (MAP) Estimation vs Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "- **Principle:** MLE is a method for estimating the parameters of a statistical model. It selects the parameter values that maximize the likelihood function, which measures the probability of observing the given sample data under different parameter values. The likelihood function depends solely on the observed data and the model parameters.\n",
    "- **Assumption:** MLE assumes that the data are observed from the model with certain parameters. It does not incorporate prior knowledge about the parameters themselves; it is solely focused on the observed data.\n",
    "- **Objective:** The objective is to find the parameter values that make the observed data most probable under the model.\n",
    "\n",
    "### Maximum a Posteriori (MAP) Estimation\n",
    "\n",
    "- **Principle:** MAP estimation also aims to find the parameter values for a statistical model, but it goes a step further by incorporating prior knowledge about the parameter distribution. It does this by maximizing the posterior distribution, which combines the likelihood of the observed data with the prior distribution of the parameters.\n",
    "- **Assumption:** MAP assumes that the parameters have a prior distribution, which reflects our knowledge or beliefs about the parameters before observing the data. This prior distribution is then updated with the observed data to form the posterior distribution.\n",
    "- **Objective:** The objective of MAP is to find the parameter values that are most probable given both the observed data and the prior distribution of the parameters.\n",
    "\n",
    "### Relationship and Key Differences\n",
    "\n",
    "- **Bayesian Framework:** MAP can be seen as a Bayesian approach because it incorporates prior information through the use of a prior distribution. MLE can be viewed as a special case of MAP where the prior is uniform or non-informative, implying that all parameter values are equally likely a priori.\n",
    "- **Output:** While both methods give point estimates of the parameters, the interpretation is different. MLE provides the parameter values that maximize the likelihood of the observed data without considering prior information, whereas MAP gives the parameter values that are most probable given both the observed data and the prior.\n",
    "- **Flexibility:** MAP is generally more flexible than MLE as it allows the incorporation of prior knowledge. This can be particularly advantageous when dealing with limited data or when prior information is strong and relevant.\n",
    "\n",
    "In summary, the key difference between MLE and MAP lies in the incorporation of prior knowledge about the parameters. MLE focuses on the likelihood of the observed data under the model, while MAP combines this likelihood with a prior distribution to find the most probable parameter values given both the data and the prior beliefs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b05afa",
   "metadata": {},
   "source": [
    "##  Maximum a Posteriori (MAP) Estimation vs Minimum Mean Square Error (MMSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e19a0",
   "metadata": {},
   "source": [
    "Refs [1](https://twitter.com/docmilanfar/status/1589855121206579201)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
