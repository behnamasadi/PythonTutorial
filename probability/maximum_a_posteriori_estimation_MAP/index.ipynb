{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40b4b82",
   "metadata": {},
   "source": [
    "## Maximum a Posteriori (MAP) Estimation\n",
    "Maximum a Posteriori Estimation (MAP) is a statistical method used to estimate an unknown quantity based on observed data. It's particularly useful in Bayesian inference, where it incorporates prior knowledge or beliefs about the parameter to be estimated, along with the likelihood of the observed data.\n",
    "\n",
    "### The MAP Equation\n",
    "\n",
    "The MAP estimate of a parameter $\\theta$ given data $D$ can be formulated using Bayes' theorem. The theorem relates the conditional and marginal probabilities of random events, and in the context of MAP, it is used to update the probability of a hypothesis as more information becomes available.\n",
    "\n",
    "The equation for MAP is given by:\n",
    "\n",
    "$\n",
    "\\hat{\\theta}_{\\text{MAP}} = \\arg \\max_{\\theta} P(\\theta | D) = \\arg \\max_{\\theta} \\frac{P(D | \\theta) P(\\theta)}{P(D)}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $\\hat{\\theta}_{\\text{MAP}}$ is the MAP estimate of the parameter $\\theta$.\n",
    "- $P(\\theta | D)$ is the posterior probability of $\\theta$ given data $D$.\n",
    "- $P(D | \\theta)$ is the likelihood of data $D$ given $\\theta$.\n",
    "- $P(\\theta)$ is the prior probability of $\\theta$, representing our initial belief about $\\theta$ before observing the data.\n",
    "- $P(D)$ is the marginal likelihood of data $D$, also known as the evidence, which acts as a normalizing constant.\n",
    "\n",
    "Since $P(D)$ is constant for all values of $\\theta$, it does not affect the arg max operation, and the MAP estimation can be simplified to:\n",
    "\n",
    "$\n",
    "\\hat{\\theta}_{\\text{MAP}} = \\arg \\max_{\\theta} P(D | \\theta) P(\\theta)\n",
    "$\n",
    "\n",
    "### Numerical Example\n",
    "\n",
    "Imagine you're trying to estimate the bias (probability of heads, $\\theta$) of a coin based on observing 10 flips, of which 7 are heads and 3 are tails. You have a prior belief that the coin is slightly biased towards heads, modeled as a Beta distribution with parameters $\\alpha = 3$ and $\\beta = 2$, representing previous knowledge or belief about the distribution of $\\theta$.\n",
    "\n",
    "The likelihood function for a binomial distribution, given $k$ successes out of $n$ trials, is:\n",
    "\n",
    "$\n",
    "P(D | \\theta) = \\binom{n}{k} \\theta^k (1 - \\theta)^{n-k}\n",
    "$\n",
    "\n",
    "For our coin flip example, $k = 7$ heads out of $n = 10$ flips.\n",
    "\n",
    "The prior distribution is the Beta distribution, which is:\n",
    "\n",
    "$\n",
    "P(\\theta) = \\frac{\\theta^{\\alpha-1} (1 - \\theta)^{\\beta-1}}{B(\\alpha, \\beta)}\n",
    "$\n",
    "\n",
    "For $\\alpha = 3$ and $\\beta = 2$, and ignoring the normalizing constant $B(\\alpha, \\beta)$ since it doesn't affect the arg max operation.\n",
    "\n",
    "The MAP estimate is obtained by maximizing the product of these two:\n",
    "\n",
    "$\n",
    "\\hat{\\theta}_{\\text{MAP}} = \\arg \\max_{\\theta} \\theta^{7+3-1} (1 - \\theta)^{10-7+2-1}\n",
    "$\n",
    "\n",
    "This can be solved analytically for simple cases like this, or numerically for more complex scenarios. Let's calculate the MAP estimate for this example.\n",
    "\n",
    "The Maximum a Posteriori (MAP) estimate of the bias ($\\theta$) of the coin, based on observing 7 heads out of 10 flips and incorporating our prior belief that the coin is slightly biased towards heads (with a Beta prior of $\\alpha = 3$ and $\\beta = 2$), is approximately $0.692$. This means that, given the observed data and our prior belief, the estimated probability of the coin landing on heads is around 69.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39e7226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import beta\n",
    "import numpy as np\n",
    "\n",
    "# Prior parameters\n",
    "alpha_prior = 3\n",
    "beta_prior = 2\n",
    "\n",
    "# Observations\n",
    "heads = 7\n",
    "tails = 3\n",
    "\n",
    "# Update parameters with observations\n",
    "alpha_post = alpha_prior + heads\n",
    "beta_post = beta_prior + tails\n",
    "\n",
    "# Compute MAP estimate: mode of the posterior Beta distribution\n",
    "# For Beta distribution, mode = (alpha - 1) / (alpha + beta - 2)\n",
    "theta_map = (alpha_post - 1) / (alpha_post + beta_post - 2)\n",
    "\n",
    "theta_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e19a0",
   "metadata": {},
   "source": [
    "Refs [1](https://twitter.com/docmilanfar/status/1589855121206579201)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e154a-dc18-4ca0-a730-6e946bc60283",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
